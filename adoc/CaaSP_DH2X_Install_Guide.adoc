
:docinfo:

= SAP Data Hub 2 on SUSE CaaS Platform 3: Installation Guide

++++
<?pdfpagebreak?>
++++

Today, more and more data is created in business and industry.
Alongside data growth, there is an increasing necessity to manage and exploit this data.
SAP Data Hub 2 is a tool that makes dealing with large amounts of data easier.
SUSE Container as a Service Platform 3 (SUSE CaaS Platform 3) is the perfect foundation for SAP Data Hub 2.

== Requirements

To install SAP Data Hub 2 on SUSE CaaS Platform 3, make sure to satisfy the
following hardware and software requirements.

=== Hardware

==== SUSE CaaS Platform 3 Cluster

Hardware requirements (see https://help.sap.com/viewer/product/SAP_DATA_HUB/[SAP Data Hub Install Guide])
See SAP's sizing recommendations:

* link:https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.3.latest/en-US/79724de552db4b2b81c4a893f2c7ed18.html[SAP Data Hub 2.3 ]

* link:https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.4.latest/en-US/7e2a9bf62ec94e9694648e2b5d2ce882.html[SAP Data Hub 2.4]

* link:https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.5.latest/en-US[SAP Data Hub 2.5]

* link:https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.6.latest/en-US[SAP Data Hub 2.6]

The minimum hardware requirements for installing SAP Data Hub 2 on premise are:

* 4 Kubernetes cluster nodes (1 master node and 3 worker nodes)

** The master node should be a 4 core machine with > 32 GiB RAM

** The three Kubernetes worker nodes should be a machine with 4 cores and with > 64 GiB RAM

// TODO check network requirements
// TODO disk requirements

* Access to a SUSE Enterprise Storage 5 system (see SAP Note link:https://launchpad.support.sap.com/#/notes/2686169[Pre-requisites for installing SAP Data Hub])

==== Management Host ("Jump Host")

It is recommended to do the installation of SAP Data Hub Foundation from an external jump host and not from within the SUSE CaaS Platform Cluster.

The hardware and operating system specifications for the jump host can be for example as follows:

- SUSE Linux Enterprise Server 12 SP4 or SUSE Linux Enterprise Server 15 (or even openSUSE Leap 15.X)
- 2 Cores
- 8 GiB RAM
- Disk space: 50 GiB for `/`, including the space for the SAP Data Hub 2 software and at least 20 GiB for `/var/lib/docker` (necessary for the SAP Data Hub 2 installation)
- Network connectivity to the SUSE CaaS Platform cluster (1 GBit/s)

=== Software Requirements

The following software is needed

* SUSE CaaS Platform 3

* SAP Data Hub 2

* optional: SAP Maintenance Planner

* optional: SAP Host Agent

* optional: Hadoop/Spark (see Vora's Spark extensions)


== Relevant Documentation:

* SUSE

** link:https://documentation.suse.com/suse-caasp/3/[SUSE CaaS Platform 3]
** link:https://documentation.suse.com/ses/5.5/[SUSE Enterprise Storage 5.5]

* SAP

** link:https://help.sap.com/viewer/product/SAP_DATA_HUB/2.6.latest/en-US[SAP Data Hub]
** link:https://launchpad.support.sap.com/#/notes/2764652[SAP Data Hub 2.6 release note]
** link:https://launchpad.support.sap.com/#/notes/2686169[Pre-requisites for installing SAP Data Hub]
** link:https://launchpad.support.sap.com/#/notes/2776522[SAP Data Hub 2: Specific Configurations for Installation on SUSE CaaS Platform]


== Installing SUSE CaaS Platform 3

=== Downloading the Installation Media

All installation media can be found at https://download.suse.com.

=== Acquiring a Subscription for SUSE CaaS Platform 3

To receive maintenance updates for SUSE products, you need a valid subscription for the respective products.
For more information about subscriptions for SUSE CaaS Platform, see
https://www.suse.com/support/?id=SUSE_CaaS_Platform.


=== Read the Deployment Guide for SUSE CaaS Platform 3

SUSE CaaS Platform is designed to make the installation of Kubernetes easy.
For more information on this topic, see the SUSE CaaS Platform 3 __Deployment Guide__ from https://documentation.suse.com.
For further reference, a __Quick Start Guide__ and an __Administrator's Guide__ are available as well.

=== Installing SUSE CaaS Platform 3

This document describes the installation of SUSE CaaS Platform 3 from ISO images.
Make sure that the host names you will use for the installation are resolvable via DNS.
A static network setup is preferred.

Have the FQDN or IP address of your time server available. A reliable system time is required.
Connect the media to your hardware and boot from the media.
// Screenshot?

From the GRUB menu, choose __Installation__.

==== Installing the Administration Node
// Add Pictures

. After initializing the hardware, the YaST installer opens the network configuration dialog:
+
image::002-SCT-CaaSP.png[title="Network Configuration Admin Node",480,640,scaledwidth=80%,align=center]

. On the installation overview screen:
+
* select the keyboard layout and language according to your needs.

* Specify your subscription credentials or the URL of your SMT/RMT server to register your installation.

* Set the password for the root account.

* Assign the role __Administration Node__.

+
image::007-SCT-CaaSP.png[title="Installation Overview Admin Node",480,640,scaledwidth=80%,align=center]


// Screen Shots????


==== Installing the Remaining Cluster Nodes

. Boot the machine from the ISO image and select __Installation__ from the GRUB boot menu.

. The installer starts and the network configuration dialog is shown.
+
image::002-SCT-CaaSP.png[title="Network Configuration Cluster Nodes",480,640,scaledwidth=80%,align=center]

. Configure the network according to your needs.

. On the installation overview screen:
+
* Select the keyboard layout and language according to your needs.
* Specify your subscription credentials or the URL of your SMT/RMT server to register your installation.

* Set the password for the root account.

* Assign the role __Cluster Node__.

+
image::021-SCT-CaaSP.png[title="Installation Overview Cluster Node",480,640,scaledwidth=80%,align=center]

//TODO Screenshots

==== Bootstrapping the Kubernetes Cluster

. After the installation of the admin node, open your browser and visit
++https://name.domain.tld++.
+
image::015-SCT-CaaSP.png[title="Velum",480,640,scaledwidth=80%,align=center]

. Create the admin account and set the admin user name and password.
Log in with the newly created admin credentials.
+
image::014-SCT-CaaSP.png[title="Velum login",480,640,scaledwidth=80%,align=center]

. Configure SUSE CaaS Platform 3.
+
image::016-SCT-CaaSP.png[title="Velum Cluster Configuration",480,640,scaledwidth=80%,align=center]

. Select the nodes to be included in the cluster.
+
image::024-SCT-CaaSP.png[title="Select Nodes and Roles 1",480,640,scaledwidth=80%,align=center]

. Assign roles (master/worker) according to your needs.
+
image::027-SCT-CaaSP.png[title="Select Nodes and Roles 2",480,640,scaledwidth=80%,align=center]
+
image::028-SCT-CaaSP.png[itle="Select Nodes and Roles 3",480,640,scaledwidth=80%,align=center]

. Bootstrap the Kubernetes cluster.
+
image::030-SCT-CaaSP.png[title="Confirm Bootstrap",480,640,scaledwidth=80%,align=center]

. After a successful bootstrap, the screen below is shown.
+
image::032-SCT-CaaSP.png[title="Cluster Status Nodes",480,640,scaledwidth=80%,scaledwidth=80%,align=center]


==== Installing the Jump Host

For further configuration and deployments on SUSE CaaS Platform it is highly recommended to install a jump host, also called a __management host__.
This section describes installing the necessary tools to be able to deploy SAP Data Hub 2 on SUSE CaaS Platform.

. Install SUSE Linux Enterprise Server 12 SP4+ or SUSE Linux Enterprise Server 15 as the operating system on the jump host.

. Register your installation against the SUSE Customer Center (SCC) or your local SMT (Subscription Management Tool) or RMT (Repository Mirroring Tool).

. Register the Container Module included in the SUSE Linux Enterprise Server subscription:
+
----
# SUSEConnect -p x86_64/SLE-Container-Module
----

. Install Docker from the Container Module:
+
----
# zypper in docker
----

. Download *kubectl* matching the Kubernetes version of your SUSE CaaS Platform installation.
Download *kubectl 1.10.11* or higher.
+
----
$ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.10.11/bin/linux/amd64/kubectl
$ sudo mv kubectl /usr/bin/kubectl
$ sudo chmod a+x /usr/bin/kubectl
----


. Download the `kubeconfig` file from the Velum Dashboard.
+
image::032a-SCT-CaaSP.png[title="kubeconfig 1",480,640,scaledwidth=80%,align=center]
+
image::033-SCT-CaaSP.png[title="kubeconfig 2",480,640,scaledwidth=80%,align=center]
+
image::034-SCT-CaaSP.png[title="kubeconfig 3",480,640,scaledwidth=80%,align=center]

// ^FIXME: Is the order correct? 1. Status page->2. Login page->3. File download
// ^FIXME 2: Is there really a necessity for three _uncropped_ screenshots?

. Enable the downloaded `kubeconfig` by doing either of the following:
+
* Configure the `KUBECONFIG` environment variable:
+
----
$ export KUBECONFIG=<PATH/TO/YOUR/kubeconfig-file>/kubeconfig
----

* Move it to default path:
+
----
$ mv <PATH/TO/YOUR/kubeconfig-file>/kubeconfig ~/.kube/config
----

. Connect to your Kubernetes cluster using *kubectl*:
+
----
$ kubectl get cluster-info
Kubernetes master is running at https://spwdfvml2054.example.com:6443
Dex is running at https://spwdfvml2054.example.com:6443/api/v1/namespaces/kube-system/services/dex:dex/proxy
KubeDNS is running at https://spwdfvml2054.example.com:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
kubernetes-dashboard is running at https://spwdfvml2054.example.com:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:https/proxy
Tiller is running at https://spwdfvml2054.example.com:6443/api/v1/namespaces/kube-system/services/tiller:tiller/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
----
+
----
$ kubectl get nodes

NAME           STATUS   ROLES    AGE    VERSION

spwdfvml2054   Ready    master   301d   v1.10.11

spwdfvml2055   Ready    <none>   301d   v1.10.11

spwdfvml2056   Ready    <none>   301d   v1.10.11

spwdfvml2082   Ready    <none>   301d   v1.10.11
----

. Download Helm from https://helm.sh. Install and configure the Helm client.
The version should match the Tiller version deployed on your SUSE CaaS Platform installation.
+
----
$ curl https://raw.githubusercontent.com/helm/helm/master/scripts/get > get_helm.sh
$ chmod 700 get_helm.sh
$ ./get_helm.sh --version 2.8.2
$ mv helm ~/bin/helm
$ helm init --client-only
----


==== Deploying Optional Components in the SUSE CaaS Platform 3 Cluster

You can deploy some useful optional applications in your SUSE CaaS Platform 3 cluster:

* Heapster allows analyzing cluster performance

* The Kubernetes dashboard allows administrating and basic monitoring of the Kubernetes cluster

See also https://documentation.suse.com/suse-caasp/3/html/caasp-admin/.

. Install Heapster:
+
----
$ helm install --name heapster-default --namespace=kube-system stable/heapster \
--version=0.2.7 --set rbac.create=true
----

. Install the Kubernetes dashboard:
+
----
$ helm install --namespace=kube-system \
--name=kubernetes-dashboard stable/kubernetes-dashboard \
--version=0.6.1
----

. Start the Kubernetes API proxy:
+
----
$ kubectl proxy
----

. Extract the *id-token* from the *kubeconfig* file:
+
----
$ grep id-token $KUBECONFIG | awk '{ print $2 }'
----
+
This token will allow you to log in in the next step.

. In your browser, open:
+
----
http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/
----
+
Use the ID you found in *kubeconfig* to log in to the Kubernetes dashboard.



== Installing SAP Data Hub 2

The following sections describe the preparation and installation of SAP Data Hub 2 on the SUSE CaaS Platform 3 Cluster.


=== Preparing the SAP Data Hub 2 Installation

To install SAP Data Hub on SUSE CaaS Platform successfully, perform the actions detailed below.


==== Downloading the SAP Data Hub 2 Software Archive

. Go to the SAP Software Download Center, log in with your SAP account and search for `SAP DATA HUB 2` or access this link.

// FIXME: Where is the link mentioned above?

. Download the SAP Data Hub Foundation file, for example: DHFOUNDATION03_3-80004015.ZIP (SAP DATA HUB - FOUNDATION 2.3) or DHFOUNDATION06_1-80004015.ZIP (SAP DATA HUB - FOUNDATION 2.6)

. Unzip the software archive on to your jump host.

There are two ways to install the SAP Data Hub 2:

* Use the SL Plugin. There are two variants of doing so:

** SL Plugin with Maintenance Planner (mpsl)

** SL Plugin only (mpfree)

* Use the command line install.sh script.

This document will focus on the latter installation method.


[[sec-prerequisite-cluster]]
==== Prerequisites on the SUSE CaaS Platform 3 Cluster

Where not stated otherwise, all of the following steps need to be performed on the jump host.

. Create the namespace in the Kubernetes cluster to install SAP Data Hub 2:
+
----
$ kubectl create namespace datahub
----

. On SUSE Enterprise Storage, create the storage class to provide volumes for SAP Data Hub 2.

. Make sure you have the connection data for your SUSE Enterprise Storage at hand:
+
* IP addresses and port number (defaults to 6789) of the monitor nodes of your SUSE Enterprise Storage

* A data pool created (data hub in this example) on your SUSE Enterprise Storage for the use with SAP Data Hub 2

. Edit the example below to fit your environment.
+
----
$ cat > storageClass.yaml <<EOF
apiVersion: storage.kubernetes.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
  name: datahub
  namespace: default
parameters:
  adminId: admin
  adminSecretName: ceph-admin-secret
  adminSecretNamespace:  default
  imageFeatures: layering
  imageFormat: "2"
  monitors: <IP ADDRESS OF MONITOR 1>:6789, <IP ADDRESS OF MONITOR 2>:6789, <IP ADDRESS OF MONITOR 3 >:6789
  pool: datahub
  userId: admin
  userSecretName: ceph-user-secret
provisioner: kubernetes.io/rbd
reclaimPolicy: Delete
volumeBindingMode: Immediate
EOF

$ kubectl create -f storageClass.yaml
----

. Create the secrets needed to access the storage.

.. From SUSE Enterprise Storage, obtain the keys located in `ceph.admin.keyring` and `ceph.user.keyring`.

.. Base64-encode the keys as follows:
+
----
echo <YOUR KEY HERE> | base64
----

.. Configure the encoded secrets:
+
----
$ cat > ceph-admin-secret.yaml <<EOF
apiVersion: v1
kind: Secret
metadata:
    name: ceph-admin-secret
type: "kubernetes.io/rbd"
data:
   key: <YOUR BASE64 ENCODED KEY HERE>
EOF
image::002-SCT-CaaSP.png
$ cat > ceph-user-secret.yaml <<EOF
apiVersion: v1
kind: Secret
metadata:
    name: ceph-user-secret
type: "kubernetes.io/rbd"
data:
   key: <YOUR BASE64 ENCODED KEY HERE>
EOF

$ kubectl create -f ceph-admin-secret.yaml
$ kubectl create -f ceph-user-secret.yaml
----


=== Installation of SAP Data Hub 2 Using the Maintenance Planner with SL Plugin (mpsl Method)

The installation method via SL plugin is a Web-based installation method recommended by SAP, offering you an option to send analytics data and feedback to SAP.
All necessary prerequisites are met by applying all the steps described above.

[IMPORTANT]
--
You need to install the latest SAP Host Agent on the jump host.
You can use the RPM package which can be downloaded from the SAP Software Download Center.
--

=== Installing SAP Data Hub 2 Using the SL Plugin (mpfree Method)

The Installation of SAP Data Hub 2 using the SL plugin is an alternative command-line-based installation method.
Refer to the SAP Data Hub documentation (2.3)) / (2.4) / (2.5) / (2.6) for more information and the exact procedure.
//TODO insert URLs


=== Installing SAP Data Hub 2 from the Command Line (Manual Installation)

. Unpack the SAP Data Hub 2.6 software archive on the jump host with the following command:
+
----
$ unzip DHFOUNDATION06_1-80004015.ZIP
----

. Run the install command as described in SAP Data Hub 2 install guide at
https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.6.latest/en-US.
+
----
$ cd SAP-Datahub-2.4.63-Foundation
$ export DOCKER_REGISTRY=<URI of your registry>
$ export NAMESPACE=datahub
$ ./install.sh
----
+
This interactive script configures the installation of SAP Data Hub.
You should have the following information at a hand:
+
* Name and credentials of your SAP S-User
* Login credentials to your secure registry

//TODO namespace, login on registry, S-User

//TODO explain cmdline options


=== Post-Installation Actions

After successful installation you can connect to the SAP Data Hub Web UI.
You need to identify the service IP and port of the SAP Data Hub UI.

----
kubectl get services
kubectl describe service
----

Point your browser to the IP and port you received from the commands above.
// TODO Example.

Use the login data you defined during the installation.

==== Post-Installation Work

Follow the documentation provided by SAP (link) to the post installation work.

// ^FIXME: Link above missing.

* Create the *vflow-secret* for the modeler app as pointed out in the SAP documentation.

* Import necessary CAs, for example, the CA that signed the certificate of the secure registry.


== Upgrading SAP Data Hub 2

To upgrade an existing SAP Data Hub 2 installation to a higher version (for example 2.3 to 2.4), follow the official instructions from SAP.
You can choose between the following upgrade methods:

* Maintenance Planner: Upgrade SAP Data Hub 2 using the Maintenance Planner / SL Plugin and SAP Host Agent (https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.6.latest/en-US/31079833a65f4f379d5a76957ff8073c.html)
* SL Plugin method: Upgrade SAP Data Hub 2 using the SL Plugin and SAP Host Agent (https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.6.latest/en-US/ff37f3ccf6504bb38d7db53936fe8017.html)
* Command line method: Upgrade SAP Data Hub 2 using the install.sh script (https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.6.latest/en-US/aec679bc0209443ba4ae03a9018d4bd8.html)


== Appendix


=== Installing a Secure Private Docker Registry

To meet the Data Hub requirements you also need a Docker Registry.
The Portus project provides an easy way to build and manage an own Docker Registry.
For more information, see the project homepage at http://port.us.org/.

To set up Portus in a dedicated virtual machine, do:

. Create a suitable a virtual machine:
+
----
# sudo virt-install --name portus-dr --ram 8192 \
  --disk path=/var/lib/libvirt/VMS/portus-dr.qcow2,size=40 --vcpus 4 \
  --os-type linux --os-variant generic --network bridge=common --graphics none \
  --console pty,target_type=serial \
  --location '/var/lib/libvirt/isos/SLE-12-SP4-Server-DVD-x86_64-GM-DVD1.iso' \
  --extra-args 'console=ttyS0,115200n8 serial ifcfg=eth0=10.10.10.11/24,10.10.10.1,10.10.10.11,suse-sap.net hostname=portus-dr domain=suse-sap.net Textmode=1'
----

. In this example, the Portus server will be connected to a local bridge providing common services (DNS, SMT, and Docker Registry) for the SAP Data Hub 2 stack.
Our Portus deployment is container-based and orchestrated locally with `docker-compose`.
Portus `docker-compose` deployment requires an up-to-date release of `docker-compose`.
To install `docker-compose`, use:
+
----
sudo curl -L "https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
----

. Clone the Portus Git repository:
+
----
# git clone https://github.com/SUSE/Portus.git /tmp/Portus-DR
# mv /tmp/Portus-DR/examples/compose ./portus
# cd portus
----

// ^ FIXME: Why first clone to /tmp and the move that cloned repo $somewhere_else instead of cloning to $somewhere_else in the first place?

. Adapt the configuration in the `.env`.
As an example, a valid configuration may look like this:
+
----
# cat .env
MACHINE_FQDN=portus-dr.suse-sap.net
SECRET_KEY_BASE=b494a25faa8d22e430e843e220e424e10ac84d2ce0e64231f5b636d21251
eb6d267adb042ad5884cbff0f3891bcf911bdf8abb3ce719849ccda9a4889249e5c2
PORTUS_PASSWORD=XXXXXXXX
DATABASE_PASSWORD=YYYYYYYY
----

. In the `nginx/nginx.conf` file, adapt the following section:
+
----
server {
    listen 443 ssl http2;
    server_name portus-dr.suse-sap.net;
    root /srv/Portus/public;
----

. Download the latest `docker-compose.yml` for Portus:
+
----
# rm docker-compose.*
# wget https://gist.githubusercontent.com/Patazerty/d05652294d5874eddf192c9b633751ee/raw/6bf4ac6ba14192a1fe5c337494ab213200dd076e/docker-compose.yml
----

. To secure your Docker Registry configuration, add TLS to your setup:
+
----
echo "subjectAltName = DNS:portus-dr.suse-sap.net" > extfile.cnf

openssl genrsa -out secrets/rootca.key 2048

openssl req -x509 -new -nodes -key secrets/rootca.key -subj "/C=FR/ST=FR/O=SUSE" \
  -sha256 -days 1024 -out secrets/rootca.crt

openssl genrsa -out secrets/portus.key 2048

openssl req -new -key secrets/portus.key -out secrets/portus.csr \
  -subj "/C=FR/ST=FR/O=SUSE/CN

openssl req -new -key secrets/portus.key -out secrets/portus.csr \
  -subj "/C=FR/ST=FR/O=SUSE/CN=portus-dr.suse-sap.net"

openssl x509 -req -in secrets/portus.csr -CA secrets/rootca.crt \
  -extfile extfile.cnf -CAkey secrets/rootca.key -CAcreateserial \
  -out secrets/portus.crt -days 500 -sha256
----

. Make the servers aware of the new certificate:
+
----
cp -p secrets/rootca.crt /etc/pki/trust/anchors/.net-ca.crt
scp secrets/rootca.crt root@jumpbox.suse-sap.net:/etc/pki/trust/anchors/portus-dr.suse-sap.net-ca.crt
----

. Update the certificate on all servers that will need to interact with the Docker registry:
+
----
sudo update-ca-certificates
sudo systemctl restart docker
----

. Start your Portus setup with the following command:
+
----
docker-compose up -d
----

. Log in to Portus and set up the registry as shown below:
+
image::portus-registry.png[title="Portus Registry",480,640,scaledwidth=80%,align=center]

// FIXME: Does the above procedure continue here or is this something different?

To install and configure a secure private registry using SUSE Linux Enterprise Server with the Container Module,
the necessary components are Docker, Docker Registry, and Portus.
Create SSL certificates as needed. Distribute the CA certificate to all your Kubernetes nodes.
Run the command:

----
# update-ca-certificates
# systemctl restart docker
----


Create the namespaces on your registry that are needed for SAP Data Hub 2:

* com.sap.hana.container

* com.sap.datahub.linuxx86_64

* com.sap.datahub.linuxx86_64.gcc6

* consul

* elasticsearch

* fabric8

* google_containers

* grafana

* kibana

* prom

* vora

* kaniko-project

* com.sap.bds.docker


//TODO list of namespaces


=== SUSE Enterprise Storage

//SAP Data Hub 2 installation on premise requires SUSE Enterprise Storage 5 or higher.
//Follow the installation documentation to create SUSE Enterprise Storage.
//Create a storage pool on your SES for the use with SAP Data Hub 2.
//You can also leverage the S3 API to create buckets on SES that can be used from applications from within SAP Data Hub 2.


An SAP Data Hub 2 installation on premise requires SUSE Enterprise Storage 5 or higher.
If you plan to use SUSE Enterprise Storage not only for your Kubernetes dynamic storage class but also for your Kubernetes Control plan, virtualized or not, reserve enough resources to address `etcd` requirements regarding
link:https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md[etcd Hardware].

The following steps show a minimalist, virtualized, test-oriented deployment of SUSE Enterprise Storage 5.5.

In the following example, we are going to build a 4 nodes (1 admin + 3 OSD) Ceph Cluster.

Before you start, be sure to:

* Collect your SUSE Linux Enterprise Server 12 SP3 and SUSE Enterprise Storage registration code from https://scc.suse.com. Alternatively, have an SMT/RMT properly set up and already mirroring these products.

** link:https://scc.suse.com[SCC]
** link:https://documentation.suse.com/sles/12-SP4/html/SLES-all/book-smt.html[SMT]

image::scc-sle.png[title="SUSE Customer Center Products",480,640,scaledwidth=80%,align=center]
image::scc-ses.png[title="SUSE Customer Center Registration Code",480,640,scaledwidth=80%,align=center]

Your DNS zone should already be set. In our example, where all Data Hub components are in the same DNS zone and subnet, it should look like the following:

image::dns.png[title="DNS Server",480,640,scaledwidth=80%,align=center]

Also, to be as efficient as possible when using an interactive shell-scripted infrastructure deployment, we recommend using an advanced terminal client or multiplexer which allows addressing multiple shells at once.

image::multi-s-virtinstall.png[title="Advanced Terminal Client - Multi Shell",480,640,align=center]

. Create virtual machines:
+
----
# sudo virt-install --name ses55-admin --ram 16384 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-admin.qcow2,size=40 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-admin-osd0.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-admin-osd1.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-admin-osd2.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-admin-osd3.qcow2,size=20 --vcpus 4 --os-type linux --os-variant generic --network bridge=caasp3 --graphics none --console pty,target_type=serial --location '/var/lib/libvirt/ISOS/SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso' --extra-args 'console=ttyS0,115200n8 serial autoyast-ses5=http://10.10.10.101/autoyast-ses5 ifcfg=eth0=10.18.10.200/24,10.18.10.1,10.10.10.11,suse-sap.net domain=suse-sap.net Textmode=1'

# sudo virt-install --name ses55-osd0 --ram 16384 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd0.qcow2,size=40 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd0-osd0.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd0-osd1.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd0-osd2.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd0-osd3.qcow2,size=20 --vcpus 4 --os-type linux --os-variant generic --network bridge=caasp3 --graphics none --console pty,target_type=serial --location '/var/lib/libvirt/ISOS/SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso' --extra-args 'console=ttyS0,115200n8 serial autoyast-ses5=http://10.10.10.101/autoyast-ses5 ifcfg=eth0=10.18.10.230/24,10.18.10.1,10.10.10.11,suse-sap.net domain=suse-sap.net Textmode=1'

# sudo virt-install --name ses55-osd1 --ram 16384 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd1.qcow2,size=40 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd1-osd0.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd1-osd1.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd1-osd2.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd1-osd3.qcow2,size=20 --vcpus 4 --os-type linux --os-variant generic --network bridge=caasp3 --graphics none --console pty,target_type=serial --location '/var/lib/libvirt/ISOS/SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso' --extra-args 'console=ttyS0,115200n8 serial autoyast-ses5=http://10.10.10.101/autoyast-ses5 ifcfg=eth0=10.18.10.231/24,10.18.10.1,10.10.10.11,suse-sap.net domain=suse-sap.net Textmode=1'

# sudo virt-install --name ses55-osd2 --ram 16384 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd2.qcow2,size=40 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd2-osd0.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd2-osd1.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd2-osd2.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd2-osd3.qcow2,size=20 --vcpus 4 --os-type linux --os-variant generic --network bridge=caasp3 --graphics none --console pty,target_type=serial --location '/var/lib/libvirt/ISOS/SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso' --extra-args 'console=ttyS0,115200n8 serial autoyast-ses5=http://10.10.10.101/autoyast-ses5 ifcfg=eth0=10.18.10.232/24,10.18.10.1,10.10.10.11,suse-sap.net domain=suse-sap.net Textmode=1'
----
+
image::multi-s-smt.png[title="Multi Shell SMT",480,640,align=center]

. Select the SUSE Enterprise Storage 5 Extension.
+
image::multi-s-addon.png[title="Multi Shell Extensions and Modules",480,640,align=center]

. On the hypervisor, you should also be able to route or bridge (either a traditional bridge using *brctl* or a virtual bridge) your upcoming SUSE Enterprise Storage 5.5 network segment. In our example, for simplicity, we are using the same bridge and network address as the SUSE CaaS Platform cluster, `--network bridge=caasp3`.

. In the following example, each node is powered by 16 GB of RAM, 4 VCPU, 40 GB for the root disk, 4 \* 20GB OSDB disk.
+
image::multi-s-default.png[title="Multi Shell Default System",480,640,align=center]

. NTP must be configured on each node
+
image::multi-s-ntp.png[title="Multi Shell NTP Server",480,640,align=center]

. Deselect AppArmor and unnecessary X and GNOME Patterns, but select the SUSE Enterprise Storage pattern.
+
image::multi-s-patterns.png[title="Multi Shell Patterns",480,640,align=center]

. Deactivate the firewall on the nodes.
Start the installation on all nodes.
+
image::multi-s-install.png[title="Multi Shell Installation",480,640,align=center]

. When the nodes are rebooted, log in and finish the network/host name and NTP configurations so that `hostname -f` returns the FQDN of the nodes and `ntpq -p` returns a stratum less than 16.
+
image::multi-s-hostname-ntp.png[title="Multi Shell Host Name",480,640,align=center]

. Using *ssh-keygen* and then *ssh-copy-id*, send your SUSE Enterprise Storage admin node SSH public key to all other nodes.

. Verify that the drives you are going to allocate for SUSE Enterprise Storage OSDs are clean by wiping them.
For more information, see
link:https://documentation.suse.com/ses/5.5/html/ses-all/ceph-install-saltstack.html#ceph-install-stack[SES Deployment Guide, section 4.3, step 12, Wipe disk].

. Install `salt-minion` on all nodes (including the administration node).

. On the administration node (in our example, `ses55-admin.suse-sap.net`), additionally install `salt-master` and `deepsea`.

. Restart `salt-minion` on all nodes and `salt-master` on the administration node.
+
image::multi-s-salt-install-restart.png[title="Multi Shell Salt Installation",480,640,align=center]

. Accept related pending Salt keys.
+
image::salt-key.png[title="Salt Keys",480,640,scaledwidth=80%,align=center]
+
Verify that `/srv/pillar/ceph/master_minion.sls` points to your administration node. In our example, it contains our `salt-master` `FQDN: master_minion: ses55-admin.suse-sap.net`.

. Prepare the cluster:
+
----
# salt-run state.orch ceph.stage.0
----
+
image::ceph-stage-0.png[title="Ceph Stage 0",480,640,scaledwidth=70%,align=center]

. Collect information about the nodes:
+
----
# salt-run state.orch ceph.stage.1
----
+
image::ceph-stage-1.png[title="Ceph Stage 1",480,640,scaledwidth=80%,align=center]

. Adapt the file  `/srv/pillar/ceph/proposals/policy.cfg` to your needs.
+
In our example, where the only deployed service is OpenAttic, it contains the following information:
+
----
cluster-ceph/cluster/ses55-osd2.suse-sap.net.sls
config/stack/default/ceph/cluster.yml
config/stack/default/global.yml
profile-default/cluster/ses55-admin.suse-sap.net.sls
profile-default/cluster/ses55-osd0.suse-sap.net.sls
profile-default/cluster/ses55-osd1.suse-sap.net.sls
profile-default/cluster/ses55-osd2.suse-sap.net.sls
profile-default/stack/default/ceph/minions/ses55-admin.suse-sap.net.yml
profile-default/stack/default/ceph/minions/ses55-osd0.suse-sap.net.yml
profile-default/stack/default/ceph/minions/ses55-osd1.suse-sap.net.yml
profile-default/stack/default/ceph/minions/ses55-osd2.suse-sap.net.yml
role-admin/cluster/ses55-admin.suse-sap.net.sls
role-admin/cluster/ses55-osd0.suse-sap.net.sls
role-admin/cluster/ses55-osd1.suse-sap.net.sls
role-admin/cluster/ses55-osd2.suse-sap.net.sls
role-master/cluster/ses55-admin.suse-sap.net.sls
role-mgr/cluster/ses55-osd0.suse-sap.net.sls
role-mgr/cluster/ses55-osd1.suse-sap.net.sls
role-mgr/cluster/ses55-osd2.suse-sap.net.sls
role-mon/cluster/ses55-osd0.suse-sap.net.sls
role-mon/cluster/ses55-osd1.suse-sap.net.sls
role-mon/cluster/ses55-osd2.suse-sap.net.sls
role-openattic/cluster/ses55-admin.suse-sap.net.sls
----

. Prepare the final state of the configuration files set:
+
----
# salt-run state.orch ceph.stage.2
----
+
image::ceph-stage-2.png[title="Ceph Stage 2",480,640,scaledwidth=80%,align=center]

. You can now safely deploy your configuration:
+
----
# salt-run state.orch ceph.stage.3
----
+
image::ceph-stage-3.png[title="Ceph Stage 3",480,640,scaledwidth=70%,align=center]

. When the stage 3 has been successfully passed, check the cluster health to insure that everything is running properly:
+
----
# ceph -s
----
+
image::ceph-health.png[title="Ceph Health",480,640,scaledwidth=80%,align=center]

. To benefit from the OpenAttic Web UI you have to initiate the *ceph.stage.4* which will install the OpenAttic service:
+
----
# salt-run state.orch ceph.stage.4
----
+
image::ceph-stage-4.png[title="Ceph Stage 4",480,640,scaledwidth=80%,align=center]

. You can now manage your cluster through the Web UI
+
image::openattic-dash.png[title="SUSE Enterprise Storage WebUI",480,640,scaledwidth=80%,align=center]

. To provide a SAP Data Hub RBD device, create a pool for it:
+
image::openattic-pool.png[title="Ceph Pool",480,640,scaledwidth=80%,align=center]

. Now provide access to this pool through an RBD device.
+
image::openattic-rbd.png[title="Ceph RBD",480,640,scaledwidth=80%,align=center]

. You can now proceed to <<sec-prerequisite-cluster>>.

// === Troubleshooting

// Standard SUSE Best Practices includes
== Legal Notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
include::common_gfdl1.2_i.adoc[]
