<?xml version="1.0" encoding="UTF-8"?>
<!--<?oxygen RNGSchema="http://www.oasis-open.org/docbook/xml/5.0/rng/docbook.rng" type="xml"?>-->
<!DOCTYPE article [
<!ENTITY % entity SYSTEM "entity-decl.ent">
%entity;
]>

<article role="sbp" xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="art-sbp-amdepyc3-sles15sp3"
  xml:lang="en">

  <info>
    <title>Optimizing Linux for AMD EPYC™ 9005 Series Processors with SUSE Linux Enterprise 15
      SP6</title>
    <productname>SUSE Linux Enterprise Server</productname>
    <productnumber>15 SP6</productnumber>
    <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
      <dm:bugtracker>
        <dm:url>https://github.com/SUSE/suse-best-practices/issues/new</dm:url>
        <dm:product>Optimizing Linux for AMD EPYC™ 9005 Series Processors with SUSE Linux Enterprise
          15 SP6</dm:product>
      </dm:bugtracker>
      <dm:editurl>https://github.com/SUSE/suse-best-practices/edit/main/xml/</dm:editurl>
    </dm:docmanager>

    <meta name="series">SUSE Best Practices</meta>
<!--    <meta name="type">Best Practices</meta>-->
    <meta name="category">
      <phrase>Tuning &amp; Performance</phrase>
    </meta>
    <meta name="task">
      <phrase>Configuration</phrase>
    </meta>
    <meta name="title">Optimizing Linux for AMD EPYC™ 9005 Series Processors with SUSE Linux Enterprise
      15 SP6</meta>
    <meta name="description">Overview of the AMD EPYC* 9005 Series Processors and tuning of
      computational-intensive workloads on SUSE Linux Enterprise Server 15 SP6.</meta>
    <meta name="productname">
      <productname version="15 SP6">SLES</productname>
      <productname version="9005">AMD EPYC*</productname>
    </meta>

    <meta name="platform">SUSE Linux Enterprise 15 SP6</meta>
    <meta name="platform">AMD EPYC™ 9005 Series Processors</meta>

    <authorgroup>
      <author>
        <personname>
          <firstname>Yousaf</firstname>
          <surname>Kaukab</surname>
        </personname>
        <affiliation>
          <jobtitle>Engineering Manager</jobtitle>
          <orgname>SUSE</orgname>
        </affiliation>
      </author>
      <author>
        <personname>
          <firstname>Mel</firstname>
          <surname>Gorman</surname>
        </personname>
        <affiliation>
          <jobtitle>Senior Kernel Engineer</jobtitle>
          <orgname>SUSE</orgname>
        </affiliation>
      </author>
      <author>
        <personname>
          <firstname>Tony</firstname>
          <surname>Jones</surname>
        </personname>
        <affiliation>
          <jobtitle>Senior Kernel Engineer</jobtitle>
          <orgname>SUSE</orgname>
        </affiliation>
      </author>
      <author>
        <personname>
          <firstname>Martin</firstname>
          <surname>Jambor</surname>
        </personname>
        <affiliation>
          <jobtitle>Tool Chain Developer</jobtitle>
          <orgname>SUSE</orgname>
        </affiliation>
      </author>
      <author>
        <personname>
          <firstname>Dario</firstname>
	        <surname>Faggioli</surname>
        </personname>
        <affiliation>
          <jobtitle>Virtualization Specialist</jobtitle>
          <orgname>SUSE</orgname>
        </affiliation>
      </author>
      <author>
        <personname>
          <firstname>Kim</firstname>
          <surname>Naru</surname>
        </personname>
        <affiliation>
          <jobtitle>Engineering Manager</jobtitle>
          <orgname>AMD</orgname>
        </affiliation>
      </author>
      <!--  <editor>
        <orgname></orgname>
        </editor>
        <othercredit>
        <orgname></orgname>
        </othercredit>-->
    </authorgroup>

    <cover role="logos">
      <mediaobject>
        <imageobject>
          <imagedata fileref="suse.svg" width="4em"/>
        </imageobject>
        <textobject><phrase>SUSE logo</phrase></textobject>
      </mediaobject>
      <!--   <mediaobject>
        <imageobject>
        <imagedata fileref="amd.jpg"/>
        </imageobject>
        </mediaobject>-->
    </cover>
    
    
    <revhistory xml:id="rh-art-sbp-amdepyc4-sles15sp4">
      <revision>
        <date>2025-04-24</date>
        <revdescription>
          <para> Virtualization section added</para>
        </revdescription>
      </revision>     
      <revision>
        <date>2024-11-15</date>
        <revdescription>
          <para> First version of AMD EPYC 5 processors document</para>
        </revdescription>
      </revision>
    </revhistory>
    

    <abstract>

      <para>The document at hand provides an overview of both the AMD EPYC™ 9005 Series
        Processors based on Zen5 and Zen5c cores. It details how some
        computational-intensive workloads can be tuned on SUSE Linux Enterprise Server
        15 SP6.</para>

      <para><emphasis role="strong">Disclaimer: </emphasis></para>
      <para>Documents published as part of
        the SUSE Best Practices series have been contributed voluntarily by SUSE employees
        and third parties. They are meant to serve as examples of how particular actions
        can be performed. They have been compiled with utmost attention to detail. However,
        this does not guarantee complete accuracy. SUSE cannot verify that actions described
        in these documents do what is claimed or whether actions described have unintended
        consequences. SUSE LLC, its affiliates, the authors, and the translators may not
        be held liable for possible errors or the consequences thereof.
      </para>

    </abstract>
  </info>

  <sect1 xml:id="sec-overview">
    <title>Overview</title>

    <para>The AMD EPYC 9005 Series Processor is the 5th generation of the AMD EPYC server
      class processor family. It is based on the Zen 5 microarchitecture introduced in 2024.
      AMD EPYC 9005 Series Processors based on Zen5 cores support up to 128 cores (256 threads) whereas
      AMD EPYC 9005 Series Processors based on Zen5c cores support up to 192 cores (384 threads). Both support 12
      memory channels per socket. At the time of writing, 1-socket and 2-socket models are expected
      to be available from Original Equipment Manufacturers (OEMs) in 2024. This document provides
      an overview of the AMD EPYC 9005 Series Processors based on Zen5 cores and how computational-intensive
      workloads can be tuned on SUSE Linux Enterprise Server 15 SP6. Additional details about the
      AMD EPYC 9005 Series Processors based on Zen5c cores are provided where appropriate.</para>

  </sect1>

  <sect1 xml:id="sec-epyc-architecture">
    <title>AMD EPYC 9005 Series Processor (Zen5 cores) architecture</title>

    <para><emphasis role="italic">Symmetric multiprocessing (SMP)</emphasis> systems are those that
      contain two or more physical processing cores. Each core may have two threads if <emphasis role="italic">Symmetric
            multithreading (SMT)</emphasis> is enabled, with some resources being shared between SMT siblings. To
      minimize access latencies, multiple layers of caches are used with each level being larger but
      with higher access costs. Cores may share different levels of cache which should be considered
      when tuning for a workload.</para>

    <para>Historically, a single socket contained several cores sharing a hierarchy of caches and
      memory channels and multiple sockets were connected via a memory interconnect. Modern
      configurations may have multiple dies as a <emphasis role="italic">Multi-Chip Module
        (MCM)</emphasis> with one set of interconnects within the socket and a separate interconnect
      for each socket. This means that some CPUs and memory are faster to access
      than others depending on the <quote>distance</quote>. This should be considered when tuning
      for <emphasis role="italic">Non-Uniform Memory Architecture (NUMA)</emphasis> as all memory
      accesses may not reference local memory incurring a variable access penalty.</para>

    <para>The 5th Generation AMD EPYC Processor has an MCM design with up to seventeen dies
      on each package. From a topology point of view, this is significantly different to the
      1st Generation AMD EPYC Processor design. However, it is similar to the 4th Generation AMD EPYC
      Processor other than the increase in die count. One die is a central IO die through
      which all off-chip communication passes through. The basic building block of a compute
      die is an eight-core Core CompleX (CCX) with its own L1-L3 cache hierarchy. Similar to
      the 4th Generation AMD EPYC Processor, one Core Complex Die (CCD) consists of one CCX
      connected via an Infinity Link to the IO die, as opposed to two CCXs used in the 2nd
      Generation AMD EPYC Processor. This allows direct communication within a CCD instead
      of using the IO link maintaining reduced communication and memory access latency.
      A 128-core 5th Generation AMD EPYC Processor socket therefore consists of 16 CCDs
      consisting of 16 CCXs (containing 8 cores each) or 128 cores in total (256 threads
      with SMP enabled) with one additional IO die for 17 dies in total. This is a large
      increase in the core count relative to the 4th Generation AMD EPYC Processor.</para>

    <para>Both the 4th and 5th Generation AMD EPYC Processors potentially have a larger
      L3 cache. In a standard configuration, a 5th Generation AMD EPYC Processor has
      32MB L3 cache. Some CPU chips may also include an AMD V-Cache expansion that can
      triple the size of the L3 cache. This potentially provides a major performance boost
      to applications as more active data can be stored in low-latency cache. The exact
      performance impact is variable, but any memory-intensive workload should benefit from
      having a lower average memory access latency because of a larger cache.</para>

    <para>Communication between the chip and memory happens via the IO die. Each CCD has one
      dedicated Infinity Fabric link to the IO die. The practical consequence of this architecture
    versus the 1st Generation AMD EPYC Processor is that the topology is simpler.
    The first generation had separate memory channels per die and links between dies giving two
    levels of NUMA distance within a single socket and a third distance when communicating between sockets. This meant
      that a two-socket machine for EPYC had 4 NUMA nodes (3 levels of NUMA distance). The
      2nd Generation AMD EPYC Processor has only 2 NUMA nodes (2 levels of NUMA distance)
      which makes it easier to tune and optimize. The NUMA distances are the same for the
      3rd, 4th and 5th Generation AMD EPYC Processors.</para>

    <para>The IO die has a total of 12 memory controllers supporting
      DDR5 <emphasis role="italic">Dual Inline Memory Modules (DIMMs)</emphasis> with the
      maximum supported speed expected to be DDR5-6000 at the time of writing. This implies
      a peak channel bandwidth of 48 GB/sec or 576 GB/sec total throughput across a
      socket. The exact bandwidth depends on the DIMMs selected, the number of memory channels
      populated, how cache is used and the efficiency of the application. Where possible,
      all memory channels should have a DIMM installed to maximize memory bandwidth.</para>

    <para>While the topologies and basic layout is similar between the 4th and 5th
      Generation AMD EPYC Processors, there are several micro-architectural
      differences. The <emphasis role="italic">Instructions Per Cycle (IPC)</emphasis> has
      improved by 17% on average for enterprise and cloud workloads and 37% higher in AI and
      high performance computing (HPC), although the exact improvement is workload-dependent.
      The improvements result from a variety of factors including, increased cache bandwidth, a larger L1d cache,
      improvements in branch prediction, wider front-end, increased number of Arithmetic
      Logic Units (ALUs), increased floating point throughput, support for AVX-512 with 512-bit data-path.
      The degree to which these changes affect performance varies between applications.
      </para>

    <para>Power management on the links is careful to minimize the amount of power
      required. If the links are idle, the power may be used to boost the frequency of
      individual cores. Hence, minimizing access is not only important from a memory
      access latency point of view, but it also has an impact on the speed of individual
      cores.</para>

    <para>There are 128 IO lanes supporting PCIe Gen 5.0 per socket. Lanes can be
      used as Infinity links, PCI Express links, SATA links (maximum 32 links) or CXL 2.0 links
      (maximum 64 links). The exact number of PCIe 5.0 and configuration links vary by chip and
      motherboard. This allows very large IO configurations and a high degree of flexibility,
      given that either IO bandwidth or the bandwidth between sockets can be optimized,
      depending on the OEM requirements. The most likely configuration is that the number
      of PCIe links will be the same for 1- and 2-socket machines, given that some lanes per
      socket will be used for inter-socket communication. While some links must be used
      for inter-socket communication, adding a socket does not compromise the number
      of available IO channels. The exact configuration used depends on the platform.</para>

  </sect1>

  <sect1 xml:id="sec-epyc9005-topology">
    <title>AMD EPYC 9005 Series Processor (Zen5 cores) topology</title>

    <para><xref linkend="fig-epyc-topology"/> below shows the topology of an example two socket machine
      with a fully populated memory configuration generated by the <package>lstopo</package>
      tool.</para>

    <figure xml:id="fig-epyc-topology">
      <title>AMD EPYC 9005 Series Processors based on Zen5 cores Topology</title>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="amd-epyc-5-topology.png" width="100%" format="PNG"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="amd-epyc-5-topology.png" width="100%" format="PNG"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>This tool is part of the <package>hwloc-gui</package> package. The two <quote>packages</quote>
      correspond to each socket. The CCXs consisting of 8 cores (16 threads) each should
      be clear, as each CCX has one L3 cache and each socket has 16 CCXs resulting in 128
      cores (256 threads). Not obvious are the links to the IO die, but the IO die should
      be taken into account when splitting a workload to optimize bandwidth to memory. In
      this example, the IO channels are not heavily used, but the focus will be on CPU and
      memory-intensive loads. If optimizing for IO, it is recommended that, where possible,
      the workload is located on the nodes local to the IO channel.</para>

    <para>The computer output below shows a conventional view of the topology using the
      <package>numactl</package> tool which is slightly edited for clarity. The CPU IDs
      that map to each node are reported on the <quote>node X cpus:</quote> lines. They note
      the NUMA distances on the table at the bottom of the computer output. Node 0 and node
      1 are a distance of 32 apart as they are on separate sockets. The distance is not
      a guarantee of the access latency, it is an estimate of the relative difference. The
      general interpretation of this distance would suggest that a remote node is 3.2 times
      longer than a local memory access but the actual latency cost can be different.  </para>

    <screen>epyc:~ # numactl --hardware
node 0 cpus: 0 .. 127 256 .. 383
node 0 size: 385996 MB
node 0 free: 384139 MB
node 1 cpus: 128 .. 255 384 .. 511
node 1 size: 386740 MB
node 1 free: 385440 MB
node distances:
node   0   1
  0:  10  32
  1:  32  10
    </screen>

    <para>Note that the two sockets displayed are masking some details. There
       are multiple CCDs and multiple channels meaning that there are slight differences
       in access latency even to <quote>local</quote> memory. If an application is so sensitive to
       latency that it needs to be aware of the precise relative distances, then the
       <emphasis role="italics">Nodes Per Socket (NPS)</emphasis> value can be adjusted
       in the BIOS. If adjusted, <package>numactl</package> will show additional nodes
       and the relative distances between them.</para>

    <para>Finally, the cache topology can be discovered in a variety of fashions. In
      addition to <package>lstopo</package> which can provide the information, the
      level, size and ID of CPUs that share cache can be identified from the files under
      <filename>/sys/devices/system/cpu/cpuN/cache</filename>.
    </para>

  </sect1>

  <sect1 xml:id="sec-memory-zen5c-variant">
    <title>AMD EPYC 9005 Series Processors (Zen5c cores)</title>

    <para>The AMD EPYC 9005 Series Processors based on Zen5c cores launched in 2024. While the fundamental
    microarchitecture is based on the <quote>Zen 5</quote> compute core, it is optimized for density and efficiency.
    Its physical layout takes less space and is designed to deliver more performance per watt.
    There are some other important differences between it and the AMD EPYC 9005 Series Processors based on Zen5 cores. Both
    processors are socket-compatible, have the same number of memory channels and the
    same number of I/O lanes. This means that the processors may be used interchangeably
    on the same platform with the same limitation that dual-socket configurations must
    use identical processors. Both processors use the same <emphasis>Instruction Set
    Architecture (ISA)</emphasis>. This means that code optimized for one processor will run
    without modification on the other.</para>

    <para>Despite the compatible ISA, the processors are physically different using a
    manufacturing process focused on increased density for both the CPU core and the
    physical cache. The L1 and L2 caches have the same capacity. The L3 cache capacity per core is half
    the capacity of the AMD EPYC 9005 Series Processors based on Zen5 cores as twice as many cores are placed on
    each CCD. The basic CCX structure in both processors is similar but each CCD with Zen5c
    has 16 cores instead of 8. While 16 CCDs (with 1 CCX each) with Zen5 cores can be placed in a package, only
    12 CCDs with Zen5c cores (each containing 16 cores) can be placed in a package. This increases the maximum number of cores per socket from 128
    cores to 192. Finally, the <emphasis>Thermal Design Points (TDPs)</emphasis> differ
    for Zen5c cores, with different frequency scaling limits
    and generally a lower peak frequency. While each individual Zen5c core may achieve less peak
    performance than the Zen5 core, the total peak compute
    throughput available is higher because of the increased number of cores.</para>

    <para>The intended use case and workloads determine which processor is superior. The
    key advantage of the AMD EPYC 9005 Series Processors based on Zen5c cores is packing more cores within
    the same socket. This may benefit cloud or hyperscale environments in that more
    containers or virtual machines can use uncontested CPUs for their workloads within
    the same physical machine. As a result, physical space in data centers can potentially
    be reduced. It may also benefit some HPC workloads that are primarily CPU and memory bound.
    For example, some HPC workloads scale to the number of available cores working on data sets that
    are too large to fit into a typical cache. For such workloads, the AMD EPYC 9005 Series Processors based on Zen5c cores may be ideal.</para>

  </sect1>

  <sect1 xml:id="sec-memory-cpu-binding">
    <title>Memory and CPU binding</title>

    <para>NUMA is a scalable memory architecture for multiprocessor systems that can reduce
      contention on a memory channel. A full discussion on tuning for NUMA is beyond the scope for
      this document. But the document <quote>A NUMA API for Linux</quote> at <link
        xlink:href="https://halobates.de/numaapi3.pdf"></link> provides a valuable introduction.</para>

    <para>The default policy for programs is the <quote>local policy</quote>. A program which calls
        <command>malloc()</command> or <command>mmap()</command> reserves virtual address space but
      does not immediately allocate physical memory. The physical memory is allocated the first time
      the address is accessed by any thread and, if possible, the memory will be local to the
      accessing CPU. If the mapping is of a file, the first access may have occurred at any time in
      the past so there are no guarantees about locality.</para>

    <para>When memory is allocated to a node, it is less likely to move if a thread changes to a CPU
      on another node or if multiple programs are remote-accessing the data unless <emphasis
      role="italic">Automatic NUMA Balancing (NUMAB)</emphasis> is enabled (see <xref linkend="sec-auto-numa-balancing"/>).
      When NUMAB is enabled, unbound process accesses are sampled. If there are enough remote accesses, then the data will
      be migrated to local memory. This mechanism is not perfect and incurs overhead of its own.
      This can be important for performance for thread and process migrations between nodes to be
      minimized and for memory placement to be carefully considered and tuned.</para>

    <para>The <package>taskset</package> tool is used to set or get the CPU affinity for new or
      existing processes. An example usage is to confine a new process to CPUs local to one node.
      Where possible, local memory will be used. But if the total required memory is larger than the
      node, then remote memory can still be used. In such configurations, it is recommended to size
      the workload such that it fits in the node. This avoids that any of the data is being paged
      out when <package>kswapd</package> wakes to reclaim memory from the local node.</para>

    <para><package>numactl</package> controls both memory and CPU policies for processes that it
      launches and can modify existing processes. In many respects, the parameters are easier to
      specify than <package>taskset</package>. For example, it can bind a task to all CPUs on a
      specified node instead of having to specify individual CPUs with <package>taskset</package>.
      Most importantly, it can set the memory allocation policy without requiring application
      awareness.</para>

    <para>Using policies, a preferred node can be specified where the task will use that node if
      memory is available. This is typically used in combination with binding the task to CPUs on
      that node. If a workload’s memory requirements are larger than a single node and predictable
      performance is required, then the <quote>interleave</quote> policy will round-robin
      allocations from allowed nodes. This gives suboptimal but predictable access latencies to main
      memory. More importantly, interleaving reduces the probability that the operating system (OS)
      will need to reclaim any data belonging to a large task.</para>

    <para>Further improvements can be made to access latencies by binding a workload to
      a single CCD within a node. As L3 caches are shared within a CCD on the 3rd, 4th
      and 5th Generation AMD EPYC Processors, binding a workload to a CCD avoids L3 cache
      misses caused by workload migration. This is an important difference from the
      2nd Generation AMD EPYC Processor which favored binding within a CCX.</para>

    <para>In most respects, the guidance for optimal bindings for cache and nodes remains
      the same between the 3rd, 4th and 5th Generation AMD EPYC Processors. However, with SUSE
      Linux Enterprise 15 SP6, the necessity to bind specifically to the L3 cache for
      optimal performance is relaxed. The CPU scheduler in SUSE Linux Enterprise 15 SP6 has
      superior knowledge of the cache topology of all generations of the AMD EPYC Processors
      and how to balance load between CPU caches, NUMA nodes and memory channels.</para>

      <note>
        <title>CPU Scheduler Awareness of Cache Topology</title>

        <para>With SUSE Linux Enterprise 15 SP6 having superior knowledge of the CPU cache
          topology and how to balance load, tuning specifically has a smaller impact to performance
          for a given workload. This is <emphasis role="strong">not</emphasis> a limitation of the
          operating system. It is a side-effect of the baseline performance being improved on
          AMD EPYC Processors in general.</para>

      </note>

    <para>See examples below on how <package>taskset</package> and <package>numactl</package> can
      be used to start commands bound to different CPUs depending on the topology.</para>

    <screen># Run a command bound to CPU 1
epyc:~ # taskset -c 1 [command]

# Run a command bound to CPUs belonging to node 0
epyc:~ # taskset -c `cat /sys/devices/system/node/node0/cpulist` [command]

# Run a command bound to CPUs belonging to nodes 0 and 1
epyc:~ # numactl –cpunodebind=0,1 [command]

# Run a command bound to CPUs that share L3 cache with cpu 1
epyc:~ # taskset -c `cat /sys/devices/system/cpu/cpu1/cache/index3/shared_cpu_list` [command]</screen>


    <sect2 xml:id="sec-tuning-without-binding">
      <title>Tuning for local access without binding</title>

      <para>The ability to use local memory where possible and remote memory if necessary is
        valuable. But there are cases where it is imperative that local memory always be used. If
        this is the case, the first priority is to bind the task to that node. If that is not
        possible, then the command <command>sysctl vm.zone_reclaim_mode=1</command> can be used to
        aggressively reclaim memory if local memory is not available. </para>

      <note>
        <title>Potential Hazard with <filename>vm.zone_reclaim_mode</filename></title>

        <para>While this option is good from a locality perspective, it can incur high costs because
          of stalls related to reclaim and the possibility that data from the task will be
          reclaimed. Treat this option with a high degree of caution and testing.</para>
      </note>
    </sect2>

    <sect2 xml:id="sec-hazards-cpu-binding">
      <title>Hazards with CPU binding</title>

      <para>There are three major hazards to consider with CPU binding.</para>

      <para>The first to watch for is remote memory nodes being used when the process is not
        allowed to run on CPUs local to that node. The scenarios when this can occur
        are outside the scope of this paper. However, a common reason is an IO-bound thread
        communicating with a kernel IO thread on a remote node bound to the IO controller.
        In such a setup, the data buffers managed by the application are stored in remote memory
        incurring an access cost for the IO.</para>

      <para>While tasks may be bound to CPUs, the resources they are accessing, such as network or
        storage devices, may not have interrupts routed locally. <package>irqbalance</package>
        generally makes good decisions. But in cases where the network or IO is extremely
        high-performance or the application has very low latency requirements, it may be necessary to
        disable <package>irqbalance</package> using <command>systemctl</command>. When that is done,
        the IRQs for the target device need to be routed manually to CPUs local to the target
        workload for optimal performance.</para>

      <para>The second is that guides about CPU binding tend to focus on binding to a
        single CPU. This is not always optimal when the task communicates with other threads,
        as fixed bindings potentially miss an opportunity for the processes to use idle
        CPUs sharing a common cache. This is particularly true when dispatching IO, be it
        to disk or a network interface, where a task may benefit from being able to migrate
        close to the related threads. It also applies to pipeline-based communicating threads
        for a computational workload. Hence, focus initially on binding to CPUs sharing L3
        cache. Then consider whether to bind based on an L1/L2 cache or a single CPU using the
        primary metric of the workload to establish whether the tuning is appropriate. </para>

      <para>The final hazard is similar: if many tasks are bound to a smaller set of CPUs,
        then the subset of CPUs could be oversaturated even though there is spare CPU
        capacity available.</para>

    </sect2>

    <sect2 xml:id="sec-cpusets-memory-control-groups">
      <title>cpusets and memory control groups</title>

      <para><emphasis role="italic">cpusets</emphasis> are ideal when multiple workloads must be
        isolated on a machine in a predictable fashion. cpusets allow a machine to be partitioned
        into subsets. These sets may overlap, and in that case they suffer from similar problems as
        CPU affinities. In the event there is no overlap, they can be switched to
          <quote>exclusive</quote> mode which treats them completely in isolation with relatively
        little overhead. Similarly, they are well suited when a primary workload must be protected
        from interference because of low-priority tasks. In such cases the low-priority tasks can be
        placed in a cpuset. The caveat with cpusets is that the overhead is higher than using
        scheduler and memory policies. Ordinarily, the accounting code for cpusets is completely
        disabled. But when a single cpuset is created, there is a second layer of checks against
        scheduler and memory policies.</para>

      <para>Similarly, <package>memcg</package> can be used to limit the amount of memory that can be
        used by a set of processes. When the limits are exceeded, the memory will be reclaimed
        by tasks within <package>memcg</package> directly without interfering with any other tasks.
        This is ideal for ensuring there is no inference between two or more sets of tasks. Similar
        to cpusets, there is some management overhead incurred. This means, if tasks can simply be
        isolated on a NUMA boundary, then this is preferred from a performance perspective. The major
        hazard is that, if the limits are exceeded, then the processes directly stall to reclaim the
        memory which can incur significant latencies. </para>

      <note>
        <para>Without <package>memcg</package>, when memory gets low, the global reclaim daemon does
          work in the background and if it reclaims quickly enough, no stalls are incurred. When
          using <package>memcg</package>, observe the <package>allocstall</package> counter in
            <filename>/proc/vmstat</filename> as this can detect early if stalling is a
          problem.</para>

      </note>

    </sect2>
  </sect1>

  <sect1 xml:id="sec-hp-storage-interrupt-affinity">
    <title>High performance storage devices and interrupt affinity</title>

    <para>High performance storage devices like <emphasis role="italic">Non-Volatile Memory Express
        (NVMe)</emphasis> or <emphasis role="italic">Serial Attached SCSI (SAS)</emphasis>
      controller are designed to take advantage of parallel IO submission. These devices typically
      support a large number of submit and receive queues, which are tied to <emphasis role="italic"
        >MSI-X</emphasis> interrupts. Ideally, these devices should provide as many MSI-X vectors as
      there are CPUs in the system. To achieve the best performance, each MSI-X vector should be assigned
      to an individual CPU.</para>

  </sect1>

  <sect1 xml:id="sec-auto-numa-balancing">
    <title>Automatic NUMA balancing</title>

    <para>Automatic NUMA Balancing (NUMAB) is a feature that identifies and relocates
      pages that are being accessed remotely for applications that are not NUMA-aware. There
      are cases where it is impractical or impossible to specify policies. In such cases, the
      balancing should be sufficient for throughput-sensitive workloads, but on occasion,
      NUMAB may be considered hazardous as it incurs a cost. Under ideal conditions,
      an application is NUMA aware and uses memory policies to control what memory is
      accessed and NUMAB simply ignores such regions. However, even if an application does
      not use memory policies, it is possible that the application still accesses mostly
      local memory and NUMAB adds overhead confirming that accesses are local which is an
      unnecessary cost. For latency-sensitive workloads, the sampling for NUMA balancing may
      be too unpredictable and would prefer to incur the remote access cost or interleave
      memory instead of using NUMA. The final corner case where NUMA balancing is a hazard
      happens when the number of runnable tasks always exceeds the number of CPUs in a
      single node. In this case, the load balancer (and potentially affine wakes) may
      pull tasks away from the preferred node as identified by NUMAB resulting in
      excessive sampling and migrations.</para>

    <para>If the workloads can be manually optimized with policies, then consider disabling NUMAB
      by specifying <command>numa_balancing=disable</command> on the kernel
      command line or via <command>sysctl kernel.numa_balancing</command>. The same applies if
      it is known that the application is mostly accessing local memory.</para>

    <note>
      <title>Changes to Automatic NUMA Balancing</title>

      <para>While a disconnect between CPU Scheduler and NUMA Balancing placement decisions
        still potentially exists in SUSE Linux Enterprise 15 SP6 when the machine is heavily
        overloaded, the impact is much reduced relative to previous releases for most
        scenarios. The placement decisions made by the CPU Scheduler and NUMA Balancing
        are now coupled. Situations where the CPU scheduler and NUMA Balancing make
        opposing decisions are relatively rare.</para>

    </note>


  </sect1>

  <sect1 xml:id="sec-evaluating-_workloads">
    <title>Evaluating workloads</title>

    <para>The first and foremost step when evaluating how a workload should be tuned is
      to establish a primary metric such as latency, throughput, operations per second
      or elapsed time. When each tuning step is considered or applied, it is critical
      that the primary metric be examined before conducting any further analysis. This is to avoid
      intensive focus on a relatively wrong bottleneck. Make sure that the metric is measured
      multiple times to ensure that the result is reproducible and reliable within reasonable
      boundaries. When that is established, analyze how the workload is using different system
      resources to determine what area should be the focus. The focus in this paper is on how
      CPU and memory is used. But other evaluations may need to consider the IO subsystem,
      network subsystem, system call interfaces, external libraries, etc. The methodologies
      that can be employed to conduct this are outside the scope of this paper. But the book
      <quote>Systems Performance: Enterprise and the Cloud</quote> by Brendan Gregg (see <link
      xlink:href="http://www.brendangregg.com/systems-performance-2nd-edition-book.html"></link>)
      is a recommended primer on the subject.</para>

    <sect2 xml:id="sec-cpu-utilization-saturation">
      <title>CPU utilization and saturation</title>

      <para>Decisions on whether to bind a workload to a subset of CPUs require that the CPU
        utilization and any saturation risk is known. Both the <command>ps</command> and
          <command>pidstat</command> commands can be used to sample the number of threads in a
        system. Typically, <command>pidstat</command> yields more useful information with the
        important exception of the run state. A system may have many threads, but if they are idle,
        they are not contributing to utilization. The <command>mpstat</command> command can
        report the utilization of each CPU in the system. </para>

      <para>High utilization of a small subset of CPUs may be indicative of a single-threaded
        workload that is pushing the CPU to the limits and may indicate a bottleneck. Conversely,
        low utilization may indicate a task that is not CPU-bound, is idling frequently or is
        migrating excessively. While each workload is different, load utilization of CPUs may show a
        workload that can run on a subset of CPUs to reduce latencies because of either migrations
        or remote accesses. When utilization is high, it is important to determine if the system
        could be saturated. The <package>vmstat</package> tool reports the number of runnable tasks
        waiting for a CPU in the <quote>r</quote> column where any value over 1 indicates that wakeup
        latencies may be incurred. While the exact wakeup latency can be calculated using trace
        points, knowing that there are tasks queued is an important step. If a system is saturated,
        it may be possible to tune the workload to use fewer threads.</para>

      <para>Overall, the initial intent should be to use CPUs from as few NUMA nodes as
        possible to reduce access latency. However, there are exceptions. The AMD EPYC 9005
        Series Processor has a large number of high-speed memory channels to main memory, so
        consider the workload thread activity. If they are cooperating threads or sharing
        data, isolate them on as few nodes as possible to minimize cross-node memory
        accesses. If the threads are completely independent with no shared data, it may
        be best to isolate them on a subset of CPUs from each node. This is to maximize
        the number of available memory channels and throughput to main memory. For some
        computational workloads, it may be possible to use hybrid models such as MPI for
        parallelization across nodes and OpenMP for threads within nodes.</para>

        <note>
          <title>Updating tuning for AMD EPYC 9005 Series Processor</title>

          <para>It is expected that tuning based on the AMD EPYC 7003 Series Processor
             will also usually perform optimally on latter series processors including AMD EPYC 9005
             series. The main consideration is to account for potential differences in L3 cache sizes
             because of AMD V-Cache if workloads are tuned specifically for cache size. Also, keep in
             mind that CPU bindings based on caches may potentially be relaxed on SUSE Linux Enterprise
             15 SP6.</para>
        </note>
    </sect2>

    <sect2 xml:id="sec-transparent-huge-pages">
      <title>Transparent Huge Pages</title>

      <para>Huge pages are a feature that can improve performance in many cases. This is
        achieved by reducing the number of page faults, the cost of translating virtual
        addresses to physical addresses because of fewer layers in the page table and
        being able to cache translations for a larger portion of memory. <emphasis
        role="italic">Transparent Huge Pages (THP)</emphasis> is supported for private
        anonymous memory that automatically backs mappings with huge pages where anonymous
        memory could be allocated as <command>heap</command>, <command>malloc()</command>,
        <command>mmap(MAP_ANONYMOUS)</command>, etc. There is also support for using THP
        pages backed by <command>tmpfs</command> which can be configured at mount time
        using the <parameter>huge=</parameter> mount option. While the THP feature has
        existed for a long time, it has evolved significantly.</para>

      <para>Many tuning guides recommend disabling THP because of problems with early
        implementations. Specifically, when the machine was running for long enough, the
        use of THP could incur severe latencies and could aggressively reclaim memory in
        certain circumstances. These problems were resolved by the time SUSE Linux
        Enterprise Server 15 SP2 was released, and this is still the case for SUSE Linux
        Enterprise Server 15 SP6. This means there are no good grounds for automatically
        disabling THP because of severe latency issues without measuring the impact. However,
        there are exceptions that are worth considering for specific workloads.</para>

      <para>Some high-end in-memory databases and other applications aggressively use
        <command>mprotect()</command> to ensure that unprivileged data is never leaked. If
        these protections are at the base page granularity, then there may be many THP splits
        and rebuilds that incur overhead. It can be identified if this is a potential
        problem by using <command>strace</command> or <command>perf trace</command> to
        detect the frequency and granularity of the system call. If they are high-frequency,
        consider disabling THP. It can also be sometimes inferred from observing the
        <command>thp_split</command> and <command>thp_collapse_alloc</command> counters
        in <filename>/proc/vmstat</filename>.</para>

      <para>Workloads that sparsely address large mappings may have a higher memory
        footprint when using THP. This could result in premature reclaim or fallback to
        remote nodes. An example would be HPC workloads operating on large sparse matrices. If
        memory usage is much higher than expected, compare memory usage with and without
        THP to decide if the trade-off is not worthwhile. This may be critical on AMD EPYC
        7003, 9004 and 9005 Series Processor given that any spillover will congest the Infinity
        links and potentially cause cores to run at a lower frequency. </para>

      <note>
        <title>Sparsely addressed memory</title>
        <para>This is specific to sparsely addressed memory. A secondary hint for this case may be
          that the application primarily uses large mappings with a much higher
          <emphasis role="strong">Virtual Size</emphasis>  (VSZ, see <xref linkend="sec-memory-utilization-saturation"/>) than <emphasis role="strong">Resident Set Size</emphasis> (RSS).
          Applications which densely address memory benefit from the use of THP by achieving greater
          bandwidth to memory.</para>
      </note>

      <para>Parallelized workloads that operate on shared buffers with thread counts
        exceeding the number of available CPUs on a single node may experience a slowdown
        with THP if the granularity of partitioning is not aligned to the huge page. The
        problem is that if a large shared buffer is partitioned on a 4K boundary, then false
        sharing may occur whereby one thread accesses a huge page locally and other threads
        access it remotely. If this situation is encountered, the granularity of sharing should
  be increased to the THP size. But if that is not possible, disabling THP is an option.</para>

      <para>Applications that are extremely latency-sensitive or must always perform in a
        deterministic fashion can be hindered by THP. While there are fewer faults, the time for
        each fault is higher as memory must be allocated and cleared before being visible. The
        increase in fault times may be in the microsecond granularity. Ensure this is a relevant
        problem as it typically only applies to extremely latency-sensitive applications. The
        secondary problem is that a kernel daemon periodically scans a process looking for
        contiguous regions that can be backed by huge pages. When creating a huge page, there is a
        window during which that memory cannot be accessed by the application and new mappings
        cannot be created until the operation is complete. This can be identified as a problem with
        thread-intensive applications that frequently allocate memory. In this case, consider
        effectively disabling <package>khugepaged</package> by setting a large value in
          <filename>/sys/kernel/mm/transparent_hugepage/khugepaged/alloc_sleep_millisecs</filename>.
        This will still allow THP to be used opportunistically while avoiding stalls when calling
          <command>malloc()</command> or <command>mmap()</command>.</para>

      <para>THP can be disabled. To do so, specify
        <command>transparent_hugepage=disable</command> on the kernel command line,
        at runtime via <filename>/sys/kernel/mm/transparent_hugepage/enabled</filename>
        or on a per-process basis by using a wrapper to execute the workload that calls
        <command>prctl(PR_SET_THP_DISABLE)</command>.</para>

    </sect2>

    <sect2 xml:id="sec-user-kernel-footprint">
      <title>User/kernel footprint</title>

      <para>Assuming an application is mostly CPU- or memory-bound, it is useful to determine if the
        footprint is primarily in user space or kernel space. This gives a hint where tuning should
        be focused. The percentage of CPU time can be measured on a coarse-grained fashion using
        <command>vmstat</command> or a fine-grained fashion using <command>mpstat</command>. If an
        application is mostly spending time in user space, then the focus should be on tuning the
        application itself. If the application is spending time in the kernel, then it should be
        determined which subsystem dominates. The <command>strace</command> or <command>perf
          trace</command> commands can measure the type, frequency and duration of system calls as
        they are the primary reasons an application spends time within the kernel. In some cases, an
        application may be tuned or modified to reduce the frequency and duration of system calls.
        In other cases, a profile is required to identify which portions of the kernel are most
        relevant as a target for tuning.</para>
    </sect2>

    <sect2 xml:id="sec-memory-utilization-saturation">
      <title>Memory utilization and saturation</title>

      <para>The traditional means of measuring memory utilization of a workload is to
        examine the <emphasis role="italic">Virtual Size (VSZ)</emphasis> and
        <emphasis role="italic">Resident Set Size (RSS)</emphasis>. This can be done by
        using either the <command>ps</command> or <command>pidstat</command> tool.
        This is a reasonable first step but is potentially misleading when shared memory
        is used and multiple processes are examined. VSZ is simply a measure of memory space reservation and
        is not necessarily used. RSS may be double accounted if it is a shared segment
        between multiple processes. The file <filename>/proc/pid/maps</filename> can be
        used to identify all segments used and whether they are private or shared. The file
        <filename>/proc/pid/smaps</filename> and <filename>/proc/pid/smaps_rollup</filename>
        reveals more detailed information including the <emphasis role="italic">Proportional
        Set Size (PSS)</emphasis>. PSS is an estimate of RSS except it is divided
        between the number of processes mapping that segment, which can give a more
        accurate estimate of utilization. Note that the <filename>smaps</filename> and
        <filename>smaps_rollup</filename> files are very expensive to read and should not be
        monitored at a high frequency. This is especially the case if workloads are using large amounts of
        address space, many threads or both. Finally, the <emphasis role="italic">Working Set
        Size (WSS)</emphasis> is the amount of active memory required to complete computations
        during an arbitrary phase of a program’s execution. It is not a value that can be
        trivially measured. But conceptually it is useful as the interaction between WSS
        relative to available memory affects memory residency and page fault rates.</para>

      <para>On NUMA systems, the first saturation point is a node overflow when the
        <quote>local</quote> policy is in effect. Given no binding of memory, when a node is
        filled, a remote node’s memory will be used transparently and background reclaim
        will take place on the local node. Two consequences of this are that remote access
        penalties will be used and old memory from the local node will be reclaimed. If the
        WSS of the application exceeds the size of a local node, then paging and re-faults
        may be incurred.</para>

      <para>The first item to identify is whether a remote node overflow occurred, which is
        accounted for in <filename>/proc/vmstat</filename> as the <command>numa_hit</command>,
        <command>numa_miss</command>, <command>numa_foreign</command>,
        <command>numa_interleave</command>, <command>numa_local</command> and
        <command>numa_other counters</command>:</para>

      <itemizedlist>
        <listitem>
          <para><command>numa_hit</command> is incremented when an allocation uses the preferred
            node where preferred may be either a local node or one specified by a memory
            policy.</para>
        </listitem>
        <listitem>
          <para><command>numa_miss</command> is incremented when an alternative node is used to
            satisfy an allocation.</para>
        </listitem>
        <listitem>
          <para><command>numa_foreign</command> is rarely useful but is accounted against a node
            that was preferred. It is a subtle distinction from <command>numa_miss</command> that is rarely
            useful.</para>
        </listitem>
        <listitem>
          <para><command>numa_interleave</command> is incremented when an interleave policy was used
            to select allowed nodes in a round-robin fashion.</para>
        </listitem>
        <listitem>
          <para><command>numa_local</command> increments when a local node is used for an allocation
            regardless of policy.</para>
        </listitem>
        <listitem>
          <para><command>numa_other</command> is used when a remote node is used for an allocation
            regardless of policy.</para>
        </listitem>
      </itemizedlist>

      <para>For the local memory policy, the <command>numa_hit</command> and
        <command>numa_miss</command> counters are the most important to pay attention
        to. An application that is allocating memory that starts incrementing the
        <command>numa_miss</command> implies that the first level of saturation has
        been reached. If monitoring the <filename>proc</filename> is undesirable, then the
        <command>numastat</command> provides the same information. If this is observed on the
        AMD EPYC 9005 Series Processor, it may be valuable to bind the application to nodes
        that represent dies on a single socket. If the ratio of hits to misses is close to 1,
        consider an evaluation of the interleave policy to avoid unnecessary reclaim.</para>

      <note>
        <title>NUMA statistics</title>

        <para>These NUMA statistics only apply at the time a physical page is allocated and are not
          related to the reference behavior of the workload. For example, if a task running on node
          0 allocates memory local to node 0, then it will be accounted for as a
            <command>node_hit</command> in the statistics. However, if the memory is shared with a
          task running on node 1, all the accesses may be remote, which is a miss from the
          perspective of the hardware but not accounted for in <filename>/proc/vmstat</filename>.
          Detecting remote and local accesses at the hardware level requires using the hardware’s
            <emphasis role="italic">Performance Monitoring Unit</emphasis> to detect. See perf-mem(1) for further details.</para>
      </note>

      <para>When the first saturation point is reached, reclaim will be active. This
        can be observed by monitoring the <command>pgscan_kswapd</command> and
        <command>pgsteal_kswapd</command> counters in <filename>/proc/vmstat</filename>. If
        this is matched with an increase in major faults or minor faults, then it may
        be indicative of severe thrashing. In this case, the interleave policy should be
        considered. An ideal tuning option is to identify if shared memory is the source
        of the usage. If this is the case, then interleave the shared memory segments. This
        can be done in some circumstances using <command>numactl</command> or by modifying
        the application directly.</para>

      <para>More severe saturation is observed if the <command>pgscan_direct</command>
        and <command>pgsteal_direct</command> counters are also increasing. These counters indicate
        that the application is stalling while memory is being reclaimed. If the application
        was bound to individual nodes, increasing the number of available nodes will
        alleviate the pressure. If the application is unbound, it indicates that the WSS
        of the workload exceeds all available memory. It can only be alleviated by tuning
        the application to use less memory or increasing the amount of RAM available.</para>

      <para>A more generalized view of resource pressure for CPU, memory and IO can be
        measured using the kernel <emphasis role="italic">Pressure
        Stall Information</emphasis> feature enabled with the command
        line <command>psi=1</command>. When enabled, proc files under
        <filename>/proc/pressure</filename> show if some or all active tasks were stalled
        recently contending on a resource. This information is not always available in
        production. But if the information is available, the memory pressure information may be used to guide
        whether a deeper analysis is necessary and which resource is the bottleneck.</para>

      <para>As before, whether to use memory nodes from one socket or two sockets depends
        on the application. If the individual processes are independent, either socket
        can be used. Where possible, keep communicating processes on the same socket
        to maximize memory throughput while minimizing the socket interconnect traffic.</para>

    </sect2>

    <sect2 xml:id="sec-other-resources">
      <title>Other resources</title>

      <para>The analysis of other resources is outside the scope of this paper. However,
        a common scenario is that an application is IO-bound. A superficial check can be made
        using the <command>vmstat</command> tool. This tool checks what percentage of CPU time
        is spent idle combined with the number of processes that are blocked and the values in
        the <emphasis role="strong">bi</emphasis> and <emphasis role="strong">bo</emphasis>
        columns. Similarly, if PSI is enabled, then the IO pressure file will show whether
        some or all active tasks are losing time because of lack of resources. Further analysis
        is required to determine if an application is IO rather than CPU- or memory-bound.
        However, this is a sufficient check to start with. </para>

    </sect2>

  </sect1>
  <sect1 xml:id="sec-power-management">
    <title>Power management</title>

    <para>Modern CPUs balance power consumption and performance through <emphasis
      role="italic" >Performance States (P-States)</emphasis>. Low utilization workloads may
      use lower P-States to conserve power while still achieving acceptable performance. When
      a CPU is idle, lower power idle states <emphasis role="italic">(C-States)</emphasis> can
      be selected to further conserve power. However, this comes with higher exit latencies
      when lower power states are selected. It is further complicated by the fact that,
      if individual cores are idle and running at low power, the additional power can be
      used to boost the performance of active cores. This means this scenario is not a
      straightforward balance between power consumption and performance. More complexity
      is added on the AMD EPYC 7003, 9004 and 9005 Series Processors whereby spare power may be
      used to boost either cores or the Infinity links.</para>

    <para>The 5th Generation AMD EPYC Processor is capable of
      making adjustments to voltage and frequency depending on the historical state of the
      CPU. There is a latency penalty when switching P-States, but the AMD EPYC 9005
      Series Processor is capable of making fine-grained adjustments to reduce the likelihood
      that the latency is a bottleneck. On SUSE Linux Enterprise
      Server 15 SP6, cpufreq subsystem uses the <command>acpi_cpufreq</command> driver by default for AMD EPYC 9005 Series Processors.
      However, this may change in the future SUSE Linux Enterprise Server releases
      as work is in progress to enable<command>amd-pstate</command> driver for AMD EPYC 9005 Series Processors.
      cpufreq subsystem allows P-States to be configured to match requested performance. However,
      this is limited in terms of the full capabilities of the hardware. It cannot boost
      the frequency beyond the maximum stated frequencies, and if a target is specified,
      then the highest frequency below the target will be used. A special case is if the
      governor is set to <emphasis role="strong">performance</emphasis>. In this situation
      the hardware will use the highest available frequency in an attempt to work quickly
      and then return to idle.</para>

    <para>What should be determined is whether power management is likely to be a factor for a
      workload. A single thread workload that is CPU-bound is likely to run at the highest
      frequency on a single core. Lastly, a workload that does not communicate heavily
      with other processes and is mostly CPU-bound is unlikely to experience any side
      effects because of power management. The exceptions are when load balancing moves
      tasks away from active CPUs if there is a compute imbalance between NUMA nodes or
      the machine is heavily overloaded.</para>

    <para>The workloads that are most likely to be affected by power management are those
      that:</para>

      <itemizedlist>
        <listitem><para>synchronously communicate between multiple threads.</para>
        </listitem>
        <listitem><para>idle frequently.</para>
        </listitem>
        <listitem><para>have low CPU utilization overall.</para>
        </listitem>
        </itemizedlist>

      <para>It will be further compounded if the threads are sensitive to wakeup latency.</para>

    <para>Power management is critical, not only for power savings, but because power saved
      from idling inactive cores can be used to boost the performance of active cores. On
      the other side, low utilization tasks may take longer to complete if the task is not
      active long enough for the CPU to run at a high frequency. In some cases, problems
      can be avoided by configuring a workload to use the minimum number of CPUs necessary
      for its active tasks. Deciding that means monitoring the power state of CPUs.</para>

    <para>The P-State and C-State of each CPU can be examined using the
      <command>turbostat</command> utility. The computer output below shows an example,
      slightly edited to fit the page, where a workload is busy on CPU 0 and other
      workloads are idle. A useful exercise is to start a workload and monitor the output
      of <command>turbostat</command> paying close attention to CPUs that have moderate
      utilization and running at a lower frequency. If the workload is latency-sensitive,
      it is grounds for either minimizing the number of CPUs available to the workload or
      configuring power management.</para>

    <screen>
Pac. Die  Core CPU Avg_M Busy% Bzy_M TSC_M IPC   IRQ    POLL C1   C2    POLL% C1%  C2%
-    -    -    -   5     0.31  1664  2737  1.36  137633 135  3475 99475 0.00  0.11 101.10
0    0    0    0   88    5.38  1638  2696  1.40  1065   0    1    924   0.00  0.00 94.66
0    0    0    256 202   5.79  3494  2696  2.68  1088   0    9    739   0.00  0.03 94.21
0    0    1    1   12    0.73  1622  2696  1.27  493    0    16   344   0.00  0.69 98.59
0    0    1    257 12    0.81  1506  2696  1.22  936    0    5    682   0.00  0.03 99.18
0    0    2    2   9     0.58  1505  2696  1.20  506    0    5    361   0.00  0.04 99.40
0    0    2    258 9     0.59  1507  2696  1.39  672    0    4    489   0.00  0.02 99.41
</screen>

      <note>
        <title>turbostat error: Too many open files</title>
        <para>Default value of the number of file descriptors that a process may allocate
          (RLIMIT_NOFILE) is set to 1024 on SLE15-SP6. <command>turbostat</command> needs to open 1028 file
          descriptors on a system with 512 CPUs. Although its possible for an application
          to increase the current limit, the version of <command>turbostat</command> that ships with SLE15-SP6
          at the time of writing is not changing this limit. As a result it fails with error
          <emphasis role="italic">open failed: Too many open files.</emphasis> A fix for this
          issue is on the way. Meanwhile, <command>turbostat</command> can be run after locally changing
          this limit by running the command <command>ulimit -n 1029</command>.
        </para>
      </note>

    <para>If tuning CPU frequency management is appropriate, the following actions can be
      taken to set the management policy to performance using the <command>cpupower</command>
      utility:</para>

    <screen>epyc:~# cpupower frequency-set -g performance
Setting cpu: 0
Setting cpu: 1
Setting cpu: 2
...</screen>

    <para>Persisting it across reboots can be done via a local <command>init</command>
      script, via <command>udev</command> or via one-shot <command>systemd</command>
      service file if necessary. Note that <command>turbostat</command>
      will still show that idling CPUs use a low frequency. The impact of the policy is
      that the highest P-State will be used as soon as possible when the CPU is active. In
      some cases, a latency bottleneck will occur because of a CPU exiting idle. If this is
      identified on the AMD EPYC 9005 Series Processor, restrict the C-state by specifying
      <command>processor.max_cstate=2</command> if lower P-States exist on the kernel command
      line. This will prevent CPUs from entering lower C-states. The availability of
      P-states can be determined with <command>cpupower idle-info</command>. It is expected
      on the AMD EPYC 9005 Series Processor that the exit latency from C1 is very low. But
      by allowing C2, it reduces interference from the idle loop injecting micro-operations
      into the pipeline and should be the best state overall. It is also possible to set
      the max idle state on individual cores using <command>cpupower idle-set</command>. If
      SMT is enabled, the idle state should be set on both siblings.</para>

  </sect1>

  <sect1 xml:id="sec-security-_mitigations">
    <title>Security mitigation</title>

    <para>On occasion, a security fix is applied to a distribution that has a performance
      impact. The most notable example is <emphasis role="strong">Meltdown</emphasis>
      and multiple variants of <emphasis role="strong">Spectre</emphasis> but includes
      others such as ForeShadow (L1TF). The AMD EPYC 9005 Series Processor is immune to the
      Meltdown variant and page table isolation is never active. However, it is vulnerable
      to a subset of Spectre variants although <emphasis role="strong">retbleed</emphasis>
      is a notable exception. The following table lists all security vulnerabilities that
      affect the 5th Generation AMD EPYC Processor. In addition, it specifies which mitigations
      are enabled by default for SUSE Linux Enterprise Server 15 SP6.</para>

    <table>
      <title>Security mitigations for AMD EPYC 9005 Series Processors</title>
      <tgroup cols="3">
        <colspec colname="col_1" colwidth="33*"/>
        <colspec colname="col_2" colwidth="33*"/>
        <colspec colname="col_3" colwidth="33*"/>
        <thead>
          <row>
            <entry>Vulnerability</entry>
            <entry>Affected </entry>
            <entry>Mitigations</entry>
          </row>
        </thead>
        <tbody>
          <row>
            <entry>
              <para>ITLB Multihit</para>
            </entry>
            <entry>
              <para>No</para>
            </entry>
            <entry>
              <para>N/A</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>L1TF</para>
            </entry>
            <entry>
              <para>No</para>
            </entry>
            <entry>
              <para>N/A</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>MDS</para>
            </entry>
            <entry>
              <para>No</para>
            </entry>
            <entry>
              <para>N/A</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>Meltdown</para>
            </entry>
            <entry>
              <para>No</para>
            </entry>
            <entry>
              <para>N/A</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>MMIO Stale Data</para>
            </entry>
            <entry>
              <para>No</para>
            </entry>
            <entry>
              <para>N/A</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>Retbleed</para>
            </entry>
            <entry>
              <para>No</para>
            </entry>
            <entry>
              <para>N/A</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>Speculative Store Bypass</para>
            </entry>
            <entry>
              <para>Yes</para>
            </entry>
            <entry>
              <para>prctl and seccomp policy</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>Spectre v1</para>
            </entry>
            <entry>
              <para>Yes</para>
            </entry>
            <entry>
              <para>usercopy/swapgs barriers and __user pointer sanitization</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>Spectre v2</para>
            </entry>
            <entry>
              <para>Yes</para>
            </entry>
            <entry>
              <para>Retpoline, RSB filling, and conditional IBPB, IBRS_FW, and STIBP</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>SRBDS</para>
            </entry>
            <entry>
              <para>No</para>
            </entry>
            <entry>
              <para>N/A</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>TSX Async Abort</para>
            </entry>
            <entry>
              <para>No</para>
            </entry>
            <entry>
              <para>N/A</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>Register File Data Sampling (RFDS)</para>
            </entry>
            <entry>
              <para>No</para>
            </entry>
            <entry>
              <para>N/A</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>Gather Data Sampling (GDS)</para>
            </entry>
            <entry>
              <para>No</para>
            </entry>
            <entry>
              <para>N/A</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>Speculative Return Stack Overflow (SRSO)</para>
            </entry>
            <entry>
              <para>No</para>
            </entry>
            <entry>
              <para>N/A</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <para>If it can be guaranteed that the server is in a trusted environment running
      only known code that is not malicious, the <parameter>mitigations=off</parameter>
      parameter can be specified on the kernel command line. This option disables all
      security mitigations and may improve performance in some cases. However, at the
      time of writing and in most cases, the gain on an AMD EPYC 9005 Series Processor is
      marginal when compared to other CPUs. Evaluate carefully whether the gain is worth
      the risk and if unsure, leave the mitigations enabled.</para>

  </sect1>

  <sect1 xml:id="sec-hw-based-profiling">
    <title>Hardware-based profiling</title>

    <para>The AMD EPYC 9005 Series Processor has extensive <emphasis role="strong">Performance Monitoring
      Unit</emphasis> (PMU) capabilities. Advanced monitoring of a workload can be conducted via the
      <command>perf</command>. The command supports a range of hardware events including
      cycles, L1 cache access/misses, TLB access/misses, retired branch instructions and
      mispredicted branches. To identify what subsystem may be worth tuning in the OS,
      the most useful invocation is <command>perf record -a -e cycles sleep 30</command>. This
      captures 30 seconds of data for the entire system. You can also call <command>perf
      record -e cycles command</command> to gather a profile of a given workload. Specific
      information on the OS can be gathered through tracepoints or creating probe points
      with <command>perf</command> or <command>trace-cmd</command>. But the details on
      how to conduct such analyses are beyond the scope of this paper.</para>

  </sect1>

  <sect1 xml:id="sec-compiler">
    <title>Compiler selection</title>

    <para>SUSE Linux Enterprise ships with multiple versions of GCC. SUSE Linux Enterprise
      15 SP6 service packs ship with <package>GCC 7</package> which at the time of writing is
      <package>GCC 7.5.0</package> with the package version <package>7-3.9.1</package>. The
      intention is to avoid unintended consequences when porting code that may affect
      the building and operation of applications. The <package>GCC 7</package> development
      originally started in 2016, with a branch created in 2017 and <package>GCC 7.5</package>
      released in 2019. This means that the system compiler has no awareness of the AMD
      EPYC 7002 or later Series processors.</para>

    <para>Fortunately, the add-on <package>Developer Tools Module</package> includes
      additional compilers with the latest version currently based on <package>GCC
      13.2.1</package>. This compiler is capable of generating optimized code targeted at
      the 4th Generation AMD EPYC Processor using the <filename>znver4</filename> target.
      It also provides additional support for <package>OpenMP 5.0</package>, extends the support
      of <package>OpenMP 5.1</package> features and very limited first support of <package>OpenMP 5.2</package>
      features. Unlike the system compiler, the major version of GCC shipped with the Developer Tools Module can change
      during the lifetime of the product. It is expected that <package>GCC 14</package> will
      be included in future releases for generating optimized code for the 5th Generation
      AMD EPYC Processor. Unfortunately, at the time of writing, there is not a version of
      GCC available optimized for the AMD EPYC 9005 Series Processor specifically.</para>

    <para>The OS packages are built against a generic target. However, where applications and
      benchmarks can be rebuilt from source, the minimum option should be
      <package>-march=znver4</package> for <package>GCC 13</package> and later versions
      of GCC.</para>

    <para>Further information on how to install the <package>Developer Tools Module</package>
      and how to build optimized versions of applications can be found in the guide
      <link xlink:href="https://documentation.suse.com/sbp/devel-tools/html/SBP-GCC-12/index.html">Advanced optimization and new capabilities of GCC 12</link>.</para>

  </sect1>

  <sect1 xml:id="sec-candidate-workloads">
    <title>Candidate workloads</title>

    <para>The workloads that will benefit most from the 5th Generation AMD EPYC Processor
      architecture are those that can be parallelized and are either memory or IO-bound. This
      is particularly true for workloads that are <quote>NUMA friendly</quote>. They can
      be parallelized, and each thread can operate independently for most of the
      workload’s lifetime. For memory-bound workloads, the primary benefit will be taking
      advantage of the high bandwidth available on each channel. For IO-bound workloads,
      the primary benefit will be realized when there are multiple storage devices, each
      of which is connected to the node local to a task issuing IO.</para>

    <sect2 xml:id="sec-test-setup">
      <title>Test setup</title>

      <para>The following sections will demonstrate how an OpenMP and MPI workload can
        be configured and tuned on an AMD EPYC 9005 Series Processor reference platform. The
        system has two processors, each with 128 cores and SMT enabled for a total of 256
        cores (512 logical CPUs). The peak bandwidth available to the machine depends on
        the type of DIMMs installed and how the DIMM slots are populated. Note that the
        peak theoretical transfer speed is rarely reached in practice, given that it can
        be affected by the mix of reads/writes and the location and temporal proximity of
        memory locations accessed.</para>

<!-- DOUBLE CHECK WHETHER TO INCLUDE MINIMAL CONFIGURATION  Half the memory banks are populated with DDR4 2666
        MHz RDIMMs, with a theoretical maximum transfer speed of 21.3 GB/sec each. With
        single ranking, the peak memory transfer speed is 341 GB/sec.
-->

      <table>
        <title>Test setup</title>
        <tgroup cols="2">
          <colspec colname="col_1" colwidth="50*"/>
          <colspec colname="col_2" colwidth="50*"/>
          <tbody>
            <row>
              <entry>
                <para>CPU </para>
              </entry>
              <entry>
                <para>2x AMD EPYC 9755</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Platform</para>
              </entry>
              <entry>
                <para>AMD Engineering Sample Platform</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Drive</para>
              </entry>
              <entry>
                <para>Samsung SSD PM9A1 512GB</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>OS</para>
              </entry>
              <entry>
                <para>SUSE Linux Enterprise Server 15 SP6</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Memory Type</para>
              </entry>
              <entry>
                <para>24x 32GB DDR5</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Memory Interleaving</para>
              </entry>
              <entry>
                <para>Channel</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Memory Speed</para>
              </entry>
              <entry>
                <para>4800 MT/sec</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Kernel command line</para>
              </entry>
              <entry>
                <para>
                  <command>mitigations=off</command>
                </para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>

    </sect2>

    <sect2 xml:id="sec-workload-stream">
      <title>Test workload: STREAM</title>

      <para><emphasis role="italic">STREAM</emphasis> is a memory bandwidth benchmark
        created by Dr. John D. McCalpin from the University of Virginia (for more
        information, see <link xlink:href="https://www.cs.virginia.edu/stream/"
        >https://www.cs.virginia.edu/stream/</link>). It can be used to measure bandwidth
        of each cache level and bandwidth to main memory while calculating four basic
        vector operations. Each operation can exhibit different throughputs to main memory
        depending on the locations and type of access.</para>

      <para>The benchmark was configured to run both single-threaded and parallelized with
        OpenMP to take advantage of multiple memory controllers. The array of elements for
        the benchmark was set at 536,870,912 elements at compile time so that each array
        was 4096MB in size for a total memory footprint of approximately 12288 MB. The size
        was selected in line with the recommendation from STREAM that the array sizes be
        at least 4 times the total size of L3 cache available in the system. Pay special
        attention to the exact size of the L3 cache if V-Cache is present. An array-size
        offset was used so that the separate arrays for each parallelized thread would
        not share a Transparent Huge Page. The reason is that NUMA balancing may
        choose to migrate shared pages leading to some distortion of the results. </para>

      <table>
        <title>Test workload: STREAM</title>
        <tgroup cols="2">
          <colspec colname="col_1" colwidth="50*"/>
          <colspec colname="col_2" colwidth="50*"/>
          <tbody>
            <row>
              <entry>
                <para>Compiler</para>
              </entry>
              <entry>
                <para>gcc-13 (SUSE Linux) 13.2.1</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Compiler flags</para>
              </entry>
              <entry>
                <para>
                  <parameter>-Ofast -march=znver4 -mcmodel=medium -DOFFSET=512</parameter>
                </para>
              </entry>
            </row>
            <row>
              <entry>
                <para>OpenMP compiler flag</para>
              </entry>
              <entry>
                <para>
                  <parameter>-fopenmp</parameter>
                </para>
              </entry>
            </row>
            <row>
              <entry>
                <para>OpenMP environment variables</para>
              </entry>
              <entry>
                <para>
                  <parameter>OMP_PROC_BIND=SPREAD</parameter>
                </para>
                <para>
                  <parameter>OMP_NUM_THREADS=32</parameter>
                </para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>The <parameter>march=znver4</parameter> is a reflection of the compiler
        available in SUSE Linux Enterprise 15 SP6 at the time of writing.
        It should be checked if a later <package>GCC</package> version is
        available in the <package>Developer Tools Module</package> that supported
        <parameter>march=znver5</parameter>. The number of OpenMP threads was selected
        to have at least one thread running for every memory channel by having one thread
        per L3 cache available. The <parameter>OMP_PROC_BIND</parameter> parameter spreads
        the threads such that one thread is bound to each available dedicated L3 cache to
        maximize available bandwidth. This can be verified using <command>perf</command>,
        as illustrated below with slight editing for formatting and clarity.</para>

      <screen>epyc:~ # perf record -e sched:sched_migrate_task ./stream
epyc:~ # perf script
...
          stream-nnn x: sched:sched_migrate_task: comm=stream pid=494780 prio=120 orig_cpu=0 dest_cpu=8
          stream-nnn x: sched:sched_migrate_task: comm=stream pid=494781 prio=120 orig_cpu=0 dest_cpu=16
          stream-nnn x: sched:sched_migrate_task: comm=stream pid=494782 prio=120 orig_cpu=0 dest_cpu=24
          stream-nnn x: sched:sched_migrate_task: comm=stream pid=494783 prio=120 orig_cpu=0 dest_cpu=32
          stream-nnn x: sched:sched_migrate_task: comm=stream pid=494784 prio=120 orig_cpu=0 dest_cpu=40
          stream-nnn x: sched:sched_migrate_task: comm=stream pid=494785 prio=120 orig_cpu=0 dest_cpu=48
          stream-nnn x: sched:sched_migrate_task: comm=stream pid=494786 prio=120 orig_cpu=0 dest_cpu=56
...
</screen>

      <para>Several options were considered for the test system that were unnecessary
        for STREAM running on the AMD EPYC 9005 Series Processor but may be useful in other
        situations. STREAM performance can be limited if a load/store instruction stalls to
        fetch data. CPUs may automatically prefetch data based on historical behavior but it
        is not guaranteed. In limited cases, depending on the CPU and workload, this may be
        addressed by specifying <parameter>-fprefetch-loop-arrays</parameter> and depending
        on whether the workload is store-intensive, <parameter>-mprefetchwt1</parameter>.
        However, care must be taken as an explicitly scheduled prefetch may
        disable a CPU’s predictive algorithms and degrade performance. Similarly, for
        some workloads branch mispredictions can be a major problem, and in some cases
        breach mispredictions can be offset against I-Cache pressure by specifying
        <parameter>-funroll-loops</parameter>. In the case of STREAM on the test
        system, the CPU accurately predicted the branches rendering the unrolling of
        loops unnecessary. For math-intensive workloads it can be beneficial to link the
        application with <parameter>-lmvec</parameter> depending on the application. In the
        case of STREAM, the workload did not use significant math-based operations and so
        this option was not used. Some styles of code blocks and loops can also be optimized
        to use vectored operations by specifying <parameter>-ftree-vectorize</parameter> and
        explicitly adding support for CPU features such as <parameter>-mavx2</parameter>. In
        all cases, STREAM does not benefit as its operations are very basic. But it should
        be considered on an application-by-application basis and when building support
        libraries such as numerical libraries. In all cases, experimentation is recommended
        but caution advised. This holds particularly true when considering options like prefetch that may
        have been advisable on much older CPUs or completely different workloads. Such
        options are not universally beneficial or always suitable for modern CPUs such as
        the AMD EPYC 9005 Series Processors.</para>

      <para>In the case of STREAM running on the AMD EPYC 9005 Series Processor,
      it was sufficient to enable <parameter>-Ofast</parameter>. This includes the
      <parameter>-O3</parameter> optimizations to enable vectorization. In addition,
      it gives some leeway for optimizations that increase the code size with additional
      options for fast math that may not be standards-compliant.</para>

      <para>For OpenMP, the <parameter>SPREAD</parameter> option was used to spread the
        load across L3 caches. OpenMP has a variety of different placement options
        if manually tuning placement. But careful attention should be paid to
        <parameter>OMP_PLACES</parameter>, given the importance of the L3 Cache topology in
        AMD EPYC 9005 Series Processors, if the operating system does not automatically
        place tasks appropriately. At the time of writing, it is not possible to
        specify <parameter>l3cache</parameter> as a place similar to what MPI has. In
        this case, the topology will need to be examined either with library support
        such as <package>hwloc</package>, directly via the <parameter>sysfs</parameter>
        or manually. While it is possible to guess via the CPUID, it is not recommended
        as CPUs may be offlined or the enumeration may vary between platforms because of BIOS
        implementations. An example specification of places based on L3 cache for the test
        system is:</para>

      <screen>
{0:8,256:8},   {8:8,264:8},   {16:8,272:8},  {24:8,280:8},  {32:8,288:8},  {40:8,296:8},
{48:8,304:8},  {56:8,312:8},  {64:8,320:8},  {72:8,328:8},  {80:8,336:8},  {88:8,344:8},
{96:8,352:8},  {104:8,360:8}, {112:8,368:8}, {120:8,376:8}, {128:8,384:8}, {136:8,392:8},
{144:8,400:8}, {152:8,408:8}, {160:8,416:8}, {168:8,424:8}, {176:8,432:8}, {184:8,440:8},
{192:8,448:8}, {200:8,456:8}, {204:8,464:8}, {216:8,472:8}, {224:8,480:8}, {232:8,488:8},
{240:8,496:8}, {248:8,504:8}
      </screen>

      <para><xref linkend="fig-stream-results"/> shows the reported bandwidth for the
        single, parallelized and parallelized with proper placement cases. The single-threaded
        bandwidth for the basic Copy vector operation on a single core was 51.3 GB/sec. This is
        higher than the theoretical maximum of a single DIMM, but the IO die interleave accesses, and
        caching effects and prefetch still apply. The total throughput for each parallel operation
        with <parameter>SPREAD</parameter> enabled ranged from 600 GB/sec to 700 GB/sec depending on
        the type of operation and how efficiently memory bandwidth was used. This is twice as much
        compared to default placement by OpenMP.  This is very roughly scaling with the number of
        memory channels available on the machine.</para>

      <note>
        <title>STREAM scores</title>

        <para>Higher STREAM scores can be reported by reducing the array sizes so that cache is
          partially used with the maximum score requiring that each threads memory footprint fits
          inside the L1 cache. Additionally, it is possible to achieve results closer to the
          theoretical maximum by manual optimization of the STREAM benchmark using vectored
          instructions and explicit scheduling of loads and stores. The purpose of this
          configuration was to illustrate the impact of properly binding a workload that can be
          fully parallelized with data-independent threads.</para>

      </note>

      <figure xml:id="fig-stream-results">
        <title>STREAM Bandwidth, Single Threaded and Parallelized</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="amd-epyc-5-graph-stream.png" width="100%" format="PNG"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="amd-epyc-5-graph-stream.png" width="100%" format="PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

    </sect2>

    <sect2 xml:id="sec-workload-nasa">
      <title>Test workload: NASA Parallel Benchmark</title>

      <para><emphasis role="italic">NASA Parallel Benchmark (NPB)</emphasis> is a small set of
        compute-intensive kernels designed to evaluate the performance of supercomputers. They are
        small compute kernels derived from <emphasis role="italic">Computational Fluid Dynamics
          (CFD)</emphasis> applications. The problem size can be adjusted for different memory
        sizes. Reference implementations exist for both MPI and OpenMP. This setup will focus on the
        MPI reference implementation.</para>

      <para>While each application behaves differently, one common characteristic is that the
        workload is very context-switch intensive, barriers frequently yield the CPU to other tasks
        and the lifetime of individual processes can be very short-lived. The following paragraphs
        detail the tuning selected for this workload.</para>

      <para>The most basic step is setting the CPU governor to <quote>performance</quote> although
        it is not mandatory. This can address issues with short-lived or mobile tasks failing to
        run long enough for a higher P-State to be selected even though the workload is very
        throughput-sensitive. The migration cost parameter is set to reduce the frequency in which
        the load balancer will move an individual task. The minimum granularity is adjusted to reduce
        overscheduling effects. </para>

      <para>Depending on the computational kernel used, the workload may require a power-of-two
        number or a square number of processes to be used. However, note that using all available
        CPUs can mean that the application can contend with itself for CPU time. Furthermore, as IO
        may be issued to shared memory backed by disk, there are system threads that also need CPU
        time. Finally, if there is CPU contention, MPI tasks can be stalled waiting on an available
        CPU and OpenMPI may yield tasks prematurely if it detects there are more MPI tasks than CPUs
        available. These factors should be carefully considered when tuning for parallelized
        workloads in general and MPI workloads in particular.</para>

      <para>In the specific case of testing NPB on the System Under Test, there was usually a limited
        advantage to limiting the number of CPUs used. For the <emphasis role="italics"
          >Embarrassingly Parallel (EP)</emphasis> load in particular, it benefits from using all
        available CPUs. Hence, the default configuration used all available CPUs (512) which is both
        a power-of-two and square number of CPUs because it was a sensible starting point. However,
        this is not universally true. Using <package>perf</package>, it was found that some
        workloads are memory-bound and do not benefit from a high degree of parallelization. In
        addition, for the final configuration, some workloads were parallelized to have one task per
        L3 cache in the system to maximize cache usage. The exception was the <emphasis
          role="italic">Scalar Pentadiagonal (SP)</emphasis> workload which was both memory-bound
        and benefited from using all available CPUs. As the number of cores can vary between chips
        and the number of populated memory channels, the tuning parameters used for this test may
        not be universally true for all AMD EPYC platforms. This highlights that there is no
        universal good choice for optimizing a workload for a platform. Thus, experimentation and
        validation of tuning parameters is vital.</para>

      <para>The basic compilation flags simply turned on all safe optimizations. The
        tuned flags used <parameter>-Ofast</parameter> which can be unsafe for some
        mathematical workloads but generated the correct output for NPB. The other
        flags used the optimal instructions available on the distribution compiler and
        vectorized some operations. <package>GCC 13</package> is more strict in terms
        of matching types in Fortran. Depending on the version of NPB used, it may
        be necessary to specify the <parameter>-fallow-argument-mismatch</parameter>
        or <parameter>-fallow-invalid-boz</parameter> to compile unless the source code
        is modified.</para>

      <para>As NPB uses shared files, an XFS partition was used for the temporary files. It is,
        however, only used for mapping shared files and is not a critical path for the benchmark and
        no IO tuning is necessary. In some cases, with MPI applications, it will be possible to use a
          <filename>tmpfs</filename> partition for OpenMPI. This avoids unnecessary IO assuming the
        increased physical memory usage does not cause the application to be paged out.</para>

      <table>
        <title>Test workload: NASA Parallel Benchmark</title>
        <tgroup cols="2">
          <colspec colname="col_1" colwidth="32*"/>
          <colspec colname="col_2" colwidth="68*"/>
          <tbody>
            <row>
              <entry>
                <para>Compiler</para>
              </entry>
              <entry>
                <para>gcc-13 (SUSE Linux) 13.2.1</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>OpenMPI </para>
              </entry>
              <entry>
                <para>openmpi4-4.1.6-150600.1.6.x86_64</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Default compiler flags</para>
              </entry>
              <entry>
                <para>
                  <parameter>-m64 -O3 -mcmodel=large</parameter>
                </para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Default number processes</para>
              </entry>
              <entry>
                <para>
                  <parameter>512</parameter>
                </para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Selective number processes</para>
              </entry>
              <entry>
                <para>
                  <!--<parameter>bt=256 cg=256 ep=512 lu=256 mg=256 sp=256</parameter>-->
                  <parameter>bt=256 ep=512 lu=256 mg=256 sp=256</parameter>
                </para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Fortran compiler flags</para>
              </entry>
              <entry>
                <para>
                  <parameter>-fallow-argument-mismatch -fallow-invalid-boz</parameter>
                </para>
              </entry>
            </row>
            <row>
              <entry>
                <para>Tuned compiler flags</para>
              </entry>
              <entry>
                <para>
                  <parameter>-Ofast -march=znver4 -mtune=znver4 -ftree-vectorize</parameter>
                </para>
              </entry>
            </row>
            <row>
              <entry>
                <para>CPU governor performance</para>
              </entry>
              <entry>
                <para>
                  <parameter>cpupower frequency-set -g performance</parameter>
                </para>
              </entry>
            </row>
            <row>
              <entry>
                <para>mpirun parameters</para>
              </entry>
              <entry>
                <para>
                  <parameter>-mca btl ^openib,udapl -np 512 --bind-to l3cache</parameter>
                </para>
              </entry>
            </row>
            <row>
              <entry>
                <para>mpirun environment</para>
              </entry>
              <entry>
                <para>
                  <parameter>TMPDIR=/xfs-data-partition</parameter>
                </para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para><xref linkend="fig-epyc-nas-mpi-results"/> shows the time, as reported by
        the benchmark, for each of the kernels to complete.</para>

      <figure xml:id="fig-epyc-nas-mpi-results">
        <title>NAS MPI Results</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="amd-epyc-5-nas.png" width="100%" format="PNG"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="amd-epyc-5-nas.png" width="100%" format="PNG"/>
          </imageobject>
        </mediaobject>
      </figure>


      <para>The <emphasis role="italics">gcc-7-default</emphasis> test used the system
        compiler, all available CPUs, basic compilation options and the
        <parameter>performance</parameter> governor. The second test <emphasis
        role="italics">gcc-13-default</emphasis> used an alternative compiler. <emphasis
        role="italics">gcc-13-tuned</emphasis> used additional compilation options, and
        bound tasks to L3 caches gaining between 1.5% and 40.6% performance on average
        relative to <emphasis role="italics">gcc-7-default</emphasis>. The final test
        <emphasis role="italics">selective</emphasis> used processes that either used
        all CPUs, avoided heavy overloaded or limited processes to one per L3 cache,
        showing additional between 2.35% and 37.66% depending on the computational
        kernel.</para>

      <para>In some cases, it will be necessary to compile an application that can run
        on different CPUs. In such cases, <parameter>-march=znver4</parameter>
        may not be suitable if it generates binaries that are incompatible with other
        vendors. In such cases, it is possible to specify the ISA-specific options that are
        cross-compatible with many x86-based CPUs such as <parameter>-mavx2</parameter>,
        <parameter>-mfma</parameter>, <parameter>-msse2</parameter> or
        <parameter>msse4a</parameter> while favoring optimal code generation for AMD
        EPYC 9005 Series Processors with <parameter>-mtune=znver4</parameter>. This can be
        used to strike a balance between excellent performance on a single CPU and great
        performance on multiple CPUs.</para>
    </sect2>

  </sect1>

  <sect1 xml:id="sec-tuning-zen5c-variant">
    <title>Tuning AMD EPYC 9005 Processors (Zen5c cores)</title>

    <para>As the Zen5 and Zen5c cores are ISA-compatible,
    no code tuning or compiler setting changes should be necessary. For Cloud environments,
    partitioning or any binding of Virtual CPUs to Physical CPUs may need to be adjusted
    to account for the increased number of cores. The additional cores may also allow
    additional containers or virtual machines to be hosted on the same physical machine
    without CPU contention. Similarly, the degree of parallelization for HPC workloads
    may need to be adjusted. In cases where the workload is tuned based on the number of
    CCD’s, adjustments may be necessary for the changed number of CCDs. An exception are cases
    where the workload already hits scaling limits. While tuning based on the different
    number of CCXs is possible, it should only be necessary for applications with very
    strict latency requirements. As the size of the cache per core is halved, partitioning based
    on cache sizes may also need to be adjusted. In some cases, where workloads are tuned
    based on the output of tools like <command>hwloc</command> partitioning
    <!-- comment: Seems a word was missing - is "partitioning" correct here?-->
    may adjust automatically but any static partitioning should be re-examined.</para>

    <para>When configuring workloads for AMD EPYC 9005 Series Processors based on either Zen5 for Zen5c cores,
    the most important task is to set expectations. While super-linear
    scaling is possible, it should not be expected. It may be possible to achieve super-linear
    scaling in Cloud Environments for the number of instances hosted without performance
    loss if individual containers or virtual machines are not utilizing 100% of CPU. However,
    it should be planned carefully and tested. This would be particularly true in cases
    where multiple instances are hosted that have different times of day or year for active
    phases. The normal expectation is a best case of 33% gain for CPU-intensive workloads
    because of the increased number of cores. But sub-linear scaling is common because of resource
    contention. Contention between SMT siblings, memory bandwidth, memory availability,
    memory interconnects, thread communication overhead or peripheral devices may prevent
    perfect linear scaling even for perfectly parallelized applications. Similarly, not all
    applications can scale perfectly. It is possible for performance to plateau and even
    regress as the degree of parallelization increases.</para>

  </sect1>

  <sect1 xml:id="sec-pmu-changes">
    <title>Performance Monitoring Unit changes</title>
    <para>Support for the following Zen related perf changes are present in SUSE Linux Enterprise 15 SP6:</para>
    <orderedlist>
      <listitem>
        <para>AMD Performance Monitoring V2 (PerfMonV2) (v5.19) Zen4+
          <itemizedlist mark='bullet'>
            <listitem>
              <para>global registers allowing enabling/disabling of multiple counters concurrently (via a single MSR operation)</para>
            </listitem>
            <listitem>
              <para>dynamic detection of available PMUs</para>
            </listitem>
          </itemizedlist>
        </para>
      </listitem>
      <listitem>
        <para>AMD Zen4 IBS extensions (v5.19 kernel, v6.0 perf userspace) Zen4+
          <itemizedlist mark='bullet'>
            <listitem>
              <para>DataSrc extension allowing the source of data to be decoded.</para><para>Perf script and perf record (raw trace) will now report DataSrc details for tagged load/store operations. See perf-amd-ibs(1) for more details.</para>
            </listitem>
            <listitem>
              <para>IBS L3 miss filtering.</para><para>Enabled by specifying an l3missonly=1 event parameter to perf. See perf-amd-ibs(1) for more details.</para>
            </listitem>
          </itemizedlist>
        </para>
      </listitem>
      <listitem>
        <para>Last Branch Record Extension v2 (LbrExtV2) (v6.1) Zen4+
          <itemizedlist mark='bullet'>
            <listitem>
              <para>LBR Freeze on PMI</para>
            </listitem>
            <listitem>
              <para>Hardware branch filtering providing additional specificity</para>
            </listitem>
            <listitem>
              <para>Branch speculation information</para>
            </listitem>
          </itemizedlist>
        </para>
      </listitem>
      <listitem>
        <para>JSON event file updates for Zen 4 (v6.2 perf userspace) Zen4+
          <itemizedlist mark='bullet'>
            <listitem>
              <para>This adds event information from the "Core Performance Monitor", "L3 Cache Performance Monitor", "Fabric Performance Monitor Counter" and "Performance Measurement" sections of the AMD Processor Programming Reference (PPR) documentation for Zen4.</para>
            </listitem>
          </itemizedlist>
        </para>
      </listitem>
    </orderedlist>
    <para>Please note.  Zen5 JSON event updates (perf userspace) were added in Linux v6.10 and will be supported in a subsequent SLE release. Support for Zen 4 unified memory controller events (kernel and perf userspace) is expected to be available in a similar timeframe.</para>
  </sect1>

  <sect1 xml:id="sec-amd-epyc2-virtualization">
    <title>Using AMD EPYC 9005 Series Processors for virtualization</title>

    <para> Running Virtual Machines (VMs) has some aspects in common with running
        <quote>regular</quote> tasks on a host operating system. Therefore, most of the tuning
      advice described so far in this document are valid and applicable to this section too. </para>

    <para> However, virtualization workloads do pose their own specific challenges and some special
      considerations need to be made, to achieve a better tailored and more effective set of tuning
      advice, for cases where a server is used only as a virtualization host. And this is especially
      relevant for large systems, such as AMD EPYC 9005 Series Processors. </para>

    <para>This is because:</para>

    <itemizedlist>
      <listitem>
        <para>VMs typically run longer, and consume more memory, than most of others
            <quote>regular</quote> OS processes.</para>
      </listitem>
      <listitem>
        <para>VMs can be configured to behave either as NUMA-aware or non NUMA-aware
            <quote>workloads</quote>.</para>
      </listitem>
    </itemizedlist>

    <para> In fact, VMs often run for hours, days, or even months, without being terminated or
      restarted. Therefore, it is almost never acceptable to pay the price of suboptimal resource
      partitioning and allocation, even when there is the expectation that things will be better
      next time. For example, it is always desirable that vCPUs run close to the memory that they
      are accessing. For reasonably big NUMA aware VMs, this happens only with proper mapping of the
      virtual NUMA nodes of the guest to physical NUMA nodes on the host. For smaller, NUMA-unaware
      VMs, that means allocating all their memory on the smallest possible number of host NUMA
      nodes, and making their vCPUs run on the pCPUs of those nodes as much as possible. </para>

    <para> Also, poor mapping of virtual machine resources (virtual CPUs and memory, but also IO) on
      the host topology induces performance degradation to everything that runs inside the virtual
      machine – and potentially even to other components of the system . </para>

    <para> Regarding NUMA-awareness, a VM is called out to be NUMA aware if a (virtual) NUMA
      topology is defined and exposed to the VM itself and if the OS that the VM runs (guest OS) is
      also NUMA aware. On the contrary, a VM is called NUMA-unaware if either no (virtual) NUMA
      topology is exposed or the guest OS is not NUMA aware. </para>

    <para> VMs that are large enough (in terms of amount of memory and number of virtual CPUs) to
      span multiple host NUMA nodes, benefit from being configured as NUMA aware VMs. However, even
      for small and NUMA-unaware VMs, wise placement of their memory on the host nodes, and
      effective mapping of their virtual CPUs (vCPUs) on the host physical CPUs (pCPUs) is key for
      achieving good and consistent performance. </para>

    <para> This second half of the present document focuses on tuning a system where CPU and memory
      intensive workloads run inside VMs. We will explain how to configure and tune both the host
      and the VMs, in a way that performance comparable to the ones of the host can be achieved. </para>

    <para> Both the Kernel-based Virtual Machine (KVM) and the Xen-Project hypervisor (Xen), as
      available in SUSE Linux Enterprise Server 15 SP6, provide mechanisms to enact this kind of
      resource partitioning and allocation. Note that this document focuses on the former (KVM), but it
      includes some <emphasis role="strong">hints</emphasis> about how to deal with the latter as well. </para>

    <note>
      <title>Virtual Machine types</title>

      <para> KVM only supports one type of VM – a fully hardware-based virtual machine (HVM). Under
        Xen, VMs can be paravirtualized (PV) or hardware virtualized machines (HVM). </para>

      <para> Xen HVM guests with paravirtualized interfaces enabled (called PVHVM, or HVM) are very
        similar to KVM VMs, which are based on hardware virtualization but also employ
        paravirtualized IO (VirtIO). In this document, we always refer to Xen VMs of the (PV)HVM
        type. </para>
    </note>

  </sect1>

  <sect1 xml:id="sec-resource-allocation-tuning-host">
    <title>Resources allocation and tuning of the host</title>

    <para> No details are given, here, about how to install and configure a system so that it
      becomes a suitable virtualization host. For similar instructions, refer to the SUSE
      documentation at <link
        xlink:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/cha-vt-installation.html"
        > SUSE Linux Enterprise Server 15 SP4 Virtualization Guide: Installation of Virtualization
        Components</link>. </para>

    <para> The same applies to configuring things such as networking and storage, either for the
      host or for the VMs. For similar instructions, refer to suitable chapters of OS and
      virtualization documentation and manuals. As an example, to know how to assign network
      interfaces (or ports) to one or more VMs, for improved network throughput, refer to <link
        xlink:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/cha-libvirt-config-gui.html#sec-libvirt-config-pci"
        > SUSE Linux Enterprise Server 15 SP6 Virtualization Guide: Assigning a Host PCI Device to a
        VM Guest</link>. </para>

    <sect2 xml:id="sec-allocating-resources-hostos">
      <title>Allocating resources to the host OS</title>

      <para> Even if the main purpose of a server is <quote>limited</quote> to running VMs, some
        activities will be carried out on the host OS. In fact, in both Xen and KVM, the host OS is
        at least responsible for helping with the IO of the VMs. It may, therefore, be necessary to
        make sure that the host OS has some resources (namely, CPUs and memory) exclusively assigned
        to itself. </para>

      <note>
        <title>Host OS on KVM and on Xen</title>
        <para> On KVM, the host OS is the SUSE Linux Enterprise distribution installed on the
          server, which then loads the hypervisor kernel modules. On Xen, the host OS still is a
          SUSE Linux Enterprise distribution, but it runs inside what is to all the effect an
          (although special) Virtual Machine (called Domain 0, or Dom0). </para>
      </note>

      <para> In the absence of any specific requirements involving host resources, a good rule of
        thumb suggests that 5 to 10 percent of the physical RAM should be reserved to the host OS.
        On KVM, increase that quota in case the plan is to run many (for example hundreds) of VMs.
        On Xen, it is okay to always give dom0 not more than a few gigabytes of memory. This is
        especially the case when planning to take advantage of disaggregation (see <link
          xlink:href="https://wiki.xenproject.org/wiki/Dom0_Disaggregation">
          https://wiki.xenproject.org/wiki/Dom0_Disaggregation</link>). </para>

      <para> In terms of CPUs, depending on the workload, it may be fine to use all the physical
        CPUs for the VMs (and this is in fact how the benchmarks in the experimental section of this
        guide have been conducted). On the other hand, if it is necessary to keep some CPUs for the
        host OS / dom0, we advise to reserve at least one physical core on each NUMA node.
        This is especially true for a system like the one show in <xref linkend="fig-epyc-topology"/>.
        In fact, host OS activities are mostly related to performing
        IO for VMs and it is beneficial for performance if the kernel threads that handle devices can
        run on the nodes to which the devices themselves are attached, which is both NUMA nodes, in our case. </para>

      <para> System administrators need to be able to reach out and login to the system, to monitor,
        manage and troubleshoot it. Therefore, it is possible that even more resources needs to be
        assigned to the host OS. This would be for making sure that management tools (for example,
        the <emphasis role="strong">SSH</emphasis> daemon) can be reached, and that the hypervisor's
        toolstack (for example, <package>libvirt</package>) can run without too much contention. </para>

      <sect3 xml:id="sec-allocating-resources-hostos-kvm">
        <title>Reserving CPUs and memory for the host on KVM</title>

        <para> When using KVM, sparing, for example, 32 cores (that is one full core for each CCX on both NUMA nodes) and 64 GB of RAM for the host OS is
          done by stopping creating VMs when the total number of vCPUs of all VMs has reached 448
          (as each core has 2 threads) and when the total cumulative amount of allocated RAM has
          reached 690 GB. </para>

        <para> To make sure that these CPUs are not used by the virtual machines and are available
          to the host, virtual CPU pinning (discussed later in this document) can be used. There are
          also other ways to enforce this, for example with <package>cgroups</package>, or by means
          of the <package>isolcpus</package> boot parameter, but these are not covered in details within this guide. </para>

      </sect3>

      <sect3 xml:id="sec-allocating-resources-hostos-xen">
        <title>Reserving CPUs and memory for the host on Xen</title>

        <para> When using Xen, dom0 resource allocation needs to be done explicitly at system boot
          time. For example, assigning 32 physical cores and 64 GB of RAM to it is done by specifying
          the following additional parameters on the hypervisor boot command line (for example, by
          editing <filename>/etc/defaults/grub</filename>, and then updating the boot loader): </para>

        <screen>dom0_mem=65536M,max:65536M dom0_max_vcpus=64</screen>

        <para> The number of vCPUs is 64 because we want Dom0 to have 32 physical cores, and the AMD
          EPYC 9005 Series Processor has Symmetric multithreading (SMT). 65536M memory (that is 64
          GB) is specified both as current and as maximum value, to prevent Dom0 from using
          ballooning (see <link
            xlink:href="https://wiki.xenproject.org/wiki/Tuning_Xen_for_Performance#Memory">
            https://wiki.xenproject.org/wiki/Tuning_Xen_for_Performance#Memory</link> ). </para>

        <para> Making sure that Dom0 vCPUs run on specific pCPUs is not strictly necessary. However, if
          wanted, it can be enforced acting on the Xen scheduler, when Dom0 is booted (as there is
          currently no mechanism to set up this via Xen boot time parameters). If using the
            <package>xl</package> toolstack, the command is:</para>

        <screen>xl vcpu-pin 0 &lt;vcpu-ID> &lt;pcpu-list></screen>

        <para>Or, with <package>virsh</package>:</para>

        <screen>virsh vcpupin 0 -\-vcpu &lt;vcpu-ID> -\-cpulist &lt;pcpu-list></screen>

        <para>Note that <command>virsh vcpupin –config ...</command> is not effective for
          Dom0.</para>

        <para> If wanting to limit Dom0 to only a specific (set of) NUMA node(s), the
            <command>dom0_nodes=&lt;nodeid></command> boot command line option can be used. This
          will affect both memory and vCPUs. In fact, it means that memory for Dom0 will be
          allocated on the specified node(s), and the vCPUs of Dom0 will be restricted to run on
          those same node(s). When the Dom0 has booted, it is still possible to use <command>xl
            vcpu-pin</command> or <command>virsh vcpupin</command> to change where its vCPUs will be
          scheduled. But the memory will never be moved from where it has been allocated during
          boot. On AMD EPYC 9005 Series Processors, using this option is not recommended. </para>

      </sect3>

      <sect3 xml:id="sec-host-cpu-for-intensive-io">
        <title>Reserving CPUs for the host under IO intensive VM workloads</title>

        <para> Tuning the host and the VMs for running IO-intensive workloads is out of the scope of this
          guide. Just as general advice, if IO done in VM is important, it may be appropriate to leave to
          the host OS either one core or one thread for each IO device used by each VM. If this is
          not possible, for example because it reduces to much the CPUs that remain available for
          running VMs (especially in case the goal is to run many of them), then exploring
          alternative solutions for handling IO devices (such as <emphasis role="Italic"
            >SR-IOV</emphasis>) is recommended. </para>

      </sect3>

    </sect2>

    <sect2 xml:id="sec-trasparent-huge-pages">
      <title>(Transparent) Huge Pages</title>

      <para> For virtualization workloads, rather than using Transparent Huge Pages (THP) on the
        host, it is recommended that 1 GB huge pages are used for the memory of the VMs. This
        sensibly reduces both the page table management overhead and the level of resource
        contention that the system faces when VMs update their page tables. Moreover, if the host is
        entirely devoted to running VMs, Huge Pages are likely not required for host OS processes.
        Actually, in this case, having them active on the host may even negatively affect
        performance, as the THP service daemon (which is there for merging <quote>regular</quote>
        pages and form Huge Pages) can interfere with VMs’ vCPUs execution. For disabling Huge Pages
        for the host OS, in a KVM setup, add the following host kernel command-line option: </para>

      <screen>transparent_hugepage=never</screen>

      <para>Or execute this command, at runtime:</para>

      <screen>echo never > /sys/kernel/mm/transparent_hugepage/enabled</screen>

      <para> To use 1 GB Huge Pages as backing memory of KVM guests, such pages need to be allocated
        at the host OS level. It is best to make that happen by adding the following boot time
        parameter: </para>

      <screen>default_hugepagesz=1GB hugepagesz=1GB hugepages=&lt;number of hugepages></screen>

      <para> The value <parameter>&lt;number of hugepages></parameter> can be computed by taking
        the amount of memory we want to devoted to VMs, and dividing it by the page size (1 GB). For
        example on our host with 754 GB of RAM, creating 672 Huge Pages means we can accommodate up
        to  GB of VMs’ RAM, and leave plenty (~80GB) to the host OS. </para>

      <para> On Xen, none of the above is necessary. In fact, Dom0 is a paravirtualized guest, for
        which Huge Pages support is not present. On the other hand, for the memory used for the
        hypervisor and for the memory allocated by the hypervisor for HVM VMs, Huge Pages are always
        used as much as possible. So no explicit tuning is needed. </para>

    </sect2>

    <sect2 xml:id="sec-automatic-numa-balancing">
      <title>Automatic NUMA balancing</title>

      <para> On KVM, NUMAB can be useful in dynamic virtualization scenarios, where VMs are created,
        destroyed and re-created relatively quickly. Or, in general, it can be useful in cases where
        it is not possible or desirable to statically partition the host resources and assign them
        explicitly to VMs. This, however, comes at the price of some latency being introduced. Plus,
        NUMAB’s own operation can interfere with VMs’ execution and cause further performance
        degradation. In any case, this document focuses on achieving the best possible performance
        for VMs, through specific tuning and static resource allocation, therefore it is recommended
        to leave NUMAB turned off. This can be done by adding the following parameter to the host
        kernel command line: </para>

      <screen>numa_balancing=disable</screen>

      <para>If anything changes and the system is repurposed to achieve different goals, NUMAB can
        be enabled on-line with the command:</para>

      <screen>echo 0 > /proc/sys/kernel/numa_balancing</screen>

      <para> On Xen, Automatic NUMA Balancing (NUMAB) in the host OS should be disabled. Dom0 is,
        currently, a paravirtualized guest without a (virtual) NUMA topology and, thus, NUMAB would
        be totally useless. </para>
    </sect2>

    <sect2 xml:id="sec-services-daemons-power">
      <title>Services, daemons and power management</title>

      <para> Service daemons have been discussed already, in the first part of the document. Most of
        the consideration done there, applies here as well. </para>

      <para> For example, <package>tuned</package> should either not be used, or the profile should
        be set to one that does not implicitly put the CPUs in polling mode. Both
          <package>throughput-performance</package> and <package>virtual-host</package> profiles
        from SUSE Linux Enterprise Server 15 SP6 are okay, as neither of them touches
          <filename>/dev/cpu_dma_latency</filename>. </para>

      <para>
        <package>irqbalance</package> can be a source of latency, for no significant performance
        improvement. The suggestion is again to disable it for latency sensitive workloads. This
        means, though, that IRQs may need to be manually bound to the appropriate CPUs, considering
        the IO topology. </para>

      <para> As far as power management is concerned, the <package>cpufreq</package> governor can
        either be kept as it is by default, or switched to <parameter>performance</parameter>,
        depending on the nature of the workloads of interest. </para>

      <note>
        <title>Power management</title>
        <para> For anything that concerns power management on KVM, changing the
            <package>tuned</package> profile or using <package>cpupower</package>, from the host OS
          will have the effects one can expect, and described already in the first half of the
          document. On Xen, CPU frequency scaling is enacted by the hypervisor. It can be controlled
          from within Dom0, but the tool that needs to be used is different. For example, for
          setting governor to <parameter>performance</parameter>, we need the following: </para>

        <screen>xenpm set-scaling-governor performance</screen>

      </note>

    </sect2>

    <sect2 xml:id="sec-sev-host">
      <title>Confidential Computing Technologies (SEV and SEV-ES)</title>

      <para> Secure Encrypted Virtualization (SEV) and SEV with Encrypted State (SEV-ES) technologies, are available on the 5th Generation AMD
        EPYC Processors and supported on SUSE Linux Enterprise Server 15 SP6. They allow the memory of the VMs to be encrypted, enabling a high level of
        confidentiality. SEV-ES is considered superior to plain SEV, as also CPU registers are encrypted when they are saved into the host memory.
        For using SEV-ES for a VM, we need to enable it in the VM’s own configuration file, but there are preparation steps that needs to occur at
        the host level. </para>

      <para> The <package>libvirt</package> documentation contains all the necessary steps required
        for enabling SEV-ES on a host that supports it. It has been enough to add the following boot
        parameters to the host kernel command line, in the boot loader: </para>

      <screen>mem_encrypt=on kvm_amd.sev=1</screen>

      <para> For further details, refer to <link
          xlink:href="https://libvirt.org/kbase/launch_security_sev.html#Host"> libvirt
          documentation: Enabling SEV on the host</link>. </para>

      <para> It should be noted that, at least as far as the workload analyzed in this document,
        enabling SEV-ES on the host has not caused any noticeable performance degradation. In fact,
        running CPU and memory intensive workloads, both on the host and in VMs, with or without the
        parameters above, resulted in indistinguishable
        results. Therefore, enabling SEV-ES at the host level can be considered safe, from the point of
        view of not hurting performance of VMs and of workloads that will not take advantage of VM memory
        encryption. </para>

      <para>On the other hand, what happens when SEV and SEV-ES are used for encrypting VMs' memory is described later in the
        guide.</para>

      <note>
        <title>SEV-SNP</title>
        <para> 5th Generation AMD EPYC Processors come with an even more advanced confidential computing feature, known as
        SNP, that is also able to guarantee (among other things) the integrity of the VM. Such feature, however, is
        not officially available and supported on SUSE Linux Enterprise Server 15 SP6, and is therefore not discussed
        any further in this document (although it is, actually, already usable via an experimental module).</para>
      </note>

      <note>
        <title>Encryption on Xen</title>
        <para>In SUSE Linux Enterprise Server 15 SP6, neither SEV nor SEV-ES (not to mention SNP) are available on Xen.</para>
      </note>

    </sect2>

  </sect1>

  <sect1 xml:id="sec-resource-allocation-tuning-vms">
    <title>Resources allocation and tuning of VMs</title>

    <para> For instructions on how to create an initial VM configuration, start the VM, and install
      a guest OS, refer to the SUSE documentation at <link
        xlink:href="https://documentation.suse.com/sles/15-SP6/html/SLES-all/cha-kvm-inst.html">
        SUSE Linux Enterprise Server 15 SP6 Virtualization Guide: Guest Installation</link>. </para>

    <para>From a VM configuration perspective, the two most important factors for achieving top
      performance on CPU and memory bound workloads are:</para>

    <orderedlist>
      <listitem>
        <para>Partitioning of host resources and placement of the VMs</para>
      </listitem>
      <listitem>
        <para>Enlightenment of the VMs about their virtual topology</para>
      </listitem>
    </orderedlist>

    <para> For example, if there are two VMs, each one should be run on one socket, to minimize CPU
      contention and maximize and memory access parallelism. Also, and especially if the VMs are big
      enough, they should be made aware of their own virtual topology. That way, all the tuning
      actions described in the first part of this document become applicable to the workloads
      running inside the VMs too. </para>

    <sect2 xml:id="sec-placement-vms">
      <title>Placement of VMs</title>

      <para> When a VM is created, memory is allocated on the host to act as its virtual RAM. Moving
        this memory, for example on a different NUMA node from the one where it is first allocated,
        incurs (when possible) in a significant performance impact. Therefore, it is of paramount
        importance that the initial placement of the VMs is as optimal as possible. Both Xen and KVM
        can make <quote>educated guesses</quote> on what a good placement might be. For the purpose
        of this document, however, we are interested in what is the absolute best possible initial
        placement of the VMs, taking into account the specific characteristics of AMD EPYC 9005
        Series Processor systems. And this can only be achieved by <emphasis role="strong"
          >manually</emphasis> doing the initial placement. Of course, this comes at the price of
        reduced flexibility, and is only possible when there is no oversubscription. </para>

      <para> Deciding on what pCPUs the vCPUs of a VM run, can be done at creation time, but also
        easily changed along the VM lifecycle. It is, however, still recommended to start the VMs
        with good vCPUs placement. This is particularly important on Xen, where vCPU placement is
        what actually drives and controls memory placement. </para>

      <para>Placement of VM memory on host NUMA nodes happens by means of the &lt;numatune> XML
        element:</para>

      <screen>
&lt;numatune>
  &lt;memory mode='strict' nodeset='0-1'/>
  &lt;memnode cellid='0' mode='strict' nodeset='0'/>
  &lt;memnode cellid='1' mode='strict' nodeset='1'/>
  ...
&lt;/numatune>
      </screen>

      <para> The <parameter>'strict'</parameter> guarantees that the all memory for the VM will come
        only from the (set of) NUMA node(s) specified in <parameter>nodeset</parameter>. A
        cell is, in fact, a virtual NUMA node, with <parameter>cellid</parameter> being its ID and
          <parameter>nodeset</parameter> telling exactly from what host physical NUMA node(s) the
        memory for this virtual node should be allocated.</para>

      <para>The correctness of this kind of tuning can be verified checking on which NUMA node(s)
        the memory of the QEMU processes representing the VMs have been allocated on, by using the
        <parameter>numastat</parameter> tool (on the host) like this:</para>

      <screen>
host:~ # numastat -p qemu-system-x86_64
Per-node process memory usage (in MBs) for PID 53153 (qemu-system-x86)
                           Node 0          Node 1           Total
                  --------------- --------------- ---------------
Huge                    344064.00       344064.00       688128.00
Heap                         0.00          150.57          150.57
Stack                        0.00            0.13            0.13
Private                    149.59         4522.84         4672.43
----------------  --------------- --------------- ---------------
Total                   344213.59       348737.54       692951.12
      </screen>

      <para>In fact, we see that there is only 1 QEMU process (as there is only
        1 VM running) and its memory footprint has been equally split between the
        two NUMA nodes.</para>

      <para>A NUMA-unaware VM can still include this
        element in its configuration file. It will have only one
          <parameter>&lt;memnode></parameter> element, and the output of
        <parameter>numastat</parameter> will look like this:</para>

      <screen>
host:~ # numastat -p qemu-system-x86_64
Per-node process memory usage (in MBs)
PID                               Node 0          Node 1           Total
-----------------------  --------------- --------------- ---------------
184717 (qemu-system-x86)       346040.98           19.30       346060.28
184980 (qemu-system-x86)           19.50       346227.04       346246.54
-----------------------  --------------- --------------- ---------------
Total                          346060.48       346246.34       692306.82
      </screen>

      <para>In this case there are 2 VMs running and their memory have been
        pretty much entirely allocated on a single NUMA node: <parameter>Node 0</parameter>
        for the VM associated to the QEMU process with PID <parameter>184717</parameter>,
        and <parameter>Node 1</parameter> for the other one.
      </para>

      <para>A similar, but more complex example is also shown below. There are
        sixteen VMs, and they have been evenly distributed between the two nodes: </para>

      <screen>
host:~ # numastat -p qemu-system

Per-node process memory usage (in MBs)
PID                               Node 0          Node 1           Total
-----------------------  --------------- --------------- ---------------
200907 (qemu-system-x86)        43635.53           55.60        43691.13
201027 (qemu-system-x86)        43691.16           55.60        43746.76
201145 (qemu-system-x86)        43712.86           55.60        43768.46
201263 (qemu-system-x86)        43712.64           55.60        43768.24
201380 (qemu-system-x86)        43736.95           55.60        43792.55
201501 (qemu-system-x86)        43754.32           55.60        43809.93
201620 (qemu-system-x86)        43690.27           55.60        43745.87
201737 (qemu-system-x86)        43776.14           55.60        43831.75
201856 (qemu-system-x86)            0.00        43800.09        43800.09
201973 (qemu-system-x86)            0.00        43733.63        43733.63
202090 (qemu-system-x86)            0.00        43765.54        43765.54
202205 (qemu-system-x86)            0.00        43798.21        43798.21
202325 (qemu-system-x86)            0.00        43752.20        43752.20
202450 (qemu-system-x86)            0.00        43759.35        43759.35
202570 (qemu-system-x86)            0.00        43775.38        43775.38
202692 (qemu-system-x86)            0.00        43800.96        43800.96
-----------------------  --------------- --------------- ---------------
Total                          349709.87       350630.17       700340.04
      </screen>

      <para>Placement of vCPUs happens via the <parameter>&lt;cputune></parameter> element, as
        in the example below:</para>

      <screen>
&lt;vcpu placement='static'>512&lt;/vcpu>
&lt;cputune>
    &lt;vcpupin vcpu="0" cpuset="0"/>
    &lt;vcpupin vcpu="1" cpuset="256"/>
    &lt;vcpupin vcpu="2" cpuset="1"/>
    &lt;vcpupin vcpu="3" cpuset="257"/>
    ...
    &lt;vcpupin vcpu="256" cpuset="128"/>
    &lt;vcpupin vcpu="257" cpuset="384"/>
    &lt;vcpupin vcpu="258" cpuset="129"/>
    &lt;vcpupin vcpu="259" cpuset="385"/>
   ...
&lt;/cputune>
      </screen>

      <para> The value <parameter>512</parameter> means that the VM has 512 vCPUs.
          <parameter>static</parameter> guarantees that each vCPU will stay on the pCPU(s) on which
        it is <quote>pinned</quote> to. The various <parameter>&lt;vcpupin></parameter> elements
        are where the mapping between vCPUs and pCPUs is established (<parameter>vcpu</parameter>
        being the vCPU ID and <parameter>cpuset</parameter> being either one or a list of pCPU IDs). </para>

      <para> To be able to create VMs with more than 255 vCPUs,
        the following element should be added in the
          <parameter>&lt;device></parameter> section:</para>

      <screen>
&lt;features>
  ...
  &lt;ioapic driver="qemu"/>
&lt;/features>
...
&lt;devices>
  ...
  &lt;iommu model='intel'>
    &lt;driver intremap='on'/>
  &lt;/iommu>
  ...
&lt;/devices>
    </screen>

      <para> When pinning vCPUs to pCPUs, adjacent vCPU IDs (like vCPU ID 0 and vCPU ID 1) must be
        assigned to host SMT siblings (like, for example, pCPU 4 and pCPU 260, on our test server):</para>

      <screen>
host:~ # cat /sys/devices/system/cpu/cpu4/topology/thread_siblings_list
4,260
      </screen>

      <para>In fact, QEMU
        uses a static SMT sibling CPU ID assignment. This is how we guarantee that virtual SMT
        siblings will always execute on actual physical SMT siblings: </para>

      <screen>
vm1:~ # cat /sys/devices/system/cpu/cpu8/topology/thread_siblings_list
8,9
    </screen>

      <para> The following sections give more specific and precise details about placement of vCPUs
        and memory for VM of varying sizes, on the reference system for this guide (see <xref
          linkend="fig-epyc-topology"/>. Note that in SUSE Linux Enterprise 15 SP6, as an enhancement, as compared
        to previous OS versions, we can actually create VMs as large as our test host is, that means a VM with 512 virtual CPUs. </para>

      <sect3 xml:id="sec-placement-one-vm">
        <title>Placement of a single large VM</title>

        <para> An interesting use case is when <quote>only one</quote> VM is used. Reasons for doing
          something like that include security/isolation, flexibility, high availability, and
          others. In this cases, typically, the VM is almost as large as the host itself. </para>

        <para> Let us, therefore, consider a VM with 512 vCPUs and 672 GB of RAM. Such VM
          spans multiple host NUMA nodes and therefore it is recommended to create a virtual NUMA
          topology for it. </para>

        <para> We should create a virtual topology with 2 virtual NUMA nodes (that is, as many as
          there are physical NUMA nodes), and split the VM’s memory equally among them. The 672
          vCPUs are assigned to the 2 nodes, 336 on each. This way, if a suitable
          virtual topology is also provided to the VM, each of the VM’s vCPUs will access its own
          memory directly, and use Infinity Fabric inter-socket links only to reach foreign memory,
          as it happens on the host. Workloads running inside the VM can be tuned exactly like they
          were running on a bare metal 5th Generation AMD EPYC Processor server. </para>

        <para>The following example <command>numactl</command> output comes from a VM configured as
          explained:</para>

        <screen>
vm1:~ # numactl --hardware
available: 2 nodes (0-1)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28
 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58
 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88
 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113
 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135
 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157
 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201
 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223
 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245
 246 247 248 249 250 251 252 253 254 255
node 0 size: 338489 MB
node 0 free: 337385 MB
node 1 cpus: 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274
 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296
 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318
 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340
 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362
 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384
 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406
 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428
 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450
 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472
 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494
 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511
node 1 size: 338432 MB
node 1 free: 337252 MB
node distances:
node   0   1
  0:  10  32
  1:  32  10
        </screen>

        <para> This is analogous to the host topology, also from the point of view of NUMA node
          distances (as the <package>libvirt</package> version present in SUSE Linux Enterprise
          Server 15 SP6 allows us to define the virtual nodes distance table). The only differences
          are the APIC IDs of the CPUs and the amount of memory. </para>

        <para>See <xref linkend="sec-appendix-a"/> for a complete VM configuration file.</para>

        <para>As said already, full cores must always be used. If possible always fully use CCXes/dies too.
          Since each die has 16 CPUs, that means that a VM with 512 vCPUs will use all the 16 CCXes, on each of the 2 nodes
          (as 16 x 16 x 2 is indeed 512).
          So, for example, vCPUs 0 to 15 can be assigned to Cores L#0 to L#7 (and hence to CPUs P#0 to P#7 and P#256 to P#263), on node P#0;
          vCPUs 16 to 31 to Cores L#$8 to L#15, and so on (and the same on node P#1).
          In fact, this is what we call coherent 1-to-1 mapping between virtual and physical topologies.</para>

      </sect3>

      <sect3 xml:id="sec-placement-two-vms">
        <title>Placement of two large VMs</title>

        <para> If wanting to run two VMs, they both can have 256 vCPUs and 336 GB memory each. Therefore, each one can fit in one of the host's NUMA nodes.
          This also means that there is no need to define virtual NUMA nodes in their configuration.</para>

        <para>Placing each VM on one node means that workloads running inside each of them will
          never need to use inter-socket interconnect.</para>

        <para>In this example scenario, <package>numactl</package> output of both VMs looks as
          follows:</para>

        <screen>
vm1:~ # numactl --hardware
available: 1 nodes (0)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28
 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58
 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88
 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113
 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135
 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157
 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201
 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223
 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245
 246 247 248 249 250 251 252 253 254 255
node 0 size: 338246 MB
node 0 free: 337317 MB
node distances:
node   0
  0:  10
        </screen>

      </sect3>

      <sect3 xml:id="sec-placement-ten-vms">
        <title>Placement of four to thirty-two medium-size VMs</title>

        <para>Following the same principles, we can <quote>split</quote> each node in 2, and have 4
          VMs with 128 vCPUs and up to 168 GB of RAM each. It is also feasible to have 8 VMs with
          64 vCPUs and up to 84 GB of RAM each, 16 VMs with 32 vCPUs and 42 GB RAM, or even
          32 VMs, all with 16 vCPUs and 21 GB RAM.</para>

        <para>In the first case, each VM will span 8 host dies. In the second and in the third cases, 4 and 2 dies.
          And in the fourth one, each VM will be pinned to 1 die.</para>

        <para> In all these situations, VMs do not need to (and, therefore, should not) be placed across the host NUMA nodes
          boundary, and hence they do not need to be NUMA-aware. They still benefit from having a
          virtual topology, in terms of cores, threads and dies which matches the resource they are
          using (discussed in more details about in the following section). </para>

      </sect3>

      <sect3 xml:id="sec-placement-lots-vms">
        <title>Placement of many small VMs</title>

        <para>If small VMs are the target, VMs with either 8, 4 or 2 vCPUs each can be created and
          efficiently allocated on AMD EPYC 9005 Series Processors servers.</para>

        <para> VMs with 8 vCPUs <quote>occupies</quote> half a die, and we can have 64 of them. VMs
          with either 4 or 2 vCPUs will be given 2 and 1 host cores and we can have 128 and 256 of
          them, respectively. And this is all possible without the need for any of the VMs to span
          more than one host NUMA node. </para>

        <para>Of course, in all these cases, VMs are sharing dies, which means VMs will interfere with each other via the L3 caches.
          This may or may not be a problem, but there is no way around it, as soon as more VMs than the number of available dies are necessary.
          The performance impact of such sharing should be tolerable, in most cases, for these configurations, but this needs to be assessed with tests and benchmarks.
          If that is not the case, then the recommendation is to not go above 32 VMs.
          More clever (but also more complex) tuning strategies can be designed, but they would require further a-priori knowledge of
          the workload and/or the load profile of the various VMs, which may not be always available (or can change during the lifecycle
          of the VMs).</para>

        <note>
          <title>Hundreds of VMs with limited RAM size and disk space</title>
          <para> When the number of VM increases so much, the limiting factor will become the
            memory or the disk. With ~700 GB of RAM available for VMs on our server, we can give only 2 GB of
            RAM to each of the 256 VMs of the last example above. Of course, this limitation is only an issue of the system
            used as reference for this guide. The AMD EPIC 9005 Series Processor architecture itself
            can accommodate much more RAM. </para>
        </note>

        <para> In cases when it is enough or desirable to have VMs with only 1 vCPUs, it is
          recommended to still not go beyond 256, and assign no less than one full core to each VM.
          In fact, it should be avoided to have vCPUs from different VMs running on two siblings threads of
          the same core. Such a solution, although functional, is not ideal for all the cases where
          good and consistent performance within VMs is a goal (as well as for security concerns, but that is out of the scope of this guide).
          What can be done for each VM and for
          each core is to use one of the two threads for the actual vCPU, and the other for its IO
          thread(s), on the host. This solution allows to take advantage of the full 5th Generation
          AMD EPYC processing power. However, we advise to check and verify whether this is a good
          setup and if it brings performance benefits for the specific workloads of interest, before
          committing to it. </para>

      </sect3>

    </sect2>

    <sect2 xml:id="sec-emulator-disaggregation">
      <title>Emulator IO threads &amp; disaggregation</title>

      <para> IO for the VMs is carried out by device emulators running on the host, or by the
        so-called <emphasis role="italic">back-ends</emphasis> of paravirtualized drivers (also
        running on the host). Both the IO threads of the emulators and the back-ends are seen as
        (either user or kernel) threads by the host OS. As such, they can be configured to run on
        specific subsets of the CPUs assigned to the host OS (Dom0’s virtual CPUs, in the case of
        Xen). And their placement on such CPUs may have an impact on performance of VMs, especially
        on IO bound workloads. </para>

      <para> For example, a 1 vCPU VM can have its vCPU bound to the one SMT thread of a core, while
        the other thread can be assigned to the host, and the IO thread(s) of the VM pinned to it.
        The idea behind this is that a common execution pattern for the vCPU is to be idle when the
        IO threads are busy. Hence this setup potentially maximizes the exploitation of hardware
        resources. </para>

      <para> On Xen, there is also the possibility of setting up driver domains. These are special
        VMs which act as back-ends of a particular IO device, for one or more VMs. In case they are
        used, make sure that they run <quote>close</quote> enough to both the hardware they are
        providing their abstraction for, and the VMs that they are servicing. </para>

    </sect2>

    <sect2 xml:id="sec-oversubscription-host">
      <title>Oversubscription of host resources</title>

      <para> Oversubscription happens when the demand for some resource is higher than the resource
        is physically available. In virtualization, this can happen for CPU and memory. </para>

      <note>
        <title>Not covering oversubscribed scenarios</title>
        <para> This guide does not cover in great details oversubscribed scenarios. However, given
          the large number of CPUs available on an AMD EPYC 9005 Series Processor system and the
          huge amount of memory the architecture supports, this is not considered a limitation.
        </para>
      </note>

      <bridgehead>CPU Oversubscription</bridgehead>

      <para> CPU oversubscription happens when the total cumulative number of vCPUs from all VMs
        becomes higher than 512. Such a situation inevitably introduces latencies, resulting in lower 
        performance compared to when host resources are sufficient. It is, however, impossible to tell a
        priori by what extent this happens, at least not without a detailed knowledge of the actual
        workload. </para>

      <para> If we knew in advance that the load on each vCPU will always stay below 50%, we could
        assume that even oversubscribing the host by a factor of 2 (like with ~1024 vCPUs in total, in our
        case!) will work well. On the other hand, if we knew that the load that each vCPU tries to
        impose on the system is always 100%, creating even 1 vCPUs more than the host has pCPUs
        can be considered a misconfiguration. </para>

      <para> When oversubscription is necessary, exclusive 1-to-1 vCPU to pCPU assignment may not the best
        approach and we need to trust the hypervisor scheduler to handle the situation well. What can be done, though, is providing such
        scheduler at least with some <quote>suggestions</quote>, if we have enough knowledge about
        the workload. For example, we can try to help the scheduler avoiding that all the VMs
        running CPU intensive workloads end on the same nodes, or avoiding cross-node VM migrations
        happening too frequently. </para>

      <para> This grouping of VMs on (a set of) nodes can be done with some less strict forms of
        vCPU pinning (often called <quote>affinity</quote>), or with other mechanisms. For example,
          <package>cgroups</package> can be leveraged on a KVM host, or <package>cpupools</package>
        on a Xen host. Xen also contains a feature, called <emphasis role="italic">soft vCPU
          affinity</emphasis>, which can be used together with <quote>traditional</quote> vCPU
        affinity (also called <emphasis role="italic">hard vCPU affinity</emphasis>) as a finer
        grained and more powerful way of controlling resource allocation in oversubscribed scenario. </para>

      <bridgehead>Memory Oversubscription</bridgehead>

      <para> Memory oversubscription happens when the total cumulative amount of memory used by all
        VMs is more than the RAM available on the host. In this case, some of such memory is kept
        outside of the RAM (<emphasis role="italic">swapped out</emphasis>) when the VMs using it
        are not running. It is put back in RAM (<emphasis role="italic">swapped in</emphasis>) when
        these VMs run again. This also has (potentially severe) performance implications, and is not
        analyzed in details in here. </para>

      <para> On KVM, swapping is handled by Linux virtual memory management and paging code and
        mechanism, exactly as it happens for host/bare metal processes. On Xen, this is not
        supported unless special technologies (<package>xenpaging</package> or transcendent memory)
        are employed. </para>

      <para>Then there are page sharing and memory ballooning.</para>
      <para>Page sharing relies on the principle that, when VMs use memory pages which are
        identical, only one copy of them needs to be kept in physical RAM, at least for most of the
        time. This is supported on KVM, via a mechanism called Kernel Samepage Merging (KSM). On
        Xen, this again require special tools and actions.</para>

      <para>Ballooning relies on the idea that VMs do not always use all the memory they are
        configured to be able to use. This is supported in both Xen and KVM. However, using it is
        not ideal for performance, thus it is not analyzed in any further details in this document. </para>

      <bridgehead>Oversubscription with a Single VM</bridgehead>

      <para>This is always considered a misconfiguration. In fact:</para>

      <itemizedlist>
        <listitem>
          <para>A single VM should never have more vCPUs than the host has pCPUs.</para>
        </listitem>
        <listitem>
          <para>A single VM should never have more memory than the host has physical RAM.</para>
        </listitem>
      </itemizedlist>

    </sect2>

    <sect2 xml:id="sec-enlightenment-vms">
      <title>Enlightenment of VMs</title>

      <para> The term <quote>enlightenment</quote> refers to letting the guest OS know as many
        details as possible about the virtual topology of the VM. This is only useful and brings
        actual performance improvements <emphasis>only</emphasis> if such topology is properly and
        effectively mapped on host resources, and if such mapping is stable and does not change
        during the life of the VM. </para>

      <sect3 xml:id="sec-CPU-topology-vm">
        <title>Virtual CPUs model and topology</title>

        <para>To ensure the VM has a vCPU topology, use the following:</para>

        <screen>
    &lt;cpu mode="host-model" check="none">
    &lt;topology sockets="2" dies="16" cores="8" threads="2"/>
    &lt;numa>
      &lt;cell id="0" cpus="0-255" memory="352321536" unit="KiB">
        &lt;distances>
          &lt;sibling id="0" value="10"/>
          &lt;sibling id="1" value="32"/>
        &lt;/distances>
      &lt;/cell>
      &lt;cell id="1" cpus="256-511" memory="352321536" unit="KiB">
        &lt;distances>
          &lt;sibling id="0" value="32"/>
          &lt;sibling id="1" value="10"/>
        &lt;/distances>
      &lt;/cell>
    &lt;/numa>
  &lt;/cpu>
        </screen>

        <para> The <parameter>&lt;topology></parameter> element specifies the CPU
          characteristics. In this case, we are creating vCPUs which will be seen by the guest OS as
          being arranged in 2 sockets, each of which has 16 dies, each of which has 8 cores with 2 threads (that is 16 CPUs).
          And this is how we match, for the one big VM, the topology of the host. </para>

        <para> Each <parameter>&lt;cell></parameter> element defines one virtual NUMA node,
          specifying how much memory and which vCPUs are part of it. The
            <parameter>&lt;distances></parameter> elements allow us to define a custom virtual
          NUMA distance table. Values should be - as it is on actual hardware - an indication of the
          latency of accessing the memory of every node from any other. And again we are defining a
          virtual distance table which exactly matches the host one, for the one big VM. </para>

        <para> CPU model is how the VM is told what type of vCPUs should be <quote>emulated</quote>,
          for example, what features and special instructions and extensions will be available. To
          achieve best performance, using <parameter>model='host-passthrough'</parameter> is often
          the right choice. However, at least on SUSE Linux Enterprise Server 15 SP6, this produces
          wrong results. In fact, such setting prevents the topology - despite it being described
          correctly in the <parameter>&lt;topology></parameter> element - from being interpreted
          correctly, inside the VM. This is visible in the output of the command
            <command>lscpu</command>: </para>

        <screen>
vm1:~ # lscpu
...
CPU(s):                   512
  On-line CPU(s) list:    0-511
Vendor ID:                AuthenticAMD
  Model name:             AMD EPYC 9755 128-Core Processor
    CPU family:           26
    Model:                2
    Thread(s) per core:   1
    Core(s) per socket:   512
    Socket(s):            1
...
Caches (sum of all):
  L1d:                    32 MiB (512 instances)
  L1i:                    32 MiB (512 instances)
  L2:                     256 MiB (512 instances)
  L3:                     8 GiB (512 instances)
...
       </screen>

        <para>Both the CPU topology and the cache layout, as seen from inside of the VM, are completely wrong, and can affect performance of applications running inside the VM.
          In fact, not only the sizes of the various cache levels are wrong. It is also the information about which CPUs share what caches that is completely misconfigured, as shown also here: </para>

        <screen>
vm1:~ # cat /sys/devices/system/cpu/cpu0/cache/index0/shared_cpu_list
0
vm1:~ # cat /sys/devices/system/cpu/cpu0/cache/index1/shared_cpu_list
0
vm1:~ # cat /sys/devices/system/cpu/cpu0/cache/index2/shared_cpu_list
0
vm1:~ # cat /sys/devices/system/cpu/cpu0/cache/index3/shared_cpu_list
0-255
        </screen>

        <para> The VM thinks that L1d, L1i and L2 are private of each vCPU, while that is not the case, as
          they are shared by the two threads of each core. Even worse, it thinks that L3 is shared
          across all the vCPUs of a socket (and that there is only one socket!), which is not true, as it is shared only among 8 cores
          (that is 16 vCPUs). </para>

        <para> For this reason, the recommendation is to use <parameter>EPYC-Genoa</parameter> as the CPU
          model (as an appropriate model for 5th Generation AMD EPYC Processors is not yet available) either manually or by using <parameter>model='host-model'</parameter>.
          This enables a virtual topology inside the VM what is more consistent (although not perfectly identical)
          with the one of the host, and benchmarks show a beneficial effect on performance coming
          from that. The output of <command>lscpu</command> with <parameter>EPYC-Genoa</parameter> as a
          model is shown below: </para>

        <screen>
vm1:~ # lscpu
...
CPU(s):                   512
  On-line CPU(s) list:    0-511
Vendor ID:                AuthenticAMD
  Model name:             AMD EPYC-Genoa Processor
    CPU family:           25
    Model:                17
    Thread(s) per core:   2
    Core(s) per socket:   128
    Socket(s):            2
...
Caches (sum of all):
  L1d:                    8 MiB (256 instances)
  L1i:                    8 MiB (256 instances)
  L2:                     256 MiB (256 instances)
  L3:                     1 GiB (32 instances)
...
        </screen>

        <para>Of course, now it is the <parameter>Model name:</parameter> that is not completely correct.
          But both the CPU and caches layout is much more in line with what we wanted to achieve by tuning the VM configuration.</para>

        <para>In fact, even if the cache sizes that are slightly off, with regard to the host value. And
          this is because they are the values taken from previous generation AMD EPYC Processors
          architecture characteristics. But the cache topology and cache level sharing among vCPUs
          are now correct (as shown below). And this is far more important for achieving good performance
          inside the VM. </para>

        <screen>
vm1:~ # cat /sys/devices/system/cpu/cpu0/cache/index0/shared_cpu_list
0-1
vm1:~ # cat /sys/devices/system/cpu/cpu0/cache/index1/shared_cpu_list
0-1
vm1:~ # cat /sys/devices/system/cpu/cpu0/cache/index2/shared_cpu_list
0-1
vm1:~ # cat /sys/devices/system/cpu/cpu0/cache/index3/shared_cpu_list
0-15
        </screen>

        <para>Note that, to achieve the outcome shown above, it is very important to use the following CPU topology description string, in the VM configuration:</para>

        <screen>&lt;topology sockets="2" dies="16" cores="8" threads="2"/></screen>

        <para>In particular, do not omit the <parameter>dies="16"</parameter> element, as that is what enables the VM to see its vCPUs grouped in dies (the CCXes of the host).
          In fact, using something like this <parameter>&lt;topology sockets="2" cores="128" threads="2"/></parameter> would result in a correct CPU topology, but the representation of the cache layout would still be inaccurate, such as:</para>

        <screen>
vm1:~ # lscpu
...
CPU(s):                   512
  On-line CPU(s) list:    0-511
Vendor ID:                AuthenticAMD
  Model name:             AMD EPYC-Genoa Processor
    CPU family:           25
    Model:                17
    Thread(s) per core:   2
    Core(s) per socket:   128
    Socket(s):            2
...
Caches (sum of all):
  L1d:                    8 MiB (256 instances)
  L1i:                    8 MiB (256 instances)
  L2:                     256 MiB (256 instances)
  L3:                     64 MiB (2 instances)
...
vm1:~ # cat /sys/devices/system/cpu/cpu0/cache/index0/shared_cpu_list
0-1
vm1:~ # cat /sys/devices/system/cpu/cpu0/cache/index1/shared_cpu_list
0-1
vm1:~ # cat /sys/devices/system/cpu/cpu0/cache/index2/shared_cpu_list
0-1
vm1:~ # cat /sys/devices/system/cpu/cpu0/cache/index3/shared_cpu_list
0-255
...
        </screen>

        <para>As a further example, in case we have two VMs, one for each NUMA node and with 256 vCPUs each, both are also configured to have 16 dies (but, of course, just 1 socket), like this:</para>

        <screen>&lt;topology sockets="1" dies="16" cores="8" threads="2"/></screen>

        <para>And the topology, from inside each VM, will look as follows:</para>

        <screen>
vm1:~ # lscpu
...
CPU(s):                   256
  On-line CPU(s) list:    0-255
Vendor ID:                AuthenticAMD
  Model name:             AMD EPYC-Genoa Processor
    CPU family:           25
    Model:                17
    Thread(s) per core:   2
    Core(s) per socket:   128
    Socket(s):            1
...
Caches (sum of all):
  L1d:                    4 MiB (128 instances)
  L1i:                    4 MiB (128 instances)
  L2:                     128 MiB (128 instances)
  L3:                     512 MiB (16 instances)
...

vm1:~ # cat /sys/devices/system/cpu/cpu0/cache/index0/shared_cpu_list
0-1
vm1:~ # cat /sys/devices/system/cpu/cpu0/cache/index1/shared_cpu_list
0-1
vm1:~ # cat /sys/devices/system/cpu/cpu0/cache/index2/shared_cpu_list
0-1
vm1:~ # cat /sys/devices/system/cpu/cpu0/cache/index3/shared_cpu_list
0-15
        </screen>

        <para>Finally, on SUSE Linux Enterprise Server 15 SP6 it is possible to select
          the <parameter>haltpoll</parameter> CPU-Idle governor, inside the VM, even if we are using <parameter>model='host-model'</parameter>
          (although, it is only partially effective, as <parameter>model='host-passthourgh'</parameter> would be necessary
          in order for the VM to be able to exploit this feature at its full potential).</para>

        <para>This is an optimization meant at reducing the number of context switches between the VM and the host
          (also known as VMExits), when doing static resource partitioning of the host itself.
          For that reason, it is something that we recommend using, although the actual benefit provides highly
          depends on the workload running inside of the VM.</para>

        <para>For enabling it, just load the kernel module (inside of the VM, of course): </para>

        <screen>
vm1:~ # modprobe cpuidle-haltpoll
        </screen>

      </sect3>


      <sect3 xml:id="sec-memory-backing">
        <title>Memory backing</title>

        <para> Memory wise, the VMs must be told to use the Huge Pages that were reserved for
          them. To effectively use 1 GB huge pages, the amount of memory each VM is given must be a
          multiple of 1 GB. Also, KSM should be disabled and we also must ensure that VMs' memory is
          never going to be swapped out. This is all achieved as follows:</para>

        <screen>
&lt;memory unit='KiB'>704643072&lt;/memory>
&lt;memoryBacking>
  &lt;hugepages>
    &lt;page size='1' unit='GiB'/>
  &lt;/hugepages>
  &lt;nosharepages/>
&lt;/memoryBacking>
        </screen>

        <para>To verify that the appropriate type of memory is being used by the VMs,
          one can check the content of <parameter>/proc/meminfo</parameter>, with the VMs running,
          and observe that all the pre-allocated Huge Pages are actually occupied.</para>

        <screen>
host:~ # cat /proc/meminfo | grep Huge
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
FileHugePages:         0 kB
HugePages_Total:     672
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:    1048576 kB
Hugetlb:       704643072 kB
        </screen>

        <note>
          <title>Huge Pages, shared pages and locked pages on Xen</title>
          <para> On Xen, for HVM guests, huge pages are used by default and there is neither any
            page sharing nor swapping (at least not in SUSE Linux Enterprise Server 15 SP6).
            Therefore, the entire <parameter>&lt;memoryBacking></parameter> element is
            technically not necessary. </para>
        </note>

      </sect3>

      <sect3 xml:id="sec-no-ballooning">
        <title>Ballooning</title>

        <para> To get the full benefit of Huge Pages, memory ballooning must also be disabled. In
          fact, if the ballooning driver is not Huge Pages-aware, using ballooning would end up
          splitting the pages in fragments, neutering their (positive) performance impact. Disabling
          memory ballooning is done as follows: </para>

        <screen>
&lt;memory unit='KiB'>704643072&lt;/memory>
&lt;currentMemory unit='KiB'>704643072&lt;/currentMemory>
        </screen>

        <para> That is, by specifying in the <parameter>&lt;currentMemory></parameter> the same
          value already used in the <parameter>&lt;memory></parameter> element (and never
          changing the memory size of the VM at runtime). </para>

      </sect3>

      <sect3 xml:id="sec-vm-transparent-huge-pages">
        <title>(Transparent) Huge Pages</title>

        <para> If huge pages are used for allocating the VMs’ memory on the host, they can also be
          used inside the VMs, either explicitly, or via THP. Whether that helps performance is
          workload dependent. The analysis and the considerations made in the first part of the
          document about using (T)HP on bare metal, can also be applied here. </para>

      </sect3>

      <sect3 xml:id="sec-vm-automatic-numa-balancing">
        <title>Automatic NUMA balancing</title>

        <para> Similarly to THP, if the VM is NUMA-aware, NUMAB can be used inside of it to boost
          the performance of most NUMA-unaware workloads running inside the VM itself. </para>

      </sect3>

      <sect3 xml:id="sec-vm-services-daemons">
        <title>Services and daemons</title>

        <para>
          <package>irqbalance</package> can be a source of latency inside of the VM, because of the
          way it uses the <filename>/proc/interrupts</filename> interface. For workloads that are
          particularly sensitive to latency, consider disabling it within the VMs (of course by
          taking the appropriate alternative measures, like binding IRQs, if necessary). </para>

      </sect3>

    </sect2>

    <sect2 xml:id="sec-sev-vm">
      <title>Secure Encrypted Virtualization with Encrypted State</title>

      <para>The following documents:</para>
      <itemizedlist>
        <listitem>
          <para>
            <link xlink:href="https://libvirt.org/kbase/launch_security_sev.html"> Libvirt
              Documentation: Launch security with AMD SEV</link>
          </para>
        </listitem>
        <listitem>
          <para>
            <link
              xlink:href="https://documentation.suse.com/sles/15-SP4/html/SLES-amd-sev/article-amd-sev.html"
              > SUSE Linux Enterprise Server 15 SP4: AMD Secure Encrypted Virtualization (AMD-SEV)
              Guide</link>
          </para>
        </listitem>
      </itemizedlist>
      <para>Explain how to configure a VM to use AMD SEV-ES (on an AMD SEV-ES capable and
        properly configured host) is not in the scope of this document. Refer to the linked materials for the details. </para>

      <para>It is possible to check if the host is properly configured to run SEV-ES VMs with the following command:</para>

      <screen>
[    0.000000] SEV-SNP: Memory for the RMP table has not been reserved by BIOS
[   33.954913] ccp 0000:55:00.5: SEV API:1.55 build:40
[   33.983047] ccp 0000:55:00.5: SEV API:1.55 build:40
[   41.100388] kvm_amd: SEV enabled (ASIDs 36 - 1006)
[   41.106817] kvm_amd: SEV-ES enabled (ASIDs 1 - 35)
      </screen>

      <para>Once inside of the VM, this is how one can check whether it is only the memory that is being encrypted (with SEV):</para>

      <screen>
dmesg | grep SEV
[    0.069436] AMD Memory Encryption Features active: SEV
      </screen>

      <para>Or if we are also fully encrypting the VM state (with SEV-ES):</para>

      <screen>
dmesg | grep SEV
[    0.069436] AMD Memory Encryption Features active: SEV SEV-ES
      </screen>

      <bridgehead>SEV and The IOMMU Device</bridgehead>

      <para>Note that it is not possible to provide a virtual IOMMU device to an encrypted VM.
        This means that, for having a functional SEV or SEV-ES VM, we need to remove the
          <parameter>iommu</parameter> element, in the <parameter>&lt;device></parameter>
        section of the VM configuration. And since that element is necessary for having more than 255
        vCPUs in the VM, all the experiments conducted with SEV enabled have been done in VMs with
        at most 128 vCPUs.</para>

    </sect2>

  </sect1>

  <sect1 xml:id="sec-virtualization-test-workload-stream">
    <title>Test VM workload: STREAM</title>

    <para>The effectiveness of the proposed tuning is demonstrated using the STREAM
      benchmark.</para>

    <sect2 xml:id="sec-virt-test-stream-onevm">
      <title>Test scenario: One large VM</title>

      <para><xref linkend="fig-stream-bm-vm"/> shows the bandwidth achieved by STREAM when the
        benchmark is built in single thread mode ('single') or parallelized with OpenMP
        ('openmp'). For both cases, we compare the results achieved on the host ('BM') and
        inside one VM ('VM'). </para>


      <figure xml:id="fig-stream-bm-vm">
        <title>STREAM Bandwidth - Bare metal compared with one VM</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="EPYC5-stream-bm-1vm.svg" width="80%" format="SVG"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="EPYC5-stream-bm-1vm.svg" width="80%" format="SVG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Both the single thread and the parallel results are identical (within 1% difference!) between bare metal (blue and yellow rectangles)
        and inside of the VM (orange and green rectangles), for all the operations (<parameter>Copy</parameter>, <parameter>Scale</parameter>, <parameter>Add</parameter> and
        <parameter>Triad</parameter>) of the benchmark.
      </para>

      <para> This clearly shows how proper tuning allows a single VM running on an AMD EPYC 7005 Series
        Processor server to achieve a memory bandwidth that matches the one that we can reach directly on the host. </para>

      <note>
        <para>Inside of the VM, the STREAM benchmark was configured almost identically to what has been
         shown already in <xref linkend="sec-workload-stream"/>. </para>
      </note>

      <para><xref linkend="fig-stream-single-1vm-model"/> and <xref linkend="fig-stream-omp-1vm-model"/>
        show the effect of using different CPU models for the virtual machine. We see that, as long as
        a single thread is used, the CPU model chosen for the VM is completely irrelevant.</para>

      <para>On the other hand, the OpenMP run clearly shows some problems when the <parameter>host-passthrough</parameter> CPU model
        is selected. In fact, since using that model builds a VM with only 2 LLCs (and also because of the other problems with the cache topology),
        running STREAM with twice as many threads as there are LLCs in the system results in the benchmark spawning only 4 of them (yellow and green rectangles).
        And this, of course, dramatically reduces the performance. We can also see that, if we instead manually set the number of threads
        to the value that we know to be the best for this VM (that is 64, dark red and cyan rectangles) performance are restored to how good
        we know things can be from <xref linkend="fig-stream-bm-vm"/> (and from the <parameter>cpumodel</parameter> results, see the blue and orange rectangles).</para>

      <para>It is also interesting to note that with <parameter>cpumodel+haltpoll</parameter> (orange rectangles) we reach the same performance of the configuration
        that should theoretically be the absolute best one (<parameter>cpupassthrough+haltpoll (64 threads)</parameter>, cyan rectangle). This means that it is fine
        to stick with the former, and it is not worthwhile setting <parameter>cpupassthrough</parameter> and coping manually with the misconfigured cache hierarchy inside of the VM
        (as that could potentially be much more complicated it has been here for the STREAM benchmark or, sometimes, even impossible). </para>

      <figure xml:id="fig-stream-single-1vm-model">
        <title>STREAM Bandwidth - Single thread in one VM with different CPU models</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="EPYC5-stream-single-1vm-model.svg" width="80%" format="SVG"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="EPYC5-stream-single-1vm-model.svg" width="80%" format="SVG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <figure xml:id="fig-stream-omp-1vm-model">
        <title>STREAM Bandwidth - OpenMP in one VM with different CPU models</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="EPYC5-stream-omp-1vm-model.svg" width="80%" format="SVG"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="EPYC5-stream-omp-1vm-model.svg" width="80%" format="SVG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>This situation also confirms the need to always run benchmarks for assessing the performance of the
        relevant workloads, on a given platform. In fact, because of specific characteristics of
        some components of the virtualization stack available in SUSE Linux Enterprise Server 15
        SP4, the <parameter>EPYC-Milan</parameter> (or, equivalently, the <parameter>host-model</parameter>) CPU model
        might be preferable to what it would have appeared to be the most obvious choice (<parameter>host-passthrough</parameter>).</para>

      <para>Finally, about the CPUIdle <parameter>haltpoll</parameter> governor, it is recommended to enable and make use of it,
        although for STREAM (and we can guess also for other, similar, memory intensive benchmarks) it introduces no significant performance difference.</para>

    </sect2>

    <sect2 xml:id="sec-virt-test-stream-twovm-all">
      <title>Test scenario: Multiple VMs</title>

      <para>
        <xref linkend="fig-stream-single-vms-avg"/> and <xref linkend="fig-stream-single-vms-sum"/> show what happens when 1, 2, 4, 8, 16 and 32 VMs are used.
        The former reports the average bandwidth achieved in each experiments, considering all the VMs involved. For example, the yellow blue bar (4 VMS) is relative to an
        experiment where 4 VMs were concurrently running the STREAM benchmark (in single thread mode) and represents the average of the 4 different memory bandwidth values,
        one for each of such VMs.
        The latter shows something similar, but the bars represents the cumulative STREAM bandwidth achieved in each experiments, considering all the VMs involved.
        For example, the cyan bar (32 VMs) is relative to an experiment where 32 VMs were concurrently running the STREAM benchmark (in single thread mode) and represents the sum
        of the 24 different memory bandwidth values, one of each of such VMs.
      </para>

      <figure xml:id="fig-stream-single-vms-avg">
        <title>STREAM Bandwidth - Single, average of the bandwidth achieved among all VMs of each group</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="EPYC5-stream-single-vms-avg.svg" width="80%" format="PNG"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="EPYC5-stream-single-vms-avg.svg" width="80%" format="PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <figure xml:id="fig-stream-single-vms-sum">
        <title>STREAM Bandwidth - Single, sum of the bandwidth achieved within all VMs of each group</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="EPYC5-stream-single-vms-sum.svg" width="80%" format="PNG"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="EPYC5-stream-single-vms-sum.svg" width="80%" format="PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>We see in <xref linkend="fig-stream-single-vms-avg"/> how the single STREAM bandwidth suffers a bit of a decline as more VMs are packed on the NUMA nodes and, hence,
        compete for the bandwidth of the memory controllers. However,
         <xref linkend="fig-stream-single-vms-sum"/> reminds us how the total bandwidth achieved, if we consider all the VMs involved in each experiments, actually goes up (and
        it scales roughly linearly, at least for some of the STREAM operations),
        until it reaches the same level that we know (for example, from <xref linkend="fig-stream-bm-vm"/>) it can touch.
      </para>

      <para><xref linkend="fig-stream-omp-vms-avg"/> and <xref linkend="fig-stream-omp-vms-sum"/> show the same, but when the parallel (via OpenMP) version of STREAM
        is run, inside of each VM of each experiment.</para>

      <figure xml:id="fig-stream-omp-vms-avg">
        <title>STREAM Bandwidth - OpenMP, average of the bandwidth achieved among all VMs of each group</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="EPYC5-stream-omp-vms-avg.svg" width="80%" format="PNG"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="EPYC5-stream-omp-vms-avg.svg" width="80%" format="PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <figure xml:id="fig-stream-omp-vms-sum">
        <title>STREAM Bandwidth - OpenMP, sum of the bandwidth achieved within all VMs of each group</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="EPYC5-stream-omp-vms-sum.svg" width="80%" format="PNG"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="EPYC5-stream-omp-vms-sum.svg" width="80%" format="PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>In <xref linkend="fig-stream-omp-vms-avg"/> we see that performance scales down in a close to linear fashion with the increase
        of the number of VMs involved (although, not perfectly linearly for all the type of STREAM operations).
        At the same time, <xref linkend="fig-stream-omp-vms-sum"/> shows how the total cumulative bandwidth of each experiments stays pretty much always at its top levels,
        as one would expect. Actually, it even seems that using many small VMs may make it possible to exploit even better (as compared to what happens when few large
        VMs are running) the large memory bandwidth provided by the memory controllers of the AMD EPYC 9005 Series Processors.
      </para>

    </sect2>

    <sect2 xml:id="sec-virt-test-stream-sev">
      <title>Test scenario: Secure Encrypted Virtualization</title>

      <para>The goal for this test was evaluating the impact on the performance of encrypting the VMs with SEV and with SEV-ES, when running a memory
        intensive workload like STREAM.</para>

      <para>Considering the case when 4 VMs are concurrently running on the host and are executing the STREAM benchmark, <xref linkend="fig-stream-single-sev"/>
        shows the average bandwidth of the single thread execution of the benchmark in both such VMs when memory is not encrypted (blue bars), when memory is
        encrypted with SEV (orange bar) and with both memory and state are encrypted with SEV-ES (yellow bar).</para>

      <figure xml:id="fig-stream-single-sev">
        <title>STREAM Bandwidth - Single, average bandwidth in 4 VMs when unencrypted, encrypted with SEV and with SEV-ES</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="EPYC4-stream-single-sev-avg.svg" width="80%" format="PNG"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="EPYC4-stream-single-sev-avg.svg" width="80%" format="PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <figure xml:id="fig-stream-openmp-sev">
        <title>STREAM Bandwidth - OpenMP, average bandwidth in 4 VMs when unencrypted, encrypted with SEV and with SEV-ES</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="EPYC4-stream-openmp-sev-avg.svg" width="80%" format="PNG"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="EPYC4-stream-openmp-sev-avg.svg" width="80%" format="PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>It is quite evident how both SEV and SEV-ES have a close to nonexistent impact on the performance
        for this workload. Actually, it may even seem that, in the OpenMP case, encrypted VMs are faster, but we
        should note how all the results fall within the other ones’ error bars.</para>

    </sect2>

  </sect1>

  <sect1 xml:id="sec-virtualization-test-workload-npb">
    <title>Test VM workload: NPB</title>

    <para>Trying to mach the experimental evaluation done on bare-metal, NPB base workloads have also been
      considered.</para>

    <sect2 xml:id="sec-virt-test-npb-onevm">
      <title>Test scenario: One large VM</title>

      <para><xref linkend="fig-nas-bm-1vm"/> shows the results of running the various NPB workloads
        on the host and in one VMs. In both cases, the tuning introduced above was applied (in the latter case, of course, inside of the VM itself).</para>

      <figure xml:id="fig-nas-bm-1vm">
        <title>NAS Completion Time - bare metal and in one VM</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="EPYC5-nas-bm-1vm.svg" width="80%" format="PNG"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="EPYC5-nas-bm-1vm.svg" width="80%" format="PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para> Like for the STREAM case, a properly tuned VM can reach pretty much exactly the performance of the host. </para>

      <para><xref linkend="fig-nas-1vm-model"/> show again the impact on the performance of different CPU models.</para>

      <figure xml:id="fig-nas-1vm-model">
        <title>STREAM Bandwidth - Single thread in one VM with different CPU models</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="EPYC5-nas-1vm-model.svg" width="80%" format="SVG"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="EPYC5-nas-1vm-model.svg" width="80%" format="SVG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para> All the results are very similar and
        the only benchmark(s) where the <parameter>cpupassthrough</parameter> model seems to have an edge is <parameter>cg.D</parameter> (and, but only slightly, <parameter>sp.D</parameter>).
        Therefore, as already said for STREAM, considering that <parameter>cpumodel</parameter> runs show extremely good results, this guide recommends using it and
        avoiding the potential issues coming from having an inaccurate topology and a misleading cache hierarchy representation inside of VMs. </para>

    </sect2>

    <sect2 xml:id="sec-virt-test-npb-multivm">
      <title>Test scenario: Two to twelve VMs</title>

      <para><xref linkend="fig-nas-bm-multivm"/> depicts the same as <xref linkend="fig-nas-bm-1vm"/>,
        but for a few more cases, in terms of number of concurrently running VMs.
      </para>

      <figure xml:id="fig-nas-bm-multivm">
        <title>NAS Completion Time - bare metal and 1 to 16 VMs</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="EPYC5-nas-bm-16vm.svg" width="80%" format="PNG"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="EPYC5-nas-bm-16vm.svg" width="80%" format="PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para> We can see how the slowdown is linear (and sometimes even sub-linear) with the number of the VMs (the only exceptions being <parameter>lu.D</parameter>
        with 16 VMs). And that makes sense considering that when we increase the number of VMs, each one of them can also
        have fewer CPUs.
        This testifies the effectiveness of the suggested tuning measures, when it comes
        to guarantee a good level of isolation among multiple concurrently running VMs on the AMD EPYC 9005 Series Processors. </para>

    </sect2>

    <sect2 xml:id="sec-virt-test-npb-sev">
      <title>Test scenario: Two VMs with Secure Encrypted Virtualization</title>

      <para>Similarly to what has been done with STREAM, we evaluate here the overhead introduced by
        SEV and SEV-ES, this time for CPU-bound workloads, and we do that by running the NPB benchmarks it inside two VMs concurrently
        that are all either unencrypted, encrypted with SEV or encrypted with SEV-ES (<xref linkend="fig-nas-tuned-2vms-sev"/>).</para>

      <figure xml:id="fig-nas-tuned-2vms-sev">
        <title>NAS Completion Time - Tuned configuration in 2 VM, with and without SEV</title>
        <mediaobject>
          <imageobject role="fo">
            <imagedata fileref="EPYC5-nas-2vms-sev.svg" width="80%" format="PNG"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="EPYC5-nas-2vms-sev.svg" width="80%" format="PNG"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>As for the memory-intensive case the overhead introduced by either of the Confidential Computing
        technologies is found to be really close to zero. In fact, it seems that --at least for the workloads analyzed in
        this document-- AMD EPYC 9005 Series Processors can handle SEV and SEV-ES with even lower overhead than its predecessors.
      </para>

    </sect2>
  </sect1>

  <sect1 xml:id="sec-conclusion">
    <title>Conclusion</title>

    <para>The introduction of the AMD EPYC 9005 Series Processors continues to push
      the boundaries of what is possible for memory and
      IO-bound workloads with significantly higher bandwidth and an increased number of channels. A
      properly configured and tuned workload can exceed the performance of many contemporary
      off-the-shelf solutions even when fully customized. The symmetric and balanced nature
      of the machine makes the task of tuning a workload considerably easier, given that
      each partition can have symmetric performance. And this is a property that can turn
      out particularly handy in virtualization, as each virtual machine can be assigned
      to each one of the partitions.</para>

    <para>With SUSE Linux Enterprise 15 SP6, all the tools to monitor and tune a workload
      are readily available. You can extract the maximum performance and
      reliability running your applications on the 5th Generation AMD EPYC Processor
      platform.</para>

  </sect1>

  <sect1 xml:id="sec-resources">
    <title>Resources</title>

    <para>For more information, refer to:</para>

    <itemizedlist>
      <listitem>
        <para>AMD EPYC 9005 Series Architecture Overview (<link
            xlink:href="https://www.amd.com/content/dam/amd/en/documents/epyc-technical-docs/user-guides/58462_amd-epyc-9005-tg-architecture-overview.pdf"
            />)</para>
      </listitem>
      <listitem>
        <para>AMD Socket SP5 Power and Performance Optimization Guide for Family 1Ah Models 00h–0Fh and 10h–1Fh (<link
            xlink:href="https://devhub.amd.com//wp-content/uploads/Docs/58412_0.78.pdf"
            />)</para>
      </listitem>
      <listitem>
        <para>Optimizing Linux for Dual-Core AMD Opteron Processors (<link
            xlink:href="http://www.novell.com/traininglocator/partners/amd/4622016.pdf"
            />)</para>
      </listitem>
      <listitem>
        <para>Advanced Optimization and New Capabilities of GCC 12 (<link
            xlink:href="https://documentation.suse.com/sbp/devel-tools/html/SBP-GCC-12/index.html"
            />)</para>
      </listitem>
      <listitem>
        <para>Advanced Optimization and New Capabilities of GCC 11 (<link
            xlink:href="https://documentation.suse.com/sbp/devel-tools/html/SBP-GCC-11/index.html"
            />)</para>
      </listitem>
      <listitem>
        <para>Systems Performance: Enterprise and the Cloud by Brendan Gregg 2nd Edition(<link
            xlink:href="http://www.brendangregg.com/systems-performance-2nd-edition-book.html"
            />)</para>
      </listitem>
      <listitem>
        <para>NASA Parallel Benchmark (<link
            xlink:href="https://www.nas.nasa.gov/publications/npb.html"
            />)</para>
      </listitem>
    </itemizedlist>

    <para/>

  </sect1>


  <!-- <sect1 xml:id="sec-glossary">
    <title>Glossary</title>

    <para/>

  </sect1>-->

  <sect1 xml:id="sec-appendix-a">
    <title>Appendix A</title>

    <para>Example of a VM configuration file:</para>

    <screen>
&lt;domain type="kvm">
  &lt;name>vm1&lt;/name>
  &lt;metadata>
    &lt;libosinfo:libosinfo xmlns:libosinfo="http://libosinfo.org/xmlns/libvirt/domain/1.0">
      &lt;libosinfo:os id="http://suse.com/sle/15.2"/>
    &lt;/libosinfo:libosinfo>
  &lt;/metadata>
  &lt;memory unit="KiB">209715200&lt;/memory>
  &lt;currentMemory unit="KiB">209715200&lt;/currentMemory>
  &lt;memoryBacking>
    &lt;hugepages>
      &lt;page size="1048576" unit="KiB"/>
    &lt;/hugepages>
    &lt;nosharepages/>
    &lt;locked/>
  &lt;/memoryBacking>
  &lt;vcpu placement="static">256&lt;/vcpu>
  &lt;cputune>
    &lt;vcpupin vcpu="0" cpuset="0"/>
    &lt;vcpupin vcpu="1" cpuset="128"/>
    &lt;vcpupin vcpu="2" cpuset="1"/>
    &lt;vcpupin vcpu="3" cpuset="129"/>
    &lt;vcpupin vcpu="4" cpuset="2"/>
    &lt;vcpupin vcpu="5" cpuset="130"/>
    &lt;vcpupin vcpu="6" cpuset="3"/>
    &lt;vcpupin vcpu="7" cpuset="131"/>
    &lt;vcpupin vcpu="8" cpuset="4"/>
    &lt;vcpupin vcpu="9" cpuset="132"/>
    &lt;vcpupin vcpu="10" cpuset="5"/>
    &lt;vcpupin vcpu="11" cpuset="133"/>
    &lt;vcpupin vcpu="12" cpuset="6"/>
    &lt;vcpupin vcpu="13" cpuset="134"/>
    &lt;vcpupin vcpu="14" cpuset="7"/>
    &lt;vcpupin vcpu="15" cpuset="135"/>
    &lt;vcpupin vcpu="16" cpuset="8"/>
    &lt;vcpupin vcpu="17" cpuset="136"/>
    &lt;vcpupin vcpu="18" cpuset="9"/>
    &lt;vcpupin vcpu="19" cpuset="137"/>
    &lt;vcpupin vcpu="20" cpuset="10"/>
    &lt;vcpupin vcpu="21" cpuset="138"/>
    &lt;vcpupin vcpu="22" cpuset="11"/>
    &lt;vcpupin vcpu="23" cpuset="139"/>
    &lt;vcpupin vcpu="24" cpuset="12"/>
    &lt;vcpupin vcpu="25" cpuset="140"/>
    &lt;vcpupin vcpu="26" cpuset="13"/>
    &lt;vcpupin vcpu="27" cpuset="141"/>
    &lt;vcpupin vcpu="28" cpuset="14"/>
    &lt;vcpupin vcpu="29" cpuset="142"/>
    &lt;vcpupin vcpu="30" cpuset="15"/>
    &lt;vcpupin vcpu="31" cpuset="143"/>
    &lt;vcpupin vcpu="32" cpuset="16"/>
    &lt;vcpupin vcpu="33" cpuset="144"/>
    &lt;vcpupin vcpu="34" cpuset="17"/>
    &lt;vcpupin vcpu="35" cpuset="145"/>
    &lt;vcpupin vcpu="36" cpuset="18"/>
    &lt;vcpupin vcpu="37" cpuset="146"/>
    &lt;vcpupin vcpu="38" cpuset="19"/>
    &lt;vcpupin vcpu="39" cpuset="147"/>
    &lt;vcpupin vcpu="40" cpuset="20"/>
    &lt;vcpupin vcpu="41" cpuset="148"/>
    &lt;vcpupin vcpu="42" cpuset="21"/>
    &lt;vcpupin vcpu="43" cpuset="149"/>
    &lt;vcpupin vcpu="44" cpuset="22"/>
    &lt;vcpupin vcpu="45" cpuset="150"/>
    &lt;vcpupin vcpu="46" cpuset="23"/>
    &lt;vcpupin vcpu="47" cpuset="151"/>
    &lt;vcpupin vcpu="48" cpuset="24"/>
    &lt;vcpupin vcpu="49" cpuset="152"/>
    &lt;vcpupin vcpu="50" cpuset="25"/>
    &lt;vcpupin vcpu="51" cpuset="153"/>
    &lt;vcpupin vcpu="52" cpuset="26"/>
    &lt;vcpupin vcpu="53" cpuset="154"/>
    &lt;vcpupin vcpu="54" cpuset="27"/>
    &lt;vcpupin vcpu="55" cpuset="155"/>
    &lt;vcpupin vcpu="56" cpuset="28"/>
    &lt;vcpupin vcpu="57" cpuset="156"/>
    &lt;vcpupin vcpu="58" cpuset="29"/>
    &lt;vcpupin vcpu="59" cpuset="157"/>
    &lt;vcpupin vcpu="60" cpuset="30"/>
    &lt;vcpupin vcpu="61" cpuset="158"/>
    &lt;vcpupin vcpu="62" cpuset="31"/>
    &lt;vcpupin vcpu="63" cpuset="159"/>
    &lt;vcpupin vcpu="64" cpuset="32"/>
    &lt;vcpupin vcpu="65" cpuset="160"/>
    &lt;vcpupin vcpu="66" cpuset="33"/>
    &lt;vcpupin vcpu="67" cpuset="161"/>
    &lt;vcpupin vcpu="68" cpuset="34"/>
    &lt;vcpupin vcpu="69" cpuset="162"/>
    &lt;vcpupin vcpu="70" cpuset="35"/>
    &lt;vcpupin vcpu="71" cpuset="163"/>
    &lt;vcpupin vcpu="72" cpuset="36"/>
    &lt;vcpupin vcpu="73" cpuset="164"/>
    &lt;vcpupin vcpu="74" cpuset="37"/>
    &lt;vcpupin vcpu="75" cpuset="165"/>
    &lt;vcpupin vcpu="76" cpuset="38"/>
    &lt;vcpupin vcpu="77" cpuset="166"/>
    &lt;vcpupin vcpu="78" cpuset="39"/>
    &lt;vcpupin vcpu="79" cpuset="167"/>
    &lt;vcpupin vcpu="80" cpuset="40"/>
    &lt;vcpupin vcpu="81" cpuset="168"/>
    &lt;vcpupin vcpu="82" cpuset="41"/>
    &lt;vcpupin vcpu="83" cpuset="169"/>
    &lt;vcpupin vcpu="84" cpuset="42"/>
    &lt;vcpupin vcpu="85" cpuset="170"/>
    &lt;vcpupin vcpu="86" cpuset="43"/>
    &lt;vcpupin vcpu="87" cpuset="171"/>
    &lt;vcpupin vcpu="88" cpuset="44"/>
    &lt;vcpupin vcpu="89" cpuset="172"/>
    &lt;vcpupin vcpu="90" cpuset="45"/>
    &lt;vcpupin vcpu="91" cpuset="173"/>
    &lt;vcpupin vcpu="92" cpuset="46"/>
    &lt;vcpupin vcpu="93" cpuset="174"/>
    &lt;vcpupin vcpu="94" cpuset="47"/>
    &lt;vcpupin vcpu="95" cpuset="175"/>
    &lt;vcpupin vcpu="96" cpuset="48"/>
    &lt;vcpupin vcpu="97" cpuset="176"/>
    &lt;vcpupin vcpu="98" cpuset="49"/>
    &lt;vcpupin vcpu="99" cpuset="177"/>
    &lt;vcpupin vcpu="100" cpuset="50"/>
    &lt;vcpupin vcpu="101" cpuset="178"/>
    &lt;vcpupin vcpu="102" cpuset="51"/>
    &lt;vcpupin vcpu="103" cpuset="179"/>
    &lt;vcpupin vcpu="104" cpuset="52"/>
    &lt;vcpupin vcpu="105" cpuset="180"/>
    &lt;vcpupin vcpu="106" cpuset="53"/>
    &lt;vcpupin vcpu="107" cpuset="181"/>
    &lt;vcpupin vcpu="108" cpuset="54"/>
    &lt;vcpupin vcpu="109" cpuset="182"/>
    &lt;vcpupin vcpu="110" cpuset="55"/>
    &lt;vcpupin vcpu="111" cpuset="183"/>
    &lt;vcpupin vcpu="112" cpuset="56"/>
    &lt;vcpupin vcpu="113" cpuset="184"/>
    &lt;vcpupin vcpu="114" cpuset="57"/>
    &lt;vcpupin vcpu="115" cpuset="185"/>
    &lt;vcpupin vcpu="116" cpuset="58"/>
    &lt;vcpupin vcpu="117" cpuset="186"/>
    &lt;vcpupin vcpu="118" cpuset="59"/>
    &lt;vcpupin vcpu="119" cpuset="187"/>
    &lt;vcpupin vcpu="120" cpuset="60"/>
    &lt;vcpupin vcpu="121" cpuset="188"/>
    &lt;vcpupin vcpu="122" cpuset="61"/>
    &lt;vcpupin vcpu="123" cpuset="189"/>
    &lt;vcpupin vcpu="124" cpuset="62"/>
    &lt;vcpupin vcpu="125" cpuset="190"/>
    &lt;vcpupin vcpu="126" cpuset="63"/>
    &lt;vcpupin vcpu="127" cpuset="191"/>
    &lt;vcpupin vcpu="128" cpuset="64"/>
    &lt;vcpupin vcpu="129" cpuset="192"/>
    &lt;vcpupin vcpu="130" cpuset="65"/>
    &lt;vcpupin vcpu="131" cpuset="193"/>
    &lt;vcpupin vcpu="132" cpuset="66"/>
    &lt;vcpupin vcpu="133" cpuset="194"/>
    &lt;vcpupin vcpu="134" cpuset="67"/>
    &lt;vcpupin vcpu="135" cpuset="195"/>
    &lt;vcpupin vcpu="136" cpuset="68"/>
    &lt;vcpupin vcpu="137" cpuset="196"/>
    &lt;vcpupin vcpu="138" cpuset="69"/>
    &lt;vcpupin vcpu="139" cpuset="197"/>
    &lt;vcpupin vcpu="140" cpuset="70"/>
    &lt;vcpupin vcpu="141" cpuset="198"/>
    &lt;vcpupin vcpu="142" cpuset="71"/>
    &lt;vcpupin vcpu="143" cpuset="199"/>
    &lt;vcpupin vcpu="144" cpuset="72"/>
    &lt;vcpupin vcpu="145" cpuset="200"/>
    &lt;vcpupin vcpu="146" cpuset="73"/>
    &lt;vcpupin vcpu="147" cpuset="201"/>
    &lt;vcpupin vcpu="148" cpuset="74"/>
    &lt;vcpupin vcpu="149" cpuset="202"/>
    &lt;vcpupin vcpu="150" cpuset="75"/>
    &lt;vcpupin vcpu="151" cpuset="203"/>
    &lt;vcpupin vcpu="152" cpuset="76"/>
    &lt;vcpupin vcpu="153" cpuset="204"/>
    &lt;vcpupin vcpu="154" cpuset="77"/>
    &lt;vcpupin vcpu="155" cpuset="205"/>
    &lt;vcpupin vcpu="156" cpuset="78"/>
    &lt;vcpupin vcpu="157" cpuset="206"/>
    &lt;vcpupin vcpu="158" cpuset="79"/>
    &lt;vcpupin vcpu="159" cpuset="207"/>
    &lt;vcpupin vcpu="160" cpuset="80"/>
    &lt;vcpupin vcpu="161" cpuset="208"/>
    &lt;vcpupin vcpu="162" cpuset="81"/>
    &lt;vcpupin vcpu="163" cpuset="209"/>
    &lt;vcpupin vcpu="164" cpuset="82"/>
    &lt;vcpupin vcpu="165" cpuset="210"/>
    &lt;vcpupin vcpu="166" cpuset="83"/>
    &lt;vcpupin vcpu="167" cpuset="211"/>
    &lt;vcpupin vcpu="168" cpuset="84"/>
    &lt;vcpupin vcpu="169" cpuset="212"/>
    &lt;vcpupin vcpu="170" cpuset="85"/>
    &lt;vcpupin vcpu="171" cpuset="213"/>
    &lt;vcpupin vcpu="172" cpuset="86"/>
    &lt;vcpupin vcpu="173" cpuset="214"/>
    &lt;vcpupin vcpu="174" cpuset="87"/>
    &lt;vcpupin vcpu="175" cpuset="215"/>
    &lt;vcpupin vcpu="176" cpuset="88"/>
    &lt;vcpupin vcpu="177" cpuset="216"/>
    &lt;vcpupin vcpu="178" cpuset="89"/>
    &lt;vcpupin vcpu="179" cpuset="217"/>
    &lt;vcpupin vcpu="180" cpuset="90"/>
    &lt;vcpupin vcpu="181" cpuset="218"/>
    &lt;vcpupin vcpu="182" cpuset="91"/>
    &lt;vcpupin vcpu="183" cpuset="219"/>
    &lt;vcpupin vcpu="184" cpuset="92"/>
    &lt;vcpupin vcpu="185" cpuset="220"/>
    &lt;vcpupin vcpu="186" cpuset="93"/>
    &lt;vcpupin vcpu="187" cpuset="221"/>
    &lt;vcpupin vcpu="188" cpuset="94"/>
    &lt;vcpupin vcpu="189" cpuset="222"/>
    &lt;vcpupin vcpu="190" cpuset="95"/>
    &lt;vcpupin vcpu="191" cpuset="223"/>
    &lt;vcpupin vcpu="192" cpuset="96"/>
    &lt;vcpupin vcpu="193" cpuset="224"/>
    &lt;vcpupin vcpu="194" cpuset="97"/>
    &lt;vcpupin vcpu="195" cpuset="225"/>
    &lt;vcpupin vcpu="196" cpuset="98"/>
    &lt;vcpupin vcpu="197" cpuset="226"/>
    &lt;vcpupin vcpu="198" cpuset="99"/>
    &lt;vcpupin vcpu="199" cpuset="227"/>
    &lt;vcpupin vcpu="200" cpuset="100"/>
    &lt;vcpupin vcpu="201" cpuset="228"/>
    &lt;vcpupin vcpu="202" cpuset="101"/>
    &lt;vcpupin vcpu="203" cpuset="229"/>
    &lt;vcpupin vcpu="204" cpuset="102"/>
    &lt;vcpupin vcpu="205" cpuset="230"/>
    &lt;vcpupin vcpu="206" cpuset="103"/>
    &lt;vcpupin vcpu="207" cpuset="231"/>
    &lt;vcpupin vcpu="208" cpuset="104"/>
    &lt;vcpupin vcpu="209" cpuset="232"/>
    &lt;vcpupin vcpu="210" cpuset="105"/>
    &lt;vcpupin vcpu="211" cpuset="233"/>
    &lt;vcpupin vcpu="212" cpuset="106"/>
    &lt;vcpupin vcpu="213" cpuset="234"/>
    &lt;vcpupin vcpu="214" cpuset="107"/>
    &lt;vcpupin vcpu="215" cpuset="235"/>
    &lt;vcpupin vcpu="216" cpuset="108"/>
    &lt;vcpupin vcpu="217" cpuset="236"/>
    &lt;vcpupin vcpu="218" cpuset="109"/>
    &lt;vcpupin vcpu="219" cpuset="237"/>
    &lt;vcpupin vcpu="220" cpuset="110"/>
    &lt;vcpupin vcpu="221" cpuset="238"/>
    &lt;vcpupin vcpu="222" cpuset="111"/>
    &lt;vcpupin vcpu="223" cpuset="239"/>
    &lt;vcpupin vcpu="224" cpuset="112"/>
    &lt;vcpupin vcpu="225" cpuset="240"/>
    &lt;vcpupin vcpu="226" cpuset="113"/>
    &lt;vcpupin vcpu="227" cpuset="241"/>
    &lt;vcpupin vcpu="228" cpuset="114"/>
    &lt;vcpupin vcpu="229" cpuset="242"/>
    &lt;vcpupin vcpu="230" cpuset="115"/>
    &lt;vcpupin vcpu="231" cpuset="243"/>
    &lt;vcpupin vcpu="232" cpuset="116"/>
    &lt;vcpupin vcpu="233" cpuset="244"/>
    &lt;vcpupin vcpu="234" cpuset="117"/>
    &lt;vcpupin vcpu="235" cpuset="245"/>
    &lt;vcpupin vcpu="236" cpuset="118"/>
    &lt;vcpupin vcpu="237" cpuset="246"/>
    &lt;vcpupin vcpu="238" cpuset="119"/>
    &lt;vcpupin vcpu="239" cpuset="247"/>
    &lt;vcpupin vcpu="240" cpuset="120"/>
    &lt;vcpupin vcpu="241" cpuset="248"/>
    &lt;vcpupin vcpu="242" cpuset="121"/>
    &lt;vcpupin vcpu="243" cpuset="249"/>
    &lt;vcpupin vcpu="244" cpuset="122"/>
    &lt;vcpupin vcpu="245" cpuset="250"/>
    &lt;vcpupin vcpu="246" cpuset="123"/>
    &lt;vcpupin vcpu="247" cpuset="251"/>
    &lt;vcpupin vcpu="248" cpuset="124"/>
    &lt;vcpupin vcpu="249" cpuset="252"/>
    &lt;vcpupin vcpu="250" cpuset="125"/>
    &lt;vcpupin vcpu="251" cpuset="253"/>
    &lt;vcpupin vcpu="252" cpuset="126"/>
    &lt;vcpupin vcpu="253" cpuset="254"/>
    &lt;vcpupin vcpu="254" cpuset="127"/>
    &lt;vcpupin vcpu="255" cpuset="255"/>
  &lt;/cputune>
  &lt;numatune>
    &lt;memory mode="strict" nodeset="0-1"/>
    &lt;memnode cellid="0" mode="strict" nodeset="0"/>
    &lt;memnode cellid="1" mode="strict" nodeset="1"/>
  &lt;/numatune>
  &lt;os>
    &lt;type arch="x86_64" machine="pc-q35-4.2">hvm&lt;/type>
    &lt;loader readonly="yes" type="pflash">/usr/share/qemu/ovmf-x86_64-ms-4m-code.bin&lt;/loader>
    &lt;nvram>/var/lib/libvirt/qemu/nvram/vm1_VARS.fd&lt;/nvram>
    &lt;boot dev="hd"/>
  &lt;/os>
  &lt;features>
    &lt;acpi/>
    &lt;apic/>
    &lt;pae/>
    &lt;vmport state="off"/>
    &lt;ioapic driver="qemu"/>
  &lt;/features>
  &lt;cpu mode="custom" match="exact" check="none">
    &lt;model fallback="allow">EPYC&lt;/model>
    &lt;topology sockets="2" cores="64" threads="2"/>
    &lt;numa>
      &lt;cell id="0" cpus="0-127" memory="104857600" unit="KiB">
        &lt;distances>
          &lt;sibling id="0" value="10"/>
          &lt;sibling id="1" value="32"/>
        &lt;/distances>
      &lt;/cell>
      &lt;cell id="1" cpus="128-255" memory="104857600" unit="KiB">
        &lt;distances>
          &lt;sibling id="0" value="32"/>
          &lt;sibling id="1" value="10"/>
        &lt;/distances>
      &lt;/cell>
    &lt;/numa>
  &lt;/cpu>
  &lt;clock offset="utc">
    &lt;timer name="rtc" tickpolicy="catchup"/>
    &lt;timer name="pit" tickpolicy="delay"/>
    &lt;timer name="hpet" present="no"/>
  &lt;/clock>
  &lt;on_poweroff>destroy&lt;/on_poweroff>
  &lt;on_reboot>restart&lt;/on_reboot>
  &lt;on_crash>destroy&lt;/on_crash>
  &lt;pm>
    &lt;suspend-to-mem enabled="no"/>
    &lt;suspend-to-disk enabled="no"/>
  &lt;/pm>
  &lt;devices>
    &lt;emulator>/usr/bin/qemu-system-x86_64&lt;/emulator>
    &lt;disk type="file" device="disk">
      &lt;driver name="qemu" type="qcow2"/>
      &lt;source file="/var/lib/libvirt/images/vm1.qcow2"/>
      &lt;target dev="vda" bus="virtio"/>
      &lt;address type="pci" domain="0x0000" bus="0x05" slot="0x00" function="0x0"/>
    &lt;/disk>
    ...
    &lt;interface type="network">
      &lt;mac address="52:54:00:16:1e:01"/>
      &lt;source network="default"/>
      &lt;model type="virtio"/>
      &lt;rom enabled="no"/>
      &lt;address type="pci" domain="0x0000" bus="0x01" slot="0x00" function="0x0"/>
    &lt;/interface>
    &lt;serial type="pty">
      &lt;target type="isa-serial" port="0">
        &lt;model name="isa-serial"/>
      &lt;/target>
    &lt;/serial>
    &lt;console type="pty">
      &lt;target type="serial" port="0"/>
    &lt;/console>
    ...
    &lt;graphics type="spice" autoport="yes">
      &lt;listen type="address"/>
    &lt;/graphics>
    &lt;video>
      &lt;model type="qxl" ram="65536" vram="65536" vgamem="16384" heads="1" primary="yes"/>
      &lt;address type="pci" domain="0x0000" bus="0x00" slot="0x01" function="0x0"/>
    &lt;/video>
    &lt;memballoon model="virtio">
      &lt;address type="pci" domain="0x0000" bus="0x06" slot="0x00" function="0x0"/>
    &lt;/memballoon>
    &lt;rng model="virtio">
      &lt;backend model="random">/dev/urandom&lt;/backend>
      &lt;address type="pci" domain="0x0000" bus="0x08" slot="0x00" function="0x0"/>
    &lt;/rng>
    &lt;iommu model="intel">
      &lt;driver intremap="on" eim="on"/>
    &lt;/iommu>
    &lt;vsock model="virtio">
      &lt;cid auto="yes"/>
      &lt;address type="pci" domain="0x0000" bus="0x07" slot="0x00" function="0x0"/>
    &lt;/vsock>
  &lt;/devices>
&lt;/domain>
    </screen>

  </sect1>

  <?pdfpagebreak style="sbp" formatter="fop"?>

  <xi:include href="sbp-legal-notice.xml"/>

  <?pdfpagebreak style="sbp" formatter="fop"?>
  <xi:include href="license-gfdl.xml"/>
</article>
