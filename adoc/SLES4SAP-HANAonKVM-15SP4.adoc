:docinfo:

:localdate:

// defining article ID
[#art-sap-hana-kvm-sles15sp4]

// Document Variables
:DocumentName: SUSE Best Practices for SAP HANA on KVM
:slesProdVersion: 15 SP4
:suse: SUSE
:SUSEReg: SUSE(R)
:sleAbbr: SLE
:sle: SUSE Linux Enterprise
:sleReg: {SUSEReg} Linux Enterprise
:slesAbbr: SLES
:sles: {sle} Server
:slesReg: {sleReg} Server
:sles4sapAbbr: {slesAbbr} for SAP
:sles4sap: {sles} for SAP applications
:sles4sapReg: {slesReg} for SAP applications
:haswell: Intel Xeon Processor E7 v3 (Haswell)
:skylake: 1st Generation Intel Xeon Scalable Processor (Skylake)
:cascadelake: 2nd Generation Intel Xeon Scalable Processor (Cascade Lake)
:launchPadNotes: https://launchpad.support.sap.com/#/notes/


//TODO: Add a support checklist, e.g. for support folks (a shortened version of the guide to help support know what to check)
//TODO: add picture to describe CPU core mappings phys/virt
//TODO: add picture to explain VM Scenarios

= {DocumentName}

{sles4sap} {slesProdVersion}

[[_sec_introduction]]
== Introduction

This best practice document describes how {sles4sap} {slesProdVersion} with KVM should be configured to run SAP HANA for its use in productive environments.
The setup of the SAP HANA system or other components like HA clusters are beyond the scope of this document.

The following sections describe how to set up and configure the three KVM components required to run SAP HANA on KVM:

* *<<_sec_hypervisor>>*: Host operating system running the hypervisor directly on the server hardware
* *<<_sec_guest_vm_xml_configuration>>*: `libvirt` domain XML description of the guest VM
* *<<_sec_guest_operating_system>>*: Operating system inside the VM where SAP HANA is running

Follow *<<_sec_supported_scenarios_prerequisites>>* and the respective SAP Notes to ensure a supported configuration.
Most of the configuration options are specific to the `libvirt` package and therefore require modifying the VM's domain XML file.

[[_sec_definitions]]
=== Definitions

Virtual Machine (VM):: The emulation of a computer.
Hypervisor:: The software running directly on the physical server to create and run VMs.
Guest OS:: The operating system running inside the VM. 
This is the OS running SAP HANA and therefore the one that should be checked for SAP HANA support as per {launchPadNotes}2235581[SAP Note 2235581 "SAP HANA: Supported Operating Systems"] and the https://www.sap.com/dmc/exp/2014-09-02-hana-hardware/enEN/["`SAP HANA Hardware Directory`"].
Paravirtualization:: Allows direct communication between the hypervisor and the VM guest resulting in a lower overhead and better performance.
libvirt:: A management interface for KVM.
qemu:: The virtual machine emulator, also seen as process on the hypervisor running the VM.
SI units:: Some commands and configurations use the decimal prefix (for example GB), while other use the binary prefix (for example GiB). In this document we use the binary prefix where possible.

For a general overview of the technical components of the KVM architecture, refer to section https://documentation.suse.com/sles/15-SP4/html/SLES-all/cha-kvm-intro.html["Introduction to KVM Virtualization"] of the Virtualization Guide.

[[_sec_sap_hana_virtualization_scenarios]]
=== SAP HANA virtualization scenarios

SAP supports virtualization technologies for SAP HANA usage on a per-scenario basis:

Single-VM:: One VM per hypervisor/physical server for SAP HANA Scale-Up. No other VM or workload is allowed on the same server.
Multi-VM:: Multiple VMs per hypervisor/physical server for SAP HANA Scale-Up deployments.
Scale-Out:: For an SAP HANA Scale-Out deployment, distributed over multiple VMs on multiple hosts.



[[_sec_supported_scenarios_prerequisites]]
== Supported scenarios and prerequisites

Follow the *{DocumentName} - {sles4sap} {slesProdVersion}*
        document at hand. It describes the steps necessary
        to create a supported SAP HANA on KVM configuration. 
        The use of {sles4sap} is recommended in this scenario. However, it is also valid to use {sles} as both hypervisor and VM guest operating system.  

Inquiries about scenarios not listed here should be directed to mailto:saphana@suse.com[saphana@suse.com].

Keep in mind that the XML examples in this guide will deal with a single-VM scenario.

[[_sec_supported_scenarios]]
=== Supported scenarios

At the time of writing, the following configurations are supported for production use:

[[_supported_combinations]]
.Supported Combinations
[cols="1,1,1,1", options="header"]
|===
| CPU Architecture
| SAP HANA scale-up (single VM)
| SAP HANA scale-up (multi VM)
| SAP HANA Scale-out

|
{skylake}
|
_Hypervisor:_ {sles4sapAbbr} 15 SP2 

_Guest:_ {sles4sapAbbr} 15 SP2 onwards 

_Size:_ max. 4 sockets, 3 TiB RAM
|
no
|
no

|
{cascadelake}
|
_Hypervisor:_ {sles4sapAbbr} 15 SP4 

_Guest:_ {sles4sapAbbr} 15 SP4 onwards 

_Size:_ max. 4 sockets footnote:max4sockets[Maximum 4 sockets using Intel standard chipsets on a single system board, for example Lenovo* SR950, Fujitsu* rx4770 etc.], 6 TiB RAM
|
_Hypervisor:_ {sles4sapAbbr} 15 SP4 

_Guest:_ {sles4sapAbbr} 15 SP4 onwards 

_Size:_ One VM per socket (max. 4 VMs per host)
|
no
|===

Check the following SAP Notes for the latest details of supported SAP HANA on KVM scenarios:

* {launchPadNotes}2284516[SAP Note 2284516 - "SAP HANA virtualized on SUSE Linux Enterprise Hypervisors"]
* {launchPadNotes}3366235[SAP Note 3366235 - "SAP HANA on SUSE KVM Virtualization with SLES 15 SP4"]

[[_sec_sizing]]
=== Sizing

When sizing for a virtualized SAP HANA system, some additional factors need to be taken into account.

[[_sec_resources_hypervisor]]
==== Resources for the hypervisor

It is recommended to reserve a minimum of about 8% of the hosts's main memory for the hypervisor.

The hypervisor will consume CPU capacity, approximately 5% to 10% of the SAPS capacity, depending on the workload characteristics:

* 5% of the SAPS capacity for mainly analytical workloads
* 10% of the SAPS capacity for mainly transactional workloads

It is however *not* required to dedicate CPUs to the hypervisor.

[[_sec_memory_sizing]]
==== Memory sizing

Since SAP HANA runs inside the VM, it is the RAM size of the VM which needs to satisfy the memory requirements of the SAP HANA Memory sizing.

The memory used by the VM must be smaller than the physical memory of the machine.
It is recommended to reserve at least 8% of the total memory reported by "`/proc/meminfo`" (in the "`MemTotal`" field) for the hypervisor.
This leaves ~ 92% to the VM.

See <<_sec_memory_backing>> for more details.

[[_sec_cpu_sizing]]
==== CPU sizing

Workload tests have shown that certain scenarios can generate CPU overhead of up to 20%. Thus, thorough testing of the configuration for the required workload is highly recommended before a "`go-live`".

There are two main ways to deal with CPU sizing from a sizing perspective:

1. Follow the fixed memory-to-core ratios for SAP HANA as defined by SAP.
2. Follow the SAP HANA TDI "`Phase 5`" rules as defined by SAP.

Both ways are described in the following sections.

===== Following the fixed memory-to-core ratios for SAP HANA

The certification of the SAP HANA Appliance hardware to be used for KVM prescribes a fixed maximum amount of memory (RAM) which is allowed for each CPU core, also known as *memory-to-core ratio*. The specific ratio also depends on what workload the system will be used for, that is the Appliance Type: OLTP (Scale-up: SoH/S4H) or OLAP (Scale-up: BWoH/BW4H/DM/SoH/S4H).

The relevant memory-to-core ratio required to size a VM can be easily calculated as follows:

* Go to the https://www.sap.com/dmc/exp/2014-09-02-hana-hardware/enEN/#/solutions["SAP HANA Certified Hardware Directory"].
* Select the required SAP HANA Appliance and Appliance Type (for example CPU Architecture "Intel Cascade Lake SP" for Appliance Type "Scale-up: BWoH").
* Look for the largest certified RAM size for the number of CPU Sockets on the server (for example 6 TiB/6144 GiB on 4-Socket).
* Look up the number of cores per CPU of this CPU Architecture used in SAP HANA Appliances. The CPU model numbers are shown in the details view of a selected SAP HANA Appliance after clicking the 'Read more' button. To get the amount of cores of a specific CPU model, you can query the product databases of the respective manufacturers (for example https://ark.intel.com["Intel"]) 
* Using the above values calculates the total number of cores on the certified appliance by multiplying the number of sockets by the number of cores (for example 4x28=112).
* Now divide the appliance RAM by the total number of cores (not hyperthreads) to get the *memory-to-core* ratio (for example 3072 GiB/112 = approx. 28 GiB per core).

<<_sap_hana_core_to_memory_ratio_examples>> below has some current examples of SAP HANA memory-to-core ratios.

// TODO: Mit Gereon reden. Passt das so auch f√ºr CSL? (identische Core Anzahl)
[[_sap_hana_core_to_memory_ratio_examples]]
.SAP HANA memory-to-core ratio examples
[cols="1,1,1,1,1,1", options="header"]
|===
| CPU Architecture
| Appliance Type
| Max Memory Size
| Sockets
| Cores per Socket
| SAP HANA memory-to-core ratio

| {cascadelake} | OLTP | 6 TiB / 6144 GiB | 4 | 28 | 54.86 GiB/core
| {cascadelake} | OLAP | 3 TiB / 3072 GiB | 4 | 28 | 27.43 GiB/core
|===

From your memory requirement, calculate the RAM size the VM needs to be compliant with the appropriate memory-to-core ratio defined by SAP.

* To get the memory per socket, multiply the memory-to-core ratio by the number of cores (not threads) of a single socket in your host.
* Divide the memory requirement by the memory per socket, and round the result up to the next full number. Multiply again that number by the memory per socket.


.Calculation example
====
* From an S/4HANA sizing you get a memory requirement for SAP HANA of 2000 GiB. 
* Your CPUs have 28 cores per socket. The memory per socket is `28 cores * 54.86 GiB/core = 1536 GiB`.   
* Divide your memory requirement `2000 GiB / 1536 GiB = 1.2987` and round this result up to 2. Then multiply `2 * 1536 GiB = 3072 GiB`.
* 3072 GiB is now the memory size to use in the VM configuration as described in <<_sec_memory_backing>>.
* On a machine with four sockets and a total of 6 TiB of memory, this VM would span over two sockets. This would leave two sockets for another VM running SAP HANA with a similar workload or different VMs running other workloads.
====


===== Following the SAP HANA TDI "Phase 5" rules
** SAP HANA TDI "Phase 5" rules allow customers to deviate from the above described SAP HANA memory-to-core sizing ratios in certain scenarios. 
However, the KVM implementation must still adhere to the *SUSE Best Practices for SAP HANA on KVM* document at hand. 
Find details on SAP HANA TDI Phase 5 in the blog article https://blogs.sap.com/2017/09/20/tdi-phase-5-new-opportunities-for-cost-optimization-of-sap-hana-hardware/["TDI Phase 5: New Opportunities for Cost Optimization of SAP HANA Hardware"] from SAP.
** Since SAP HANA TDI Phase 5 rules use SAPS-based sizing, SUSE recommends applying the same overhead as measured with SAP HANA on KVM for the respective KVM version/CPU architecture. SAPS values for servers can be requested from the respective hardware vendor.


The following SAP HANA sizing documentation should also be useful: 

// Not Found:  * SAP Best Practice "`Sizing Approaches for SAP HANA`": https://websmp203.sap-ag.de/~sapidb/011000358700000050632013E
* https://help.sap.com/viewer/eb3777d5495d46c5b2fa773206bbfb46/2.0.03/en-US/d4a122a7bb57101493e3f5ca08e6b039.html["SAP HANA Master Guide: Sizing SAP HANA"]
* http://sap.com/sizing["General SAP Sizing information"]


[[_sec_kvm_hypervisor_version]]
=== Configuring the KVM hypervisor version

The hypervisor must be configured according to the *SUSE Best Practices for SAP
          HANA on KVM" ({sles4sap} {slesProdVersion}) guide at hand. In addition, it must fulfill the following minimal requirements:

* {sles4sap} {slesProdVersion} ("Unlimited Virtual Machines" subscription)
** kernel (Only major version 5.14, minimum package version 5.4.21.150400.24.46)
** libvirt (Only major version 8.0, minimum package version 8.0.0-150400.7.3.1)
** qemu (Only major version 6.2, minimum package version 6.2.0-150400.37.11.1)

// (Hypervisor) root@kvmclx2:~ # uname -a
// Linux kvmclx2 5.14.21-150400.24.46-default #1 SMP PREEMPT_DYNAMIC Thu Feb 9 08:38:18 UTC 2023 (2d95137) x86_64 x86_64 x86_64 GNU/Linux
//
// (Hypervisor) root@kvmclx2:~ # zypper info libvirt
// (...)
// Information for package libvirt:
// (...)
// Status         : out-of-date (version 8.0.0-150400.7.3.1 installed)
//
// (Hypervisor) root@kvmclx2:~ # zypper info qemu
// (...)
// Information for package qemu:
// (...)
// Status         : out-of-date (version 6.2.0-150400.37.11.1 installed)


[[_sec_hypervisor_hardware]]
=== Hypervisor hardware

Use SAP HANA certified servers and storage as per SAP HANA Hardware Directory at https://www.sap.com/dmc/exp/2014-09-02-hana-hardware/enEN/.

[[_sec_guest_vm]]
=== Guest VM

The guest VM must:

* run {sles} or {sles4sap} 15 SP4 or later.
* be a {sles} supported VM guest as per Section 7.4 "Supported guest operating systems" of the https://documentation.suse.com/sles/15-SP4/single-html/SLES-virtualization/#book-virtualization[SUSE Virtualization Guide].
* comply with KVM limits as per https://documentation.suse.com/sles/15-SP4/single-html/SLES-virtualization/#virt-hypervisors-limits["SUSE Linux Enterprise Server 15 SP4 Hypervisor Limits"].
* fulfill the SAP HANA Hardware and Cloud Measurent Tools (HCMT) storage KPI's as per {launchpadnotes}2493172[SAP Note 2493172 "SAP HANA Hardware and Cloud Measurement Tools"]. 
  Refer to <<_sec_storage>> for storage configuration details.
* be configured according to the *SUSE Best Practices for SAP HANA on KVM* ({sles4sap} {slesProdVersion}) document at hand.


[[_sec_hypervisor]]
== Setting up and configuring the hypervisor

The following sections describe how to set up and configure the hypervisor for a virtualized SAP HANA scenario.

[[_sec_kvm_hypervisor_installation]]
=== Installing the KVM hypervisor 

For details, refer to section 6 https://documentation.suse.com/sles/15-SP4/single-html/SLES-virtualization/#cha-vt-installation[Installation of Virtualization Components] of the SUSE Virtualization Guide.

This guide assumes that there is a bare installation of {sles} or {sles4sap} available on the system. For installation instructions on {sles} or {sles4sap}, refer to the https://documentation.suse.com/sles/15-SP4/html/SLES-all/article-installation.html["SUSE Linux Enterprise Server 15 SP4 Installation Quick Start Guide"]

Install the KVM packages using the following Zypper patterns:

----
zypper in -t pattern kvm_server kvm_tools
----

In addition, it is also useful to install the `lstopo` tool which is part of the `hwloc` package available from the *HPC Module* for SUSE Linux Enterprise Server.

[[_sec_configure_networking_on_hypervisor]]
=== Configuring networking on the hypervisor

To achieve maximum performance required for productive SAP HANA workloads, one of the host networking devices must be assigned directly to the KVM guest VM.
A Network Interface Card (NIC) including support for the technology that goes under the name of Single Root I/O Virtualization (SR-IOV) is required.
This guarantees that the overhead is avoided in which we would have incurred if using IO Virtualization.

To check whether such technology is available, assuming that `17:00.0` is the address of the NIC on the PCI bus (as visible in the output of the `lspci` tool), the following command can be issued:

----
lspci -vs 17:00.0
17:00.0 Ethernet controller: Intel Corporation Ethernet Controller X710 for 10GbE SFP+ (rev 01)
        Subsystem: Intel Corporation Ethernet Converged Network Adapter X710-2
        Flags: bus master, fast devsel, latency 0, IRQ 247, NUMA node 0
        Memory at 9c000000 (64-bit, prefetchable) [size=8M]
        Memory at 9d008000 (64-bit, prefetchable) [size=32K]
        Expansion ROM at 9d680000 [disabled] [size=512K]
        Capabilities: [40] Power Management version 3
        Capabilities: [50] MSI: Enable- Count=1/1 Maskable+ 64bit+
        Capabilities: [70] MSI-X: Enable+ Count=129 Masked-
        Capabilities: [a0] Express Endpoint, MSI 00
        Capabilities: [e0] Vital Product Data
        Capabilities: [100] Advanced Error Reporting
        Capabilities: [140] Device Serial Number d8-ef-c3-ff-ff-fe-fd-3c
        Capabilities: [150] Alternative Routing-ID Interpretation (ARI)
        Capabilities: [160] Single Root I/O Virtualization (SR-IOV)
        Capabilities: [1a0] Transaction Processing Hints
        Capabilities: [1b0] Access Control Services
        Capabilities: [1d0] #19
        Kernel driver in use: i40e
        Kernel modules: i40e
----

The output should contain a line similar to the following: `Single Root I/O Virtualization (SR-IOV)`.
If such line is not present, it might be the case that SR-IOV needs to be explicitly enabled in the BIOS.

[[_sec_assign_network_port_at_pci_nic_level]]
==== Preparing a Virtual Function (VF) for a guest VM

After checking that the NIC is SR-IOV capable, the host and the guest VM should be configured to use one of the available Virtual Functions (VFs) as (one of) the guest VM's network device(s).
Find more information about SR-IOV as a technology and how to properly configure everything necessary for it to work well in the general case in the https://documentation.suse.com/sles/15-SP4/html/SLES-all/book-virtualization.html[SUSE Virtualization Guide for SUSE Linux Enterprise Server 15 SP4].
Specifically, have a look here at the section https://documentation.suse.com/sles/15-SP4/html/SLES-all/cha-libvirt-config-virsh.html#sec-libvirt-config-io[Adding SR-IOV Devices].


*Enabling PCI passthrough for the host kernel*

Make sure that the host kernel boot command line contains these two parameters: `intel_iommu=on iommu=pt`.
This can be done by editing [path]_/etc/default/grub_: 

* Append `intel_iommu=on iommu=pt` to the string that is assigned to the variable `GRUB_CMDLINE_LINUX_DEFAULT`. 
* Then run `update-bootloader` (more detailed information is provided later in the document).

*Loading and configuring SR-IOV host drivers*

Before starting the VM, SR-IOV must be enabled on the desired NIC, and the VFs must be created.

Always make sure that the proper SR-IOV-capable driver is loaded. For example, for an *Intel Corporation Ethernet Controller X710* NIC, the driver resides in the `i40e` kernel module.
It can be loaded with the `modprobe` command, but chances are high that it is already loaded by default.

If the SR-IOV-capable module is not in use by default and it also fails to load with `modprobe`, this could mean that another driver, potentially one that is not SR-IOV-capable, 
is currently loaded instead. In which case, it should be removed with the `rmmod` command.

When the proper module is loaded, creating at least one VF can be done by issueing the following command (which creates four of them):

----
echo 4 > /sys/bus/pci/devices/0000\:17\:00.0/sriov_numvfs
----

Or, assuming that the designated NIC corresponds to the symbolic name of `eth10`, use the following command:

----
echo 4 > /sys/class/net/eth10/device/sriov_numvfs
----

This procedure is temporary until the next reboot. To make it reboot persistent it can be automated and needs to run at boot time. To do so create the following `systemd` unit file [path]_/etc/systemd/system/after.local_:

----
[Unit]
Description=/etc/init.d/after.local Compatibility
After=libvirtd.service
Requires=libvirtd.service
[Service]
Type=oneshot
ExecStart=/etc/init.d/after.local
RemainAfterExit=true

[Install]
WantedBy=multi-user.target
----

After that, create the script [path]_/etc/init.d/after.local_:

----
#! /bin/sh
#
# Copyright (c) 2010 SuSE LINUX Products GmbH, Germany.  All rights reserved.
# ...
echo 4 > /sys/class/net/eth10/device/sriov_numvfs
----

[[_sec_storage_hypervisor]]
=== Configuring storage on the hypervisor

As with compute resources, the storage used for running SAP HANA must also be SAP certified.
Therefore, only the storage from SAP HANA Appliances or SAP HANA Certified Enterprise Storage (https://www.sap.com/dmc/exp/2014-09-02-hana-hardware/enEN/#/solutions?filters=v:deCertified;storage) is supported.
In all cases, the SAP HANA storage configuration recommendations from the respective hardware vendor and the https://me.sap.com/notes/1900823[SAP HANA Storage Requirements for TDI] should be followed.

There are three supported storage options to use for the SAP HANA database inside a VM and on top of a {sles} 15 SP4 hypervisor: Fibre Channel (FC) storage, Network Attached Storage (NAS) and local storage.

==== Network attached Storage
The SAP HANA storage is attached via the NFSv4 protocol. 
In this case, nothing needs to be configured on the hypervisor. 
However, make sure that the VM has access to one or more dedicated 10 Gbit Ethernet interfaces for the network traffic to the network-attached storage.

==== Fibre Channel storage

As described in <<_sec_configure_networking_on_hypervisor>>, to reach the adequate level of performance, the storage drives for actual SAP HANA data are attached to the guest VM via directly assigning the SAN HBA controller to it.
One difference, though, is that there is no counterpart of SR-IOV commonly available for storage controllers.
Therefore, a full SAN HBA controller must be dedicated and directly assigned to the guest VM.

To figure out which SAN HBA should be used, check the available ones, for example with the `lspci` command:

----
lspci | grep -i "Fibre Channel"
85:00.0 Fibre Channel: QLogic Corp. ISP2722-based 16/32Gb Fibre Channel to PCIe Adapter (rev 01)
85:00.1 Fibre Channel: QLogic Corp. ISP2722-based 16/32Gb Fibre Channel to PCIe Adapter (rev 01)
ad:00.0 Fibre Channel: QLogic Corp. ISP2722-based 16/32Gb Fibre Channel to PCIe Adapter (rev 01)
ad:00.1 Fibre Channel: QLogic Corp. ISP2722-based 16/32Gb Fibre Channel to PCIe Adapter (rev 01)
----

The HBAs that are assigned to the guest VM must not be in use on the host.

The remaining storage configuration details are available in <<_sec_storage>>. This includes also information about how to add the disks and the HBA controllers to the guest VM configuration file, 
and what to do with them from inside the guest VM itself. 

==== Local Storage

Local storage configurations should be in line with current guidelines by the respective hardware vendors and their recommendations regarding SAP HANA.

[[_sec_hypervisor_operating_system_configuration]]
=== Configuring the hypervisor operating system 

The hypervisor host operating system needs to be configured to assure compatibility and maximized performance for an SAP HANA VM.


[[_sec_vhostmd]]
==== Installing `vhostmd`
The hypervisor needs to have the `vhostmd` package installed and the corresponding `vhostmd` service enabled and started. 
This is described in {launchPadNotes}1522993[SAP Note 1522993 - "Linux: SAP on SUSE KVM - Kernel-based Virtual Machine"].


[[_sec_tuned]]
==== Tuning the generic host with `tuned`

To apply some less specific, but nevertheless effective, tuning to the host, the *TuneD* tool (https://tuned-project.org/) can be used.

When installed (the package name is `tuned`), one of the preconfigured profiles can be selected, or a custom one created.
Specifically, the `virtual-host` profile should be chosen.
Do not use the `sap-hana profile` on the hypervisor.
This can be achieved with the following commands:

----
zypper in tuned

systemctl enable tuned

systemctl start tuned

tuned-adm profile virtual-host
----

The `tuned` daemon should now start automatically at boot time. Also, it should always load the `virtual-host` profile, so there is no need to add any of the above commands in any custom start-up script.
If in doubt, check with the following command whether `tuned` is running and what the current profile is:

----
tuned-adm profile

Available profiles:
- balanced                    - General non-specialized tuned profile
...
- virtual-guest               - Optimize for running inside a virtual guest
- virtual-host                - Optimize for running KVM guests
Current active profile: virtual-host
----

[[_sec_verify_tuned_has_set_cpu_frequency_governor_and_performance_bias]]
===== Power management considerations

Set the CPU frequency governor to *performance* to avoid latency issues because of ramping the CPU frequency up and down in response to changes in the system's load.
The selected `tuned` profile should have done this already. You can verify it with the following command:
----
cpupower -c all  frequency-info
----

Verify the governor setting by looking at the *current policy*.

Additionally, set the performance bias setting to 0 (performance). You can verify the performance bias setting with the following command:

----
cpupower -c all info
----

Modern processors also attempt to save power when they are idle, by switching to a lower power state.
Unfortunately, this incurs latency when switching in and out of these states.

To avoid that, and to achieve better and more consistent performance, the CPUs should not be allowed to switch those power saving modes (known as *C-states*) and should stay in normal operation mode all the time.
Therefore, it is recommended to only use the state *C0*.

This can be enforced by adding the following parameters to the kernel boot command line: `intel_idle.max_cstate=0`.

To double-check that only the desired C-states are available, use the following command:

----
cpupower idle-info
----

Verify the idle state settings by looking at the line containing `Available idle states:`.


[[_sec_irqbalance]]
==== `irqbalance`

The `irqbalance` service should be disabled because it can cause latency issues when the _/proc/irq/*_ files are read.
To disable `irqbalance`, run the following command:

----
systemctl stop irqbalance.service

systemctl disable irqbalance.service
----

[[_sec_no_ksm]]
==== Kernel Samepage Merging (ksm)

Kernel Samepage Merging (KSM, https://www.kernel.org/doc/html/latest/admin-guide/mm/ksm.html ) should be disabled.
The following command makes sure that it is turned off and that any sharing and de-duplication activity that may have happened is reverted in case it was enabled:

----
echo 2 >  /sys/kernel/mm/ksm/run
----

[[_sec_customize_the_linux_kernel_boot_options]]
==== Customizing the Linux kernel boot options

To edit the boot options for the Linux kernel, perform the following steps:

. Edit [path]_/etc/default/grub_ and add the following boot options to the line *GRUB_CMDLINE_LINUX_DEFAULT* (a detailed explanation of these options will follow).
+

----
intremap=no_x2apic_optout intel_idle.max_cstate=0 processor.max_cstate=0 intel_iommu=on iommu=pt transparent_hugepage=never default_hugepagesz=1GB hugepagesz=1GB hugepages=<number of hugepages> numa_balancing=disable kvm_intel.ple_gap=0 kvm_intel.ple_window=0 mitigations=auto kvm.nx_huge_pages=off security=apparmor mmio_stale_data=off tsx=off
----
+

. Run the following command:
+

----
update-bootloader
----
. Reboot the system:
+

----
reboot
----


[[_sec_technical_explanation_of_the_above_described_configuration_settings]]
==== Technical explanation of the configuration settings

*Hardware vulnerabilities mitigations*
----
mitigations=auto kvm.nx_huge_pages=off tsx=off
----
Recently, a class of side channel attacks exploiting the branch prediction and the speculative execution capabilities of modern CPUs appeared.
On an affected CPU, these problems cannot be fixed, but their effect and their actual exploitability can be mitigated in software.
However, this sometimes has a non-negligible impact on the performance.

To achieve the best possible security, the software mitigations for these vulnerabilities are being enabled (`mitigations=auto`) with the exceptions of those that deal with "Machine Check Error Avoidance on Page Size Change" (CVE-2018-12207, also known as "iTLB Multiht") and "TSX asynchronous abort" (CVE-2019-11135).

*Automatic NUMA balancing*
----
numa_balancing=disable
----
Automatic NUMA balancing can result in increased system latency and should therefore be disabled.

*KVM PLE-GAP*
----
kvm_intel.ple_gap=0 kvm_intel.ple_window=0
----
*Pause Loop Exit* (PLE) is a feature whereby a spinning guest CPU releases the physical CPU until a lock is free.
This is useful in cases where multiple virtual CPUs are using the same physical CPU. But it causes unnecessary delays when the system is not overcommitted.

*Transparent huge pages*
----
transparent_hugepage=never
----
Because 1 GiB pages are used for the virtual machine, there is no additional benefit from having THP enabled.
Disabling it will avoid `khugepaged` interfering with the virtual machine while it scans for pages to promote to hugepages.

*Processor C-states*
----
intel_idle.max_cstate=0
----
Optimal performance is achieved by limiting the processor to states *C0* (normal running state). Any other state includes the possibility for the operating system to put certain CPU cores in a lower-powered idle state and the 'wake-up' time can impact performance inside the virtual machine.

*Huge pages*
----
default_hugepagesz=1G hugepagesz=1G hugepages=<number of hugepages>
----
The use of 1 GiB huge pages is meant to reduce overhead and contention when the guest is updating its page tables.
This requires allocation of 1 GiB huge pages on the host.
The number of pages to allocate depends on the memory size of the guest.

1 GiB pages are not pageable by the OS. Thus, they always remain in RAM and therefore, the `locked` definition in `libvirt` XML files is not required.

It is also important to ensure the order of the huge page options. Specifically the `<number of hugepages>` option must be placed *after* the 1 GiB huge page size definitions.

.Calculating value
[NOTE]
====
Calculate the value for `<number of hugepages>` by taking the number GiB`'s of RAM minus approximately 8% for the hypervisor OS.
For example, 6 TiB RAM (6144 GiB) minus 8% are approximately 5650 huge pages.
====

*PCI Passthrough*
----
intel_iommu=on iommu=pt
----
For being able to directly assign host devices (like storage controllers and NIC Virtual Functions) to the VM, with PCI Passthrough and SR-IOV, the IOMMU must be enabled. 
On top of that, `iommu=pt` makes sure that you set up the devices for the best performance (that is, passthrough mode).

*Interrupt remapping*
----
 intremap=no_x2apic_optout
----
Interrupt remapping allows the kernel to overwrite the interrupt remapping tables created by the BIOS or UEFI Firmware. This ensures that certain interrupts from peripheral devices are routed to a certain CPU. With the 'no_x2apic_optout' you make sure that this feature is always enabled.

*Processor MMIO Stale Data Vulnerabilities*
----
mmio_stale_data=off
----
Processor MMIO Stale Data Vulnerabilities are a set of vulnerabilities that can expose data to attackers in a very limited scope of environments. Processors of the type {cascadelake} are not affected by this vulnerability. Therefore, it is unnecessary to mitigate it in the environment at hand. For more information, refer to the https://www.intel.com/content/www/us/en/developer/topic-technology/software-security-guidance/processors-affected-consolidated-product-cpu-model.html[Intel Guidance for Security Issues on Intel Processors] and to the indicated https://docs.kernel.org/6.2/admin-guide/hw-vuln/processor_mmio_stale_data.html[Article in the official documentation of the Linux Kernel].


[[_sec_guest_vm_xml_configuration]]
== Configuring the guest VM

[NOTE]
====
This section describes the creation of a single VM on a single system. For the configuration of multiple VM's, most of the values regarding CPU count and/or memory amount need to be divided by the total socket count of the system and multiplied by the desired socket count of the single VM. Pay attention to the memory assignment. Try to assign memory that is located on the socket you are using for the VM.
====

This section describes the modifications required to the `libvirt` XML definition of the guest VM.
You can edit the `libvirt` XML using the following command:

----
virsh edit <Guest VM name>
----

[[_sec_create_an_initial_guest_vm_xml]]
=== Creating an initial guest VM XML

Refer to section 10 "Guest Installation" of the https://documentation.suse.com/sles/15-SP4/html/SLES-all/cha-kvm-inst.html[SUSE Virtualization Guide].

[[_sec_global_vcpu_configuration]]
=== Configuring global vCPU

The virtual CPU configuration of the VM guest should reflect the host CPU configuration as close as possible.
There cannot be any overcommitting of memory or CPU resources.

Set the CPU model to `host-passthrough`, and disable any `check`.
In addition, the `rdtscp`, `invtsc` and `x2apic` features are required.

[[_sec_memory_backing]]
=== Backing memory

Huge pages, sized 1 GiB (that is, 1048576 KiB), must be used for all the guest VM memory.
This guarantees optimal performance for the guest VM.

It is necessary that each NUMA cell of the guest VM have a whole number of huge pages assigned to them (that is, no fractions of huge pages).
All the NUMA cells should also have the same number of huge pages assigned to them (that is, the guest VM memory configuration must be balanced).

Therefore, the number of huge pages needs to be dividable by the number of NUMA cells.

For example, if the host has 6339943304 KiB (that is, 6 TiB) of memory and you want to leave 91.75% of it to the hypervisor (see <<_sec_memory_sizing>>), and there are 4 NUMA cells, each NUMA cell will have the following number of huge pages:

* (6339943304 * (91.75/100)) / 1048576 / 4 = 1386

This means that, in total, there needs to be the following number of huge pages:

* 1386 * 4 = 5544

Such number must be passed to the host kernel command line parameter on boot (that is `hugepages=5544`, see <<_sec_technical_explanation_of_the_above_described_configuration_settings>>).

Both the total amount of memory the guest VM should use and the fact that such memory must come from 1 GiB huge pages need to be specified in the guest VM configuration file.

You must also ensure that the `memory` and the `currentMemory` element have the same value. This is to disable memory ballooning, which, if enabled, would cause unacceptable latency:

----
<domain type='kvm'>
  <!-- ... -->
  <memory unit='KiB'>5813305344</memory>
  <currentMemory unit='KiB'>5813305344</currentMemory>
  <memoryBacking>
    <hugepages>
      <page size='1048576' unit='KiB'/>
    </hugepages>
    <nosharepages/>
  </memoryBacking>
  <!-- ... -->
</domain>
----

.Memory Unit
[NOTE]
====
The memory unit can be set to GiB to ease the memory computations.
====

[[_sec_vcpu_and_vnuma_topology]]
=== Mapping vCPU and vNUMA topology and pinning

It is important to map the host topology into the guest VM, as described below.
This allows HANA to spread its own workload threads across many virtual CPUs and NUMA nodes.

For example, for a 4-socket system, with 28 cores per socket and hyperthreading enabled, the virtual CPU configuration for a single VM will also have 4 sockets, 28 cores, 2 threads. In a multi-VM scenario it is also imperative that the hosts NUMA topology is maped to the VMs. Two VMs on a 4-socket systems would both reflect the NUMA topology of two of the hosts NUMA nodes/sockets. Four VMs on a 4-socket system would reflect the NUMA topology of a single NUMA node/socket.

Always make sure that, in the guest VM configuration file:

* the `cpu` `mode` attribute is set to `host-passthrough`.
* the `cpu` `topology` attribute describes the vCPU NUMA topology of the guest, as discussed above.
* the attributes of the `numa` elements describe which vCPU number ranges belong to which NUMA cell. Care should be taken since these number ranges are not the same as on the host. Additionally:
** the `cell` elements describe how much RAM should be distributed per NUMA node. In this 4-node example, enter 25% (or 1/4) of the entire guest VM memory.
Also refer to <<_sec_memory_backing>> and <<_sec_memory_sizing>> of the document at hand for further details.
** each NUMA cell of the guest VM has 56 vCPUs.
** the distances between the cells are identical to those of the physical hardware (as per the output of  the command `numactl --hardware`).

----
<domain type='kvm'>
  <!-- ... -->
  <cpu mode='host-passthrough' check='none' migratable='on'>
    <topology sockets='4' dies='1' cores='28' threads='2'/>
    <feature policy='require' name='rdtscp'/>
    <feature policy='require' name='invtsc'/>
    <feature policy='require' name='x2apic'/>
    <numa>
      <cell id='0' cpus='0-55' memory='1453326336' unit='KiB'>
        <distances>
          <sibling id='0' value='10'/>
          <sibling id='1' value='21'/>
          <sibling id='2' value='21'/>
          <sibling id='3' value='21'/>
        </distances>
      </cell>
      <cell id='1' cpus='56-111' memory='1453326336' unit='KiB'>
        <distances>
          <sibling id='0' value='21'/>
          <sibling id='1' value='10'/>
          <sibling id='2' value='21'/>
          <sibling id='3' value='21'/>
        </distances>
      </cell>
      <cell id='2' cpus='112-167' memory='1453326336' unit='KiB'>
        <distances>
          <sibling id='0' value='21'/>
          <sibling id='1' value='21'/>
          <sibling id='2' value='10'/>
          <sibling id='3' value='21'/>
        </distances>
      </cell>
      <cell id='3' cpus='168-223' memory='1453326336' unit='KiB'>
        <distances>
          <sibling id='0' value='21'/>
          <sibling id='1' value='21'/>
          <sibling id='2' value='21'/>
          <sibling id='3' value='10'/>
        </distances>
      </cell>
    </numa>
  </cpu>
  <!-- ... -->
</domain>
----

It is also necessary to pin virtual CPUs to physical CPUs. This limits the overhead caused by virtual CPUs being moved around physical CPUs by the host scheduler.
Similarly, the memory for each NUMA cell of the guest VM must be allocated only on the corresponding host NUMA node.

Note that KVM/QEMU uses a static hyperthread sibling CPU APIC ID assignment for virtual CPUs, irrespective of the actual physical CPU APIC ID values on the host.
For example, assuming that the first hyperthread sibling pair is CPU 0 and CPU 112 on the host, you will need to pin that sibling pair to vCPU 0 and vCPU 1.

It is recommended to pin both the various sibling pairs of vCPUs to (the corresponding) sibling pairs of host CPUs.
For example, vCPU 0 should be pinned to pCPU 0 and 112, and the same applies to vCPU 1.
As far as both the vCPUs always run on the same physical core, the host scheduler is allowed to execute them on either thread, for example in case only one is free while the other is busy executing host or hypervisor activities.

Using the above information, the CPU and memory pinning section of the guest VM XML can be created.
Below find a practical example based on the hypothetical example above.

Make sure to take note of the following configuration components:

* The `vcpu placement` element lists the total number of vCPUs in the guest.
* The `cputune` element contains the attributes describing the mappings of vCPUs to physical CPUs.
* The `numatune` element contains the attributes to describe the distribution of RAM across the virtual NUMA nodes (CPU sockets).
** The `mode` attribute should be set to `strict`.
** The appropriate number of nodes should be entered in the `nodeset` and `memnode` attributes. In this example, there are 4 sockets, therefore the values are `nodeset=0-3` and `cellid` 0 to 3.

----
<domain type='kvm'>
  <vcpu placement='static'>224</vcpu>
  <cputune>
    <vcpupin vcpu='0' cpuset='0,112'/>
    <vcpupin vcpu='1' cpuset='0,112'/>
    <vcpupin vcpu='2' cpuset='1,113'/>
    <vcpupin vcpu='3' cpuset='1,113'/>
    <vcpupin vcpu='4' cpuset='2,114'/>
    <vcpupin vcpu='5' cpuset='2,114'/>
    <vcpupin vcpu='6' cpuset='3,115'/>
    <vcpupin vcpu='7' cpuset='3,115'/>
    <vcpupin vcpu='8' cpuset='4,116'/>
    <vcpupin vcpu='9' cpuset='4,116'/>
    <vcpupin vcpu='10' cpuset='5,117'/>
    <vcpupin vcpu='11' cpuset='5,117'/>
    <!-- output abbreviated -->
    <vcpupin vcpu='218' cpuset='109,221'/>
    <vcpupin vcpu='219' cpuset='109,221'/>
    <vcpupin vcpu='220' cpuset='110,222'/>
    <vcpupin vcpu='221' cpuset='110,222'/>
    <vcpupin vcpu='222' cpuset='111,223'/>
    <vcpupin vcpu='223' cpuset='111,223'/>
  </cputune>
  <numatune>
    <memory mode='strict' nodeset='0-3'/>
    <memnode cellid='0' mode='strict' nodeset='0'/>
    <memnode cellid='1' mode='strict' nodeset='1'/>
    <memnode cellid='2' mode='strict' nodeset='2'/>
    <memnode cellid='3' mode='strict' nodeset='3'/>
  </numatune>
  <!-- ... -->
</domain>
----

The following script generates a section of the domain configuration according to the described specifications:

----
#!/usr/bin/env bash
NUM_VCPU=$(ls -d /sys/devices/system/cpu/cpu[0-9]* | wc -l)
echo "  <vcpu placement='static'>${NUM_VCPU}</vcpu>"
echo "  <cputune>"
THREAD_PAIRS="$(cat /sys/devices/system/cpu/cpu*/topology/core_cpus_list | sort -n | uniq )"
VCPU=0
for THREAD_PAIR in ${THREAD_PAIRS}; do
  for i in 1 2; do
    echo "    <vcpupin vcpu='${VCPU}' cpuset='${THREAD_PAIR}'/>"
    VCPU=$(( VCPU + 1 ))
  done
done
echo "  </cputune>"
----

Use the following commands to determine the CPU details on the hypervisor host:

----
lscpu --extended=CPU,SOCKET,CORE

lstopo-no-graphics
----

It is not necessary to isolate the guest VM's `iothreads`, nor to statically reserve any host CPU to either them or any other kind of host activity.

[[_sec_network]]
=== Configuring networking

One of the Virtual Functions prepared in <<_sec_configure_networking_on_hypervisor>> must be added to the guest VM as (one of) its network adapter(s).
This can be done by adding the following details to the guest VM configuration file:

----
 <domain type='kvm'>
  <!-- ... -->
  <devices>
    <!-- ... -->
    <interface type='hostdev' managed='yes'>
      <mac address='52:54:00:7f:12:fb'/>
      <driver name='vfio'/>
      <source>
        <address type='pci' domain='0x0000' bus='0x17' slot='0x02' function='0x0'/>
      </source>
    </interface>
    <!-- ... -->
  </devices>
  <!-- ... -->
</domain>
----

The various properties (for example `domain`, `bus`, etc.) of the `address` element should contain the proper values for pointing at the desired device (check with `lspci`).

[[_sec_storage]]
=== Configuring storage

The storage configuration is critical, as it plays an important role in terms of performance.

[[_sec_storage_configuration_for_operating_system_volumes]]
==== Configuring storage for operating system volumes

The performance of storage where the operating system is installed is not critical for the performance of SAP HANA. 
Therefore, you might use any KVM-supported storage to deploy the operating system itself. See an example below:


----
<domain type='kvm'>
  <!-- ... -->
  <devices>
    <!-- ... -->
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='native'/>
      <source dev='/dev/disk/by-id/wwn-0x600000e00d29000000293db000520000'/>
      <target dev='vda' bus='virtio'/>
    </disk>
    <!-- ... -->
  </devices>
  <!-- ... -->
</domain>
----

The `dev` attribute of the `source` element should contain the appropriate path.

[[_sec_storage_configuration_for_sap_hana_volumes]]
==== Configuring storage for SAP HANA volumes

The configuration depends on the type of storage used for the SAP HANA Database.

In any case, the storage for SAP HANA must be able to fulfill the storage requirements for SAP HANA from within the VM. 
The SAP HANA Hardware and Cloud Measurement Tools (HCMT) can be used to assess if the storage meets the requirements. 
For details on HCMT, refer to {launchpadnotes}2493172[SAP Note 2493172 - "SAP HANA Cloud and Hardware Measurement Tools"].

===== Network attached storage 

Follow the SAP HANA-specific best practices of the storage system vendor. 
Make sure that the VM has access to one or more dedicated 10 GiB (or better) Ethernet interfaces for the network traffic to the network attached storage.

===== Fibre Channel storage 

Since storage controller passthrough is used (see <<_sec_storage_hypervisor>>), any LVM (Logical Volume Manager) and Multipathing configuration should be made inside the guest VM, always following the storage layout recommendations from the appropriate hardware vendor.

The guest VM XML configuration must be based on the underlying storage configuration on the hypervisor (see <<_sec_storage_hypervisor>>).

Since the storage for HANA (`/data`, `/log` and `/shared` volumes) is performance-critical, it is recommended to take advantage of an SAN HBA that is passed through to the guest VM.


Note that it is not possible to only use one function of the adapter, and both must always be attached to the guest VM.
An example guest VM configuration with storage passthrough configured would look like the below (adjust the domain, bus, slot and function attributes of the `address` elements to match the adapter you chose):

----
<domain type='kvm'>
  <!-- ... -->
  <devices>
    <!-- ... -->
    <hostdev mode='subsystem' type='pci' managed='yes'>
      <source>
        <address domain='0x0000' bus='0x85' slot='0x00' function='0x0'/>
      </source>
    </hostdev>
    <hostdev mode='subsystem' type='pci' managed='yes'>
      <source>
        <address domain='0x0000' bus='0x85' slot='0x00' function='0x1'/>
      </source>
    </hostdev>
    <!-- ... -->
  </devices>
  <!-- ... -->
</domain>
----

More details about how to directly assign PCI devices to a guest VM are described in https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-libvirt-config-virsh.html#sec-libvirt-config-pci-virsh[section 14.7 "Adding a PCI Device" of the Virtualization Guide].

===== Local storage

To achieve the best possible performance, it is recommended to directly attach the block device(s) and/or raid controllers, which will be used as storage for the SAP HANA data files. If there is a dedicated raid controller available in the system that only manages devices and raid volumes that will be used in one single VM, the recommendation is to connect it via PCI passthrough as described in the section above. If single devices need to be used (for example NVMe devices), you can connect those to the VM by doing something similar to the below:
// TODO: Trockencode! Check this before publishing!!!

----
virsh attach-disk <VMNAME> <DEVICE-PATH> <VDISK-PATH>
e.g. virsh attach-disk hana01 /dev/nvme0n1 vdc
----

[[_sec_vhostmd_guest]]
=== Setting up a vhostmd device

The `vhostmd` device is passed to the VM so that the `vm-dump-metrics` command can retrieve metrics about the hypervisor provided by `vhostmd`.
You can use either a vbd disk or a virtio-serial device (preferred) to set this up (see {launchPadNotes}1522993[SAP Note 1522993 - "Linux: SAP on SUSE KVM - Kernel-based Virtual Machine"] for details).


[[_sec_clocks_timers]]
=== Setting up clocks and timers

Make sure that the clock timers are set up in the guest VM configuration file as follows:

----
<domain type='kvm'>
  <!-- ... -->
  <clock offset='utc'>
    <timer name='rtc' tickpolicy='catchup'/>
    <timer name='pit' tickpolicy='delay'/>
    <timer name='hpet' present='no'/>
  </clock>
  <!-- ... -->
</domain>
----

[[_sec_features]]
=== Configuring special features

It is necessary to enable a set of optimizations for the guest VM that are specific for the cases when the vCPUs are pinned and have (semi-)dedicated pCPUs all for themselves.
You can do so by providing the following details in the guest VM configuration file:

----
<domain type='kvm'>
  <!-- ... -->
  <features>
    <!-- ... -->
    <kvm>
      <hint-dedicated state='on'/>
    </kvm>
  </features>
  <!-- ... -->
</domain>
----

Note that this is a requirement for making it possible to load and use the "`cpuidle-haltpoll`" kernel module inside the guest VM OS (see <<_sec_cpuidle_haltpoll>>).


[[_sec_guest_operating_system]]
== Installing the guest operating system

[[_sec_install_sles_for_sap_inside_the_guest_vm]]
=== Installing SUSE Linux Enterprise Server for SAP applications inside the Guest VM

Refer to the https://documentation.suse.com/sles-sap/15-SP4/[SUSE Guide "`SUSE Linux Enterprise Server for SAP applications 15 SP4].
          

[[_sec_guest_operating_system_configuration_for_sap_hana]]
=== Configuring the guest operating system for SAP HANA

Install and configure {sles4sap} {slesProdVersion} and SAP HANA as described in: 

* {launchPadNotes}1944799[SAP Note 1944799 - "SAP HANA Guidelines for SLES Operating System Installation"]
* {launchPadNotes}2684254[SAP Note 2684254 - "SAP HANA DB: Recommended OS settings for SLES 15 / SLES for SAP applications 15"]

[[_sec_customizing_linux_cmdline_guest]]
==== Customizing the Linux kernel parameters of the guest

Like the hypervisor host, the VM also needs special kernel parameters to be set. 
To edit the boot options for the Linux kernel, do the following:

. Edit [path]_/etc/default/grub_ and add the following boot options to the line "`GRUB_CMDLINE_LINUX_DEFAULT`".
+

----
mitigations=auto kvm.nx_huge_pages=off intremap=no_x2apic_optout intel_idle.max_cstate=0 processor.max_cstate=0 numa_balancing=disable tsx=off mmio_stale_data=off
----
+

A detailed explanation of these parameters has been given in <<_sec_technical_explanation_of_the_above_described_configuration_settings>>.

[[_sec_enabling_host_monitoring_guest]]
==== Enabling host monitoring

The VM needs to have the `vm-dump-metrics` package installed, which dumps the metrics provided by the `vhostmd` service running on the hypervisor (see <<_sec_vhostmd_guest>> for information how to set this up on hypervisor level). This enables SAP HANA to collect data about the hypervisor. 
{launchPadNotes}1522993[SAP Note 1522993 - "Linux: SAP on SUSE KVM - Kernel-based Virtual Machine"] describes how to set up the virtual devices for `vhostmd` and how to configure it.
When using a virtual disk for `vhostmd`, the virtual disk device must be world-readable, which is ensured with the boot time configuration below.


[[_sec_configuring_guest_at_boot_time]]
==== Configuring the Guest at boot time

The folling settings need to be configured at boot time of the VM. 
To persist these configurations, it is recommended to put the commands provided below into a script which is executed as part of the boot process.

===== Disabling `irqbalance` 

The irqbalance service should be disabled because it can cause latency issues when the `/proc/irq/*` files are read.
To disable irqbalance run the following command:

----
systemctl disable irqbalance.service
systemctl stop irqbalance.service
----

===== Activating and configuring `sapconf` or `saptune`

The following parameters need to be set in `sapconf` version 5. Edit the file `/etc/sysconfig/sapconf` to reflect the settings below, and then restart the `sapconf` service.

----
GOVERNOR=performance
PERF_BIAS=performance
MIN_PERF_PCT=100
FORCE_LATENCY=5 
----

NOTE: When using `sapconf` version 5, stop and disable the `tuned` service and instead enable and start the `sapconf` service.

If you use `saptune`, configure it accordingly:

* Apply the `HANA` solution: `saptune solution apply HANA`
* Create the file `/etc/saptune/override/2684254` with the following content.
----
[cpu]
force_latency=5
----
* Re-apply the recommendations for SAP Note 2684254: `saptune note apply 2684254`

Detailed documentation on `saptune` is available in chapter https://documentation.suse.com/sles-sap/15-SP4/html/SLES-SAP-guide/cha-tune.html[Tuning systems with `saptune`] of the {sles4sap} Guide.



[[_sec_cpuidle_haltpoll]]
===== Activating and configuring `haltpoll`



----
POLL_NS=2400000
GROW_START=24000000
modprobe cpuidle-haltpoll
echo $POLL_NS > /sys/module/haltpoll/parameters/guest_halt_poll_ns
echo $GROW_START > /sys/module/haltpoll/parameters/guest_halt_poll_grow_start
----

===== Setting the clock source

You need to set the clock source to `kvm-clock`:

----
echo kvm-clock > /sys/devices/system/clocksource/clocksource0/current_clocksource
----

===== Disabling Kernel Same Page Merging

Kernel Same Page Merging (KSM) needs to be disabled, like on the hypervisor (see <<_sec_no_ksm>>):

----
echo 2 >/sys/kernel/mm/ksm/run
----

===== Implementing automatic configuration at boot time
The following script provides an example for a script implementing above recommendations, to be executed at boot time of the VM.

.Script
----
#!/usr/bin/env bash
#
# Configure KVM guest for SAP HANA
#
 
POLL_NS=2400000
GROW_START=24000000
 
# disable irqbalance 
systemctl disable --now irqbalance
 
modprobe cpuidle-haltpoll
echo $POLL_NS > /sys/module/haltpoll/parameters/guest_halt_poll_ns
echo $GROW_START > /sys/module/haltpoll/parameters/guest_halt_poll_grow_start
 
# Set clocksource to tsc
echo kvm-clock > /sys/devices/system/clocksource/clocksource0/current_clocksource
 
# disable Kernel Samepage Merging
echo 2 >/sys/kernel/mm/ksm/run
# 2: disable it, but make sure you also purify everything with fire!
 
# fix access to vhostmd device, so that SIDadm can read it
# see function setup_vhostmd_guest_device() in qacss-schwifty-common
 
# the vhostmd device has exactly 256 blocks, try to catch that from /proc/partitions
VHOSTMD_DEVICE=$(grep "   256 " /proc/partitions | awk '{print $4}' )
if [ -n "$VHOSTMD_DEVICE" ]; then
  chmod o+r /dev/"$VHOSTMD_DEVICE"
else
  echo "Missing vhostmd device, please check you XML file."
fi
----

Both `sapconf` and `saptune` apply their settings automatically at boot time and do not need to be included in the script above.

[[_sec_guest_operating_system_storage_configuration_for_sap_hana_volumes]]
=== Configuring the guest operating system storage for SAP HANA volumes

* Follow the storage layout recommendations from the appropriate hardware vendors.
* Only use LVM (Logical Volume Manager) inside the VM for SAP HANA. Nested LVM should not be used.


[[_sec_performance_considerations]]
== Performance considerations

The Linux kernel has code to mitigate existing vulnerabilities of the {skylake} and {cascadelake} CPUs. Our testing showed no visible impact of those mitigations with regard to SAP HANA performance, except for the https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/multihit.html[iTLB Multihit] and the https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/tsx_async_abort.html[TSX Asynchronous Abort] mitigation. These mitigations can be controlled by the kernel parameter `kvm.nx_huge_pages` (see https://www.suse.com/support/kb/doc/?id=000019411[SUSE support document 7023735]) and `tsx`.

In general, the setting of parameter `kvm.nx_huge_pages` has an impact on performance. 
The implications on performance need to be considered as laid out in the Skylake example below.

Performance deviations for virtualization as measured on Intel Skylake (Bare Metal to single VM):

* Setting `kvm.nx_huge_pages=off`
** The measured performance deviation for OLTP or mixed OLTP/OLAP workload is below 
10%.
** The measured performance deviation for OLAP workload is below 5%.
* Setting `kvm.nx_huge_pages=auto`
** The measured performance deviation for OLTP or mixed OLTP/OLAP was impacted by 
this setting. 
For S/4HANA standard workloads, OLTP transactional request times show an overhead of up to 30 ms. 
This overhead leads to an additional transactional throughput loss. However, it did not exceed 10%, running at a very high system load, when compared to the underlying bare metal environment.
** The measured performance deviation for OLAP workload is below 5%.
** During performance analysis with standard workload, most of the test cases stayed within the defined KPI of 10% performance degradation compared to bare metal. 
However, there are low-level performance tests in the test suite exercising various HANA kernel components that exhibit a performance degradation of more than 10%. 
This also indicates that there are particular scenarios which might not be suited for SAP HANA on SUSE KVM with kvm.nx_huge_pages = AUTO; especially those workloads generating high resource utilization, which must be considered when sizing SAP HANA instance in a SUSE KVM virtual machine. 
Thorough tests of configurations for all workload conditions are highly recommended.



[[_sec_administration]]
== Administration

For a full explanation of administration commands, refer to official SUSE Virtualization documentation such as:

* https://documentation.suse.com/sles/15-SP4/html/SLES-all/cha-libvirt-managing.html[Section 11 "Basic VM Guest Management"] and others in the https://documentation.suse.com/sles/15-SP4/html/SLES-all/book-virtualization.html[SUSE Virtualization Guide for SUSE Linux Enterprise Server 15]
* https://documentation.suse.com/sles/15-SP4/html/SLES-all/article-virtualization-best-practices.html[SUSE Virtualization Best Practices for SUSE Linux Enterprise Server 15]


[[_sec_useful_commands_on_the_hypervisor]]
=== Useful commands on the hypervisor

Check kernel boot options used:

----
cat /proc/cmdline
----

Check the huge page status (this command can also be used to monitor the progress of huge page allocation during VM start):

----
cat /proc/meminfo | grep Huge
----

List all VM guest domains configured on the hypervisor:

----
virsh list --all
----

Start a VM (and keep in mind that VM start times can take some minutes on larger RAM systems, check the progress with `/proc/meminfo | grep Huge`):

----
virsh start <VM/Guest Domain name>
----

Shut down a VM:

----
virsh shutdown <VM/Guest Domain name>
----

This is the location of VM guest configuration files:

----
/etc/libvirt/qemu
----

This is the location of VM log files:

----
/var/log/libvirt/qemu
----

[[_sec_useful_commands_inside_the_vm_guest]]
=== Useful commands inside the VM guest

Checking L3 cache has been enabled in the guest:

----
lscpu | grep L3
----

Validate guest and host CPU topology:

----
lscpu
----

[[_sec_examples]]
== Examples


[[_sec_example_guest_vm_xml]]
=== Example guest VM XML 

// TODO: XML config sample needs to be replaced with a CSL 6TB version!!!

.XML configuration example
[IMPORTANT]
====
The XML file below is only an *example* showing the key configurations to assist in understanding how to configure a valid VM in this environment via an XML file.
The actual XML configuration must be based on your respective hardware configuration and VM requirements.
====

Points of interest in this example (refer to the detailed sections of the *SUSE Best Practices for SAP HANA on KVM* ({sles4sap} {slesProdVersion}) document at hand for a full explanation):

* Memory
** The hypervisor has 6 TiB RAM (or 6144 GiB), of which 5544 GiB have been allocated as 1 GB huge pages and therefore 5544 GiB is the max VM size in this case
** 5544 GiB = 5813305344 KiB
** In the `numa` section memory is split evenly over the 4 NUMA nodes (CPU sockets)
* CPU pinning
** Note the alternating CPU pinning on the hypervisor, see <<_sec_vcpu_and_vnuma_topology>> for details
** Note the topology of the guest VM mirrors the one of the hypervisor (4x28 CPU cores)
* Network I/O
** Virtual functions of the physical network interface card have been added as PCI devices
* Storage I/O
** A single SAN HBA is passed through to the VM as `hostdev` device (one for each function/port)
** See <<_sec_storage>> for details
* `qemu:commandline` elements to describe CPU attributes, for details see <<_sec_global_vcpu_configuration>>


The following VM definition is an example for a VM configured to consume a 4-socket server with 3 TiB of main memory. It is taken from our actual validation machine.
Note that this file is abridged for clarity; the cut is denoted by a `[...]` mark. 

----
# cat /etc/libvirt/qemu/SUSEKVM.xml
!--
WARNING: THIS IS AN AUTO-GENERATED FILE. CHANGES TO IT ARE LIKELY TO BE
OVERWRITTEN AND LOST. Changes to this xml configuration should be made using:
  virsh edit SUSEKVM
or other application using the `libvirt` API.
--

<domain type='kvm'>
  <name>kvmvm11</name>
  <uuid>f529e0b0-93cc-4e83-87dc-65cb9922336d</uuid>
  <description>kvmvm11</description>
  <metadata>
    <libosinfo:libosinfo xmlns:libosinfo="http://libosinfo.org/xmlns/libvirt/domain/1.0">
      <libosinfo:os id="http://suse.com/sle/15.4"/>
    </libosinfo:libosinfo>
  </metadata>
  <memory unit='KiB'>5813305344</memory>
  <currentMemory unit='KiB'>5813305344</currentMemory>
  <memoryBacking>
    <hugepages>
      <page size='1048576' unit='KiB'/>
    </hugepages>
    <nosharepages/>
  </memoryBacking>
  <vcpu placement='static'>224</vcpu>
  <cputune>
    <vcpupin vcpu='0' cpuset='0,112'/>
    <vcpupin vcpu='1' cpuset='0,112'/>
    <vcpupin vcpu='2' cpuset='1,113'/>
    <vcpupin vcpu='3' cpuset='1,113'/>
    <vcpupin vcpu='4' cpuset='2,114'/>
    <vcpupin vcpu='5' cpuset='2,114'/>
    <vcpupin vcpu='6' cpuset='3,115'/>
    <vcpupin vcpu='7' cpuset='3,115'/>
    <vcpupin vcpu='8' cpuset='4,116'/>
    <vcpupin vcpu='9' cpuset='4,116'/>
    <vcpupin vcpu='10' cpuset='5,117'/>
    <vcpupin vcpu='11' cpuset='5,117'/>
    <vcpupin vcpu='12' cpuset='6,118'/>
    <vcpupin vcpu='13' cpuset='6,118'/>
    <vcpupin vcpu='14' cpuset='7,119'/>
    <vcpupin vcpu='15' cpuset='7,119'/>
    <vcpupin vcpu='16' cpuset='8,120'/>
    <vcpupin vcpu='17' cpuset='8,120'/>
    <vcpupin vcpu='18' cpuset='9,121'/>
    <vcpupin vcpu='19' cpuset='9,121'/>
    <vcpupin vcpu='20' cpuset='10,122'/>
    <vcpupin vcpu='21' cpuset='10,122'/>
    <vcpupin vcpu='22' cpuset='11,123'/>
    <vcpupin vcpu='23' cpuset='11,123'/>
    <vcpupin vcpu='24' cpuset='12,124'/>
    <vcpupin vcpu='25' cpuset='12,124'/>
    <vcpupin vcpu='26' cpuset='13,125'/>
    <vcpupin vcpu='27' cpuset='13,125'/>
    <vcpupin vcpu='28' cpuset='14,126'/>
    <vcpupin vcpu='29' cpuset='14,126'/>
    <vcpupin vcpu='30' cpuset='15,127'/>
    <vcpupin vcpu='31' cpuset='15,127'/>
    <vcpupin vcpu='32' cpuset='16,128'/>
    <vcpupin vcpu='33' cpuset='16,128'/>
    <vcpupin vcpu='34' cpuset='17,129'/>
    <vcpupin vcpu='35' cpuset='17,129'/>
    <vcpupin vcpu='36' cpuset='18,130'/>
    <vcpupin vcpu='37' cpuset='18,130'/>
    <vcpupin vcpu='38' cpuset='19,131'/>
    <vcpupin vcpu='39' cpuset='19,131'/>
    <vcpupin vcpu='40' cpuset='20,132'/>
    <vcpupin vcpu='41' cpuset='20,132'/>
    <vcpupin vcpu='42' cpuset='21,133'/>
    <vcpupin vcpu='43' cpuset='21,133'/>
    <vcpupin vcpu='44' cpuset='22,134'/>
    <vcpupin vcpu='45' cpuset='22,134'/>
    <vcpupin vcpu='46' cpuset='23,135'/>
    <vcpupin vcpu='47' cpuset='23,135'/>
    <vcpupin vcpu='48' cpuset='24,136'/>
    <vcpupin vcpu='49' cpuset='24,136'/>
    <vcpupin vcpu='50' cpuset='25,137'/>
    <vcpupin vcpu='51' cpuset='25,137'/>
    <vcpupin vcpu='52' cpuset='26,138'/>
    <vcpupin vcpu='53' cpuset='26,138'/>
    <vcpupin vcpu='54' cpuset='27,139'/>
    <vcpupin vcpu='55' cpuset='27,139'/>
    <vcpupin vcpu='56' cpuset='28,140'/>
    <vcpupin vcpu='57' cpuset='28,140'/>
    <vcpupin vcpu='58' cpuset='29,141'/>
    <vcpupin vcpu='59' cpuset='29,141'/>
    <vcpupin vcpu='60' cpuset='30,142'/>
    <vcpupin vcpu='61' cpuset='30,142'/>
    <vcpupin vcpu='62' cpuset='31,143'/>
    <vcpupin vcpu='63' cpuset='31,143'/>
    <vcpupin vcpu='64' cpuset='32,144'/>
    <vcpupin vcpu='65' cpuset='32,144'/>
    <vcpupin vcpu='66' cpuset='33,145'/>
    <vcpupin vcpu='67' cpuset='33,145'/>
    <vcpupin vcpu='68' cpuset='34,146'/>
    <vcpupin vcpu='69' cpuset='34,146'/>
    <vcpupin vcpu='70' cpuset='35,147'/>
    <vcpupin vcpu='71' cpuset='35,147'/>
    <vcpupin vcpu='72' cpuset='36,148'/>
    <vcpupin vcpu='73' cpuset='36,148'/>
    <vcpupin vcpu='74' cpuset='37,149'/>
    <vcpupin vcpu='75' cpuset='37,149'/>
    <vcpupin vcpu='76' cpuset='38,150'/>
    <vcpupin vcpu='77' cpuset='38,150'/>
    <vcpupin vcpu='78' cpuset='39,151'/>
    <vcpupin vcpu='79' cpuset='39,151'/>
    <vcpupin vcpu='80' cpuset='40,152'/>
    <vcpupin vcpu='81' cpuset='40,152'/>
    <vcpupin vcpu='82' cpuset='41,153'/>
    <vcpupin vcpu='83' cpuset='41,153'/>
    <vcpupin vcpu='84' cpuset='42,154'/>
    <vcpupin vcpu='85' cpuset='42,154'/>
    <vcpupin vcpu='86' cpuset='43,155'/>
    <vcpupin vcpu='87' cpuset='43,155'/>
    <vcpupin vcpu='88' cpuset='44,156'/>
    <vcpupin vcpu='89' cpuset='44,156'/>
    <vcpupin vcpu='90' cpuset='45,157'/>
    <vcpupin vcpu='91' cpuset='45,157'/>
    <vcpupin vcpu='92' cpuset='46,158'/>
    <vcpupin vcpu='93' cpuset='46,158'/>
    <vcpupin vcpu='94' cpuset='47,159'/>
    <vcpupin vcpu='95' cpuset='47,159'/>
    <vcpupin vcpu='96' cpuset='48,160'/>
    <vcpupin vcpu='97' cpuset='48,160'/>
    <vcpupin vcpu='98' cpuset='49,161'/>
    <vcpupin vcpu='99' cpuset='49,161'/>
    <vcpupin vcpu='100' cpuset='50,162'/>
    <vcpupin vcpu='101' cpuset='50,162'/>
    <vcpupin vcpu='102' cpuset='51,163'/>
    <vcpupin vcpu='103' cpuset='51,163'/>
    <vcpupin vcpu='104' cpuset='52,164'/>
    <vcpupin vcpu='105' cpuset='52,164'/>
    <vcpupin vcpu='106' cpuset='53,165'/>
    <vcpupin vcpu='107' cpuset='53,165'/>
    <vcpupin vcpu='108' cpuset='54,166'/>
    <vcpupin vcpu='109' cpuset='54,166'/>
    <vcpupin vcpu='110' cpuset='55,167'/>
    <vcpupin vcpu='111' cpuset='55,167'/>
    <vcpupin vcpu='112' cpuset='56,168'/>
    <vcpupin vcpu='113' cpuset='56,168'/>
    <vcpupin vcpu='114' cpuset='57,169'/>
    <vcpupin vcpu='115' cpuset='57,169'/>
    <vcpupin vcpu='116' cpuset='58,170'/>
    <vcpupin vcpu='117' cpuset='58,170'/>
    <vcpupin vcpu='118' cpuset='59,171'/>
    <vcpupin vcpu='119' cpuset='59,171'/>
    <vcpupin vcpu='120' cpuset='60,172'/>
    <vcpupin vcpu='121' cpuset='60,172'/>
    <vcpupin vcpu='122' cpuset='61,173'/>
    <vcpupin vcpu='123' cpuset='61,173'/>
    <vcpupin vcpu='124' cpuset='62,174'/>
    <vcpupin vcpu='125' cpuset='62,174'/>
    <vcpupin vcpu='126' cpuset='63,175'/>
    <vcpupin vcpu='127' cpuset='63,175'/>
    <vcpupin vcpu='128' cpuset='64,176'/>
    <vcpupin vcpu='129' cpuset='64,176'/>
    <vcpupin vcpu='130' cpuset='65,177'/>
    <vcpupin vcpu='131' cpuset='65,177'/>
    <vcpupin vcpu='132' cpuset='66,178'/>
    <vcpupin vcpu='133' cpuset='66,178'/>
    <vcpupin vcpu='134' cpuset='67,179'/>
    <vcpupin vcpu='135' cpuset='67,179'/>
    <vcpupin vcpu='136' cpuset='68,180'/>
    <vcpupin vcpu='137' cpuset='68,180'/>
    <vcpupin vcpu='138' cpuset='69,181'/>
    <vcpupin vcpu='139' cpuset='69,181'/>
    <vcpupin vcpu='140' cpuset='70,182'/>
    <vcpupin vcpu='141' cpuset='70,182'/>
    <vcpupin vcpu='142' cpuset='71,183'/>
    <vcpupin vcpu='143' cpuset='71,183'/>
    <vcpupin vcpu='144' cpuset='72,184'/>
    <vcpupin vcpu='145' cpuset='72,184'/>
    <vcpupin vcpu='146' cpuset='73,185'/>
    <vcpupin vcpu='147' cpuset='73,185'/>
    <vcpupin vcpu='148' cpuset='74,186'/>
    <vcpupin vcpu='149' cpuset='74,186'/>
    <vcpupin vcpu='150' cpuset='75,187'/>
    <vcpupin vcpu='151' cpuset='75,187'/>
    <vcpupin vcpu='152' cpuset='76,188'/>
    <vcpupin vcpu='153' cpuset='76,188'/>
    <vcpupin vcpu='154' cpuset='77,189'/>
    <vcpupin vcpu='155' cpuset='77,189'/>
    <vcpupin vcpu='156' cpuset='78,190'/>
    <vcpupin vcpu='157' cpuset='78,190'/>
    <vcpupin vcpu='158' cpuset='79,191'/>
    <vcpupin vcpu='159' cpuset='79,191'/>
    <vcpupin vcpu='160' cpuset='80,192'/>
    <vcpupin vcpu='161' cpuset='80,192'/>
    <vcpupin vcpu='162' cpuset='81,193'/>
    <vcpupin vcpu='163' cpuset='81,193'/>
    <vcpupin vcpu='164' cpuset='82,194'/>
    <vcpupin vcpu='165' cpuset='82,194'/>
    <vcpupin vcpu='166' cpuset='83,195'/>
    <vcpupin vcpu='167' cpuset='83,195'/>
    <vcpupin vcpu='168' cpuset='84,196'/>
    <vcpupin vcpu='169' cpuset='84,196'/>
    <vcpupin vcpu='170' cpuset='85,197'/>
    <vcpupin vcpu='171' cpuset='85,197'/>
    <vcpupin vcpu='172' cpuset='86,198'/>
    <vcpupin vcpu='173' cpuset='86,198'/>
    <vcpupin vcpu='174' cpuset='87,199'/>
    <vcpupin vcpu='175' cpuset='87,199'/>
    <vcpupin vcpu='176' cpuset='88,200'/>
    <vcpupin vcpu='177' cpuset='88,200'/>
    <vcpupin vcpu='178' cpuset='89,201'/>
    <vcpupin vcpu='179' cpuset='89,201'/>
    <vcpupin vcpu='180' cpuset='90,202'/>
    <vcpupin vcpu='181' cpuset='90,202'/>
    <vcpupin vcpu='182' cpuset='91,203'/>
    <vcpupin vcpu='183' cpuset='91,203'/>
    <vcpupin vcpu='184' cpuset='92,204'/>
    <vcpupin vcpu='185' cpuset='92,204'/>
    <vcpupin vcpu='186' cpuset='93,205'/>
    <vcpupin vcpu='187' cpuset='93,205'/>
    <vcpupin vcpu='188' cpuset='94,206'/>
    <vcpupin vcpu='189' cpuset='94,206'/>
    <vcpupin vcpu='190' cpuset='95,207'/>
    <vcpupin vcpu='191' cpuset='95,207'/>
    <vcpupin vcpu='192' cpuset='96,208'/>
    <vcpupin vcpu='193' cpuset='96,208'/>
    <vcpupin vcpu='194' cpuset='97,209'/>
    <vcpupin vcpu='195' cpuset='97,209'/>
    <vcpupin vcpu='196' cpuset='98,210'/>
    <vcpupin vcpu='197' cpuset='98,210'/>
    <vcpupin vcpu='198' cpuset='99,211'/>
    <vcpupin vcpu='199' cpuset='99,211'/>
    <vcpupin vcpu='200' cpuset='100,212'/>
    <vcpupin vcpu='201' cpuset='100,212'/>
    <vcpupin vcpu='202' cpuset='101,213'/>
    <vcpupin vcpu='203' cpuset='101,213'/>
    <vcpupin vcpu='204' cpuset='102,214'/>
    <vcpupin vcpu='205' cpuset='102,214'/>
    <vcpupin vcpu='206' cpuset='103,215'/>
    <vcpupin vcpu='207' cpuset='103,215'/>
    <vcpupin vcpu='208' cpuset='104,216'/>
    <vcpupin vcpu='209' cpuset='104,216'/>
    <vcpupin vcpu='210' cpuset='105,217'/>
    <vcpupin vcpu='211' cpuset='105,217'/>
    <vcpupin vcpu='212' cpuset='106,218'/>
    <vcpupin vcpu='213' cpuset='106,218'/>
    <vcpupin vcpu='214' cpuset='107,219'/>
    <vcpupin vcpu='215' cpuset='107,219'/>
    <vcpupin vcpu='216' cpuset='108,220'/>
    <vcpupin vcpu='217' cpuset='108,220'/>
    <vcpupin vcpu='218' cpuset='109,221'/>
    <vcpupin vcpu='219' cpuset='109,221'/>
    <vcpupin vcpu='220' cpuset='110,222'/>
    <vcpupin vcpu='221' cpuset='110,222'/>
    <vcpupin vcpu='222' cpuset='111,223'/>
    <vcpupin vcpu='223' cpuset='111,223'/>
  </cputune>
  <numatune>
    <memnode cellid='0' mode='strict' nodeset='0'/>
    <memnode cellid='1' mode='strict' nodeset='1'/>
    <memnode cellid='2' mode='strict' nodeset='2'/>
    <memnode cellid='3' mode='strict' nodeset='3'/>
  </numatune>
  <resource>
    <partition>/machine</partition>
  </resource>
  <os>
    <type arch='x86_64' machine='pc-q35-6.2'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
    <kvm>
      <hint-dedicated state='on'/>
    </kvm>
    <vmport state='off'/>
  </features>
  <cpu mode='host-passthrough' check='none' migratable='on'>
    <topology sockets='4' dies='1' cores='28' threads='2'/>
    <feature policy='require' name='rdtscp'/>
    <feature policy='require' name='invtsc'/>
    <feature policy='require' name='x2apic'/>
    <numa>
      <cell id='0' cpus='0-55' memory='1453326336' unit='KiB'>
        <distances>
          <sibling id='0' value='10'/>
          <sibling id='1' value='21'/>
          <sibling id='2' value='21'/>
          <sibling id='3' value='21'/>
        </distances>
      </cell>
      <cell id='1' cpus='56-111' memory='1453326336' unit='KiB'>
        <distances>
          <sibling id='0' value='21'/>
          <sibling id='1' value='10'/>
          <sibling id='2' value='21'/>
          <sibling id='3' value='21'/>
        </distances>
      </cell>
      <cell id='2' cpus='112-167' memory='1453326336' unit='KiB'>
        <distances>
          <sibling id='0' value='21'/>
          <sibling id='1' value='21'/>
          <sibling id='2' value='10'/>
          <sibling id='3' value='21'/>
        </distances>
      </cell>
      <cell id='3' cpus='168-223' memory='1453326336' unit='KiB'>
        <distances>
          <sibling id='0' value='21'/>
          <sibling id='1' value='21'/>
          <sibling id='2' value='21'/>
          <sibling id='3' value='10'/>
        </distances>
      </cell>
    </numa>
  </cpu>
  <clock offset='utc'>
    <timer name='rtc' tickpolicy='catchup'/>
    <timer name='pit' tickpolicy='delay'/>
    <timer name='hpet' present='no'/>
  </clock>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>destroy</on_crash>
  <pm>
    <suspend-to-mem enabled='no'/>
    <suspend-to-disk enabled='no'/>
  </pm>
  <devices>
    <emulator>/usr/bin/qemu-system-x86_64</emulator>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='native'/>
      <source dev='/dev/disk/by-id/wwn-0x600000e00d29000000293db000520000'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x04' slot='0x00' function='0x0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/dev/shm/vhostmd0'/>
      <target dev='vdx' bus='virtio'/>
      <readonly/>
      <address type='pci' domain='0x0000' bus='0x0b' slot='0x00' function='0x0'/>
    </disk>
    <controller type='usb' index='0' model='qemu-xhci' ports='15'>
      <address type='pci' domain='0x0000' bus='0x02' slot='0x00' function='0x0'/>
    </controller>
    <controller type='sata' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x1f' function='0x2'/>
    </controller>
    <controller type='pci' index='0' model='pcie-root'/>
    <controller type='pci' index='1' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='1' port='0x10'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0' multifunction='on'/>
    </controller>
    <controller type='pci' index='2' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='2' port='0x11'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x1'/>
    </controller>
    <controller type='pci' index='3' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='3' port='0x12'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x2'/>
    </controller>
    <controller type='pci' index='4' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='4' port='0x13'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x3'/>
    </controller>
    <controller type='pci' index='5' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='5' port='0x14'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x4'/>
    </controller>
    <controller type='pci' index='6' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='6' port='0x15'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x5'/>
    </controller>
    <controller type='pci' index='7' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='7' port='0x16'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x6'/>
    </controller>
    <controller type='pci' index='8' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='8' port='0x17'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x7'/>
    </controller>
    <controller type='pci' index='9' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='9' port='0x18'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0' multifunction='on'/>
    </controller>
    <controller type='pci' index='10' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='10' port='0x19'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x1'/>
    </controller>
    <controller type='pci' index='11' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='11' port='0x1a'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x2'/>
    </controller>
    <controller type='pci' index='12' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='12' port='0x1b'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x3'/>
    </controller>
    <controller type='pci' index='13' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='13' port='0x1c'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x4'/>
    </controller>
    <controller type='pci' index='14' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='14' port='0x1d'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x5'/>
    </controller>
    <controller type='virtio-serial' index='0'>
      <address type='pci' domain='0x0000' bus='0x03' slot='0x00' function='0x0'/>
    </controller>
    <interface type='direct'>
      <mac address='0c:fd:37:92:dc:99'/>
      <source dev='eth11' mode='vepa'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x01' slot='0x00' function='0x0'/>
    </interface>
    <interface type='hostdev' managed='yes'>
      <mac address='52:54:00:7f:12:fb'/>
      <driver name='vfio'/>
      <source>
        <address type='pci' domain='0x0000' bus='0x17' slot='0x02' function='0x0'/>
      </source>
      <address type='pci' domain='0x0000' bus='0x0c' slot='0x00' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target type='isa-serial' port='0'>
        <model name='isa-serial'/>
      </target>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <channel type='unix'>
      <target type='virtio' name='org.qemu.guest_agent.0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
    <channel type='spicevmc'>
      <target type='virtio' name='com.redhat.spice.0'/>
      <address type='virtio-serial' controller='0' bus='0' port='2'/>
    </channel>
    <input type='tablet' bus='usb'>
      <address type='usb' bus='0' port='1'/>
    </input>
    <input type='mouse' bus='ps2'/>
    <input type='keyboard' bus='ps2'/>
    <graphics type='spice' autoport='yes'>
      <listen type='address'/>
      <image compression='off'/>
    </graphics>
    <sound model='ich9'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x1b' function='0x0'/>
    </sound>
    <video>
      <model type='qxl' ram='65536' vram='65536' vgamem='16384' heads='1' primary='yes'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x0'/>
    </video>
    <!-- SAN 2-port HBA passthrough configuration -->
    <hostdev mode='subsystem' type='pci' managed='yes'>
      <source>
        <address domain='0x0000' bus='0x85' slot='0x00' function='0x0'/>
      </source>
      <address type='pci' domain='0x0000' bus='0x0d' slot='0x00' function='0x0'/>
    </hostdev>
    <hostdev mode='subsystem' type='pci' managed='yes'>
      <source>
        <address domain='0x0000' bus='0x85' slot='0x00' function='0x1'/>
      </source>
      <address type='pci' domain='0x0000' bus='0x0e' slot='0x00' function='0x0'/>
    </hostdev>
    <redirdev bus='usb' type='spicevmc'>
      <address type='usb' bus='0' port='2'/>
    </redirdev>
    <redirdev bus='usb' type='spicevmc'>
      <address type='usb' bus='0' port='3'/>
    </redirdev>
    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x05' slot='0x00' function='0x0'/>
    </memballoon>
    <rng model='virtio'>
      <backend model='random'>/dev/urandom</backend>
      <address type='pci' domain='0x0000' bus='0x06' slot='0x00' function='0x0'/>
    </rng>
  </devices>
</domain>
----

[[_sec_additional_information]]
== Additional information

[[_sec_resources]]
=== Resources

* https://documentation.suse.com/sbp-supported.html[SUSE Best Practices]
* https://documentation.suse.com/sles/15-SP2/html/SLES-all/book-virt.html[SUSE Virtualization Guide for SUSE Linux Enterprise Server 15]
* {launchpadnotes}3120786[SAP Note 3120786 - "SAP HANA on SUSE KVM Virtualization with SLES 15 SP2"]
* {launchPadNotes}2284516[SAP Note 2284516 - "SAP HANA virtualized on SUSE Linux Enterprise Hypervisors"]
* {launchPadNotes}1944799[SAP Note 1944799 - "SAP HANA Guidelines for SLES Operating System Installation"]
* {launchPadNotes}2684254[SAP Note 2684254 - "SAP HANA DB: Recommended OS settings for SLES 15 / SLES for SAP"] 
* {launchPadNotes}1522993[SAP Note 1522993 - "Linux: SAP on SUSE KVM - Kernel-based Virtual Machine"]


[[_sec_feedback]]
=== Feedback

Several feedback channels are available:

Bugs and Enhancement Requests::
For services and support options available for your product, refer to http://www.suse.com/support/.

To report bugs for a product component, go to https://scc.suse.com/support/ requests, log in, and select Submit New SR (Service Request).

Report Documentation Bug::
To report errors or suggest enhancements for a certain document, use the mailto:Report Documentation Bug[] icon at the right side of each section in the online documentation.
Provide a concise description of the problem and refer to the respective section number and page (or URL).

Mail::
For feedback on the documentation of this product, you can also send a mail to mailto:doc-team@suse.com[].
Make sure to include the document title, the product version and the publication date of the documentation.


////
[[_sec_version_history]]
=== Version History

[cols="1,1,2,3", options="header"]
|===
| Version | Publication Date | Author(s) | Comment

| 0.5 | Nov 2021 | Dario Faggioli, Gereon Vey | Initial draft

|===
////

++++
<?pdfpagebreak?>
++++

// :leveloffset: 0
// Standard SUSE Best Practices includes
== Legal notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
// include::common_gfdl1.2_i.adoc[]

:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

