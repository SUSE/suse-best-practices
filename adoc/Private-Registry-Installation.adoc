== Deployment of {spr}

=== Preparation

* Ensure you have `kubectl` and `helm` v3 installed and check that you have access to the target {kube} cluster where {spr} will be installed
* Decide between using a {kube} Ingress or just a Load Balancer to expose the {spr} services
** If a {kube} Ingress is the chosen option: ensure you have an Ingress Controller set up in the target {kube} cluster and two resolvable FQDN values prepared, one for the Harbor UI/API and the other for the Notary API (the latter is only needed if `Notary` will be enabled)
** If using just a LoadBalancer: Ensure that you have one of the following:
*** A predefined external IP address that can be associated with the Load Balancer service used to expose the {spr} services
*** An FQDN value that can be later on mapped in the external DNS to the external IP address dynamically allocated to the Load Balancer service during installation
* Choose between using auto-generated TLS certificates or providing your own custom TLS certificates for the Harbor UI/API and Notary API. In the latter case, have the required custom TLS certificates ready.
* Verify that the target {kube} cluster provides the required `StorageClass`(es). A `StorageClass` with `ReadWriteMany` access mode is required to fully enable high-availability and scalability for {spr} component
* Choose the persistent storage back-end that will be used to store OCI artifacts.
* Choose between using an external or internal database service. The internal database service doesn't support high-availability and scalability and is therefore *not recommended for production*.
The external database service needs to be prepared separately beforehand.
* Choose between using an external or internal Redis service. Similar to the database case, the internal Redis service doesn't support high-availability and scalability and *is not recommended for production*.
** If a public cloud managed Redis service is to be used as an external Redis, it needs to be prepared separately beforehand
** Instructions on installing the SUSE `redis-ha` operator service are included in the {spr} installation steps covered this section, if this is the preferred choice
* Determine resource requests and limits based on your Kubernetes cluster setup.

=== Installation Steps

. Download the Helm chart from the official SUSE repository:
+
[source,bash]
----
export HELM_EXPERIMENTAL_OCI=1
# download a chart from public registry
helm chart pull registry.suse.com/harbor/harbor:1.5
# export the chart to local directory
helm chart export registry.suse.com/harbor/harbor:1.5
----

. Make sure `KUBECONFIG` is set correctly
+
When installing on {caasp}, it is expected that the `KUBECONFIG` environment variable is set correctly pointing to the {kube} cluster.
+
When installing into hosted {kube} clusters like EKS or AKS, configuration must be fetched first so the following `kubectl` and helm commands work correctly.
+
For AKS, it is possible to use the `az` command line tool to get the `kubeconfig`:
+
[source,bash]
----
az aks get-credentials --resource-group <azure-resource-group> --name <aks-cluster-name> --file kubeconfig.yaml
export KUBECONFIG=<full path to kubeconfig.yaml>
----
+
For EKS, the `aws` command line tool can be used to generate the `kubeconfig`:
+
[source,bash]
----
aws eks --region <region-code> update-kubeconfig --name <eks-cluster_name> --kubeconfig kubeconfig.yaml
export KUBECONFIG=<full path to kubeconfig.yaml>
----

. Prepare a `harbor-values.yaml` file to specify custom {spr} configuration values
+
[WARNING]
====
The default configuration provided with the {spr} helm chart is not suited for production use!

A separate YAML file (referred to as the `harbor-values.yaml` file in the following sections) needs to be populated with customized configuration values to be used during installation.
The exact configuration options that can be customized, as well as the values that they can take are covered in detail in the next installation steps.
====
+
[TIP]
====
The full list of configuration options and default values that can overridden in the `harbor-values.yaml` file is included in the helm chart itself and can be viewed in YAML format by running the following command:

[source,bash]
----
helm show values harbor
----

It can also be used as a YAML template for configuration values that need to be customized, although it is recommended to keep in the `harbor-values.yaml` file only the configuration options that are changed from their default values, to allow default configuration changes to be introduced during upgrades.
====
+
[IMPORTANT]
====
The `harbor-values.yaml` file prepared and used during installation is the source of truth for the {spr} configuration.

It will also be required for some administrative operations, such as subsequent configuration changes and upgrades.
Make sure to preserve this file in a safe place, preferably under version control and to update it with every configuration change that is subsequently made to the deployed {spr} instance.
====

. (Optional) Disable unnecessary components
+
By default, {spr} has all supported components enabled. Some components may be disabled in the configuration, if they are not required:
+
--
[loweralpha]
. `trivy` - may be disabled, if security vulnerability scanning features are not required
. `notary` - may be disabled, if artifact signing features are not required
--
+
To disable unnecessary components, set the relevant configuration options to false in the `harbor-values.yaml` file:
+
.harbor-values.yaml
[source,yaml]
----
trivy:
  enabled: false
notary:
  enabled: false
----

. Configure a way to expose the {spr} UI and public APIs
+
The default and recommended way to expose the {spr} services to be consumed from outside the {kube} cluster is to use a {kube} Ingress.
This requires a {kube} Ingress controller to be already configured in your cluster and resolvable FQDNs to be prepared for the Harbor UI/API and the Notary API (if enabled) services.
Alternatively, services may be exposed using a {kube} LoadBalancer instead.
+
--
[loweralpha]
. Expose {spr} using a {kube} Ingress
+
This option assumes a {kube} Ingress Controller is already configured for your {kube} cluster, as described in the <<requirements-ingress>> section.
Update the `harbor-values.yaml` configuration file with the following configuration values:
+
.harbor-values.yaml
[source,yaml]
----
expose:
  # Set the way how to expose the service. Default value is "ingress".
  ingress:
    hosts:
      core: "<core_fqdn>"
      notary: "<notary_fqdn>"

# The external URL for Harbor core service. It is used to
# 1) populate the docker/helm commands showed on portal
# 2) populate the token service URL returned to docker/Notary client
#
# Format: protocol://domain[:port]. Usually:
# 1) if "expose.type" is "ingress", the "domain" should be
# the value of "expose.ingress.hosts.core"
#
# If Harbor is deployed behind the proxy, set it as the URL of proxy
externalURL: "https://<core_fqdn>"
----
+
Replace `<core_fqdn>` and `<notary_fqdn>` values with the resolvable FQDN values that were prepared as detailed in the <<requirements>> section.
If the Notary service was not enabled in the configuration, the `<notary_fqdn>` entry may be omitted.
The `harbor-values.yaml` configuration would look like this, if, for example, a public service like link:nip.io[nip.io] was used to provide FQDNs:
+
.harbor-values.yaml
[source,yaml]
----
expose:
  ingress:
    hosts:
      core: harbor.10.86.0.237.nip.io
      notary: notary.10.86.0.237.nip.io
externalURL: "https://harbor.10.86.0.237.nip.io"
----
+
Depending on which {kube} Ingress Controller is used, additional annotations may need to be added to the {spr} Ingress configuration:
+
.harbor-values.yaml
[source,yaml]
----
expose:
  ingress:
	...
    annotations:
      # To be used for the nginx ingress on AKS:
      kubernetes.io/ingress.class: nginx
      # To be used for the ALB ingress on EKS:
      kubernetes.io/ingress.class: alb
----

.  Expose {spr} using a {kube} LoadBalancer
+
Update the `harbor-values.yaml` configuration file with the following configuration values:
+
.harbor-values.yaml
[source,yaml]
----
expose:
  type: loadBalancer
  loadBalancer:
    # Set the IP if the LoadBalancer supports assigning IP
    IP: ""

# The external URL for Harbor core service. It is used to
# 1) populate the docker/helm commands showed on portal
# 2) populate the token service URL returned to docker/Notary client
#
# Format: protocol://domain[:port]. Usually:
# 1) if "expose.type" is "ingress", the "domain" should be
# the value of "expose.ingress.hosts.core"
#
# If Harbor is deployed behind the proxy, set it as the URL of proxy
externalURL: "https://<harbor_fqdn_or_ip_addr>"
----
+
The `<harbor_fqdn_or_ip_addr>` value needs to be set to an FQDN value that can be resolved to the external IP address allocated to the Harbor Load Balancer service.
Alternatively, if the LoadBalancer solution used for the underlying {kube} distribution supports assigning an IP address beforehand, both the `expose.loadBalancer.IP` configuration option and the `<harbor_fqdn>` value may be set to a predefined external IP address value.
--

. Configure external TLS and certificates
// TODO - Missing renewal methods (manual, automatic with cert-manager)
+
TLS certificates are required to secure access to the {spr} services that are exposed for external consumption - the Harbor UI/API and the Notary API (if Notary is enabled).
These certificates may either be generated automatically during installation (default), or provided as {kube} secrets, or configured beforehand as the default TLS certificate for the {kube} Ingress Controller used to expose the services, as explained in the <<requirements-tls,TLS Certificates requirements>> section.
+
--
[loweralpha]
. Auto-generated certificates
+
This is the default helm chart setting. If an Ingress was used to expose the {spr} services, the FQDN values configured for the ingress will be used to generate the TLS certificates automatically.
If using a LoadBalancer to expose the services instead of Ingress, please also set the `commonName` option to the pre-allocated external IP address or the FQDN value that will be resolved to it:
+
.harbor-values.yaml
[source,yaml]
----
expose:
..
  tls:
    enabled: true
    # The source of the tls certificate. Set it as "auto", "secret"
    # or "none" and fill the information in the corresponding section
    # 1) auto: generate the tls certificate automatically
    # 2) secret: read the tls certificate from the specified secret.
    # The tls certificate can be generated manually or by cert manager
    # 3) none: configure no tls certificate for the ingress. If the default
    # tls certificate is configured in the ingress controller, choose this option
    certSource: auto
    auto:
      # The common name used to generate the certificate, it's necessary
      # when the type isn't "ingress"
      commonName: "<harbor_fqdn_or_ip_addr>"
----

. Custom certificates
+
One or two custom certificates are required for exposed {spr} services: one for the Harbor UI/API and another one for the Notary API (required only if Notary is enabled). The certificates need to reflect the FQDN values or external IP address values used at the previous step to configure the Kubernete Ingress or LoadBalancer service exposure settings. The helm chart also supports using a single certificate instead of two, as long as the CN or SAN certificate field values match both FQDNs. The certificates need to be supplied in the form of {kube} secrets:
+
[source,bash]
----
kubectl create secret tls -n registry <harbor-tls-secret> --key ${HARBOR_CERT_KEY_FILE} --cert ${HARBOR_CERT_FILE}
kubectl create secret tls -n registry <notary-tls-secret> --key ${NOTARY_CERT_KEY_FILE} --cert ${NOTARY_CERT_FILE}
----
+
In case the certificate has intermediate CAs, you can bundle them into the CERT_FILE prior creating the secret, e.g.:
+
[source,bash]
----
cat $CERT_FILE $bundle_ca_file > bundled_cert_file
kubectl create secret tls -n registry <tls-secret> --key ${KEY_FILE} --cert bundled_cert_file
----
+
.harbor-values.yaml
[source,yaml]
----
expose:
..
  tls:
    enabled: true
    # The source of the tls certificate. Set it as "auto", "secret"
    # or "none" and fill the information in the corresponding section
    # 1) auto: generate the tls certificate automatically
    # 2) secret: read the tls certificate from the specified secret.
    # The tls certificate can be generated manually or by cert manager
    # 3) none: configure no tls certificate for the ingress. If the default
    # tls certificate is configured in the ingress controller, choose this option
    certSource: secret
    secret:
      # The name of secret which contains keys named:
      # "tls.crt" - the certificate
      # "tls.key" - the private key
      secretName: "<harbor-tls-secret>"
      # The name of secret which contains keys named:
      # "tls.crt" - the certificate
      # "tls.key" - the private key
      # Only needed when the "expose.type" is "ingress".
      notarySecretName: "<notary-tls-secret>"
----

. Default Ingress certificate
+
If a default TLS certificate has been set up for the {kube} Ingress Controller earlier, as covered in the TLS Certificates section, certificates don't need to be explicitly supplied during the {spr} installation. It's sufficient to set the `tls.certSource` option to `none`:
+
.harbor-values.yaml
[source,yaml]
----
expose:
..
  tls:
    enabled: true
    # The source of the tls certificate. Set it as "auto", "secret"
    # or "none" and fill the information in the corresponding section
    # 1) auto: generate the tls certificate automatically
    # 2) secret: read the tls certificate from the specified secret.
    # The tls certificate can be generated manually or by cert manager
    # 3) none: configure no tls certificate for the ingress. If the default
    # tls certificate is configured in the ingress controller, choose this option
    certSource: none
----
--

. Configure internal TLS
+
In addition to securing external connections to exposed services, {spr} also supports using TLS to secure internal communication between its components.
TLS certificates will be generated automatically for this purpose. Enabling internal TLS is optional, but highly recommended:
+
.harbor-values.yaml
[source,yaml]
----
internalTLS:
  enabled: true
----
+
[IMPORTANT]
====
Internal TLS support does not yet cover the internal database and Redis services.
====

. Configure Persistent Storage
.. Configure Persistent Volumes
+
By default, persistent volumes are enabled for all stateful components of {spr}.
However, a default `StorageClass` must be configured in the {kube} cluster to be able to provision volumes dynamically.
Alternatively, explicit `StorageClass` values may be configured for each component.
+
For each component that uses persistent storage, the following settings can be configured:
+
--
[lowerroman]
... `storageClass`: Specify the "storageClass" used to provision the volume, if empty the default `StorageClass` will be used (default: `empty`).
... `accessMode`: Volumes can be mounted on a container in any way supported by the storage provider. Valid values are:
[arabic]
.... `ReadWriteOnce`: the volume can be mounted as read-write by a single container
.... `ReadWriteMany`: the volume can be mounted as read-write by many containers (required for jobservice when configured in high-availability mode and for registry when configured in high-availability mode and using persistent volume to store OCI artifacts)
(default: `ReadWriteOnce`)
... size: the size of the volume to be provisioned (e.g. 5Gi for 5 gigabytes). Default values varies by component:
+
[arabic]
.... registry: 5Gi
.... jobservice: 1Gi
.... databasae: 1Gi
.... redis: 1Gi
.... trivy: 5Gi

+
[WARNING]
====
The default volume sizes provided by {spr} are *not recommended for production*.

It is recommended to carefully plan and set the volumes size according to the expected usage.
Expanding in-use persistent volumes claims is supported only by some storage providers and in some cases it requires restarting the pods which will impact the service availability.
====

For configuring persistent storage update `harbor-values.yaml` configuration file with the following configuration and set their values accordingly:

.harbor-values.yaml
[source,yaml]
----
persistence:
  persistentVolumeClaim:
    registry:
      storageClass: ""
      accessMode:
      size:
    jobservice:
      storageClass: ""
      accessMode:
      size:
    database:
      storageClass: ""
      accessMode:
      size:
    redis:
      storageClass: ""
      accessMode:
      size:
    trivy:
      storageClass: ""
      accessMode:
      size:
----

.Using external services
[NOTE]
====
The above settings will be ignored and may be omitted for components configured to use an external service (`database`, `redis`), as well as for the registry component when external storage is configured for OCI artifacts.
====

[WARNING]
====
In the absence of a {kube} StorageClass with ReadWriteMany access mode capabilities, the `updateStrategy.type` option must set to `Recreate` in the `harbor-values.yaml` file, otherwise running `helm upgrade` to apply subsequent configuration changes or to perform upgrades will result in failure:

[source,yaml]
----
# The update strategy for deployments with persistent volumes(jobservice, registry
# and chartmuseum): "RollingUpdate" or "Recreate"
# Set it as "Recreate" when "RWM" for volumes isn't supported
updateStrategy:
  type: Recreate
----
====
--

.. Configure External Storage for OCI Artifacts
+
The default option for storing OCI artifacts, such as container images and helm charts, is using a persistent volume provided by the default `storageClass` of your {kube} cluster (as described on the previous section).
However, it is possible to configure {spr} to use an external storage solution such as Amazon S3 or Azure Blob Storage to store those artifacts.
+
For example, for Azure Blob Storage, an Azure Storage Account and Azure Storage Container needs to be pre-configured.
Using the `az` command line client, the following commands can be executed to create and fetch necessary resources:
+
[source,bash]
----
az storage account create --resource-group <azure-resource-group> --name <azure-storage-account-name>
az storage account keys list --resource-group <azure-resource-group> --account-name <azure-storage-account-name> -o tsv | head -n 1 | cut -f 3
az storage container create --account-name <azure-storage-account-name> --name <azure-storage-container-name> --auth-mode key
----
+
Then, the "imageChartStorage" section needs to be configured in the `harbor-values.yaml` file as follows:
+
.harbor-values.yaml
[source,yaml]
----
persistence:
...
  imageChartStorage:
    type: azure
    azure:
      accountname: <azure-storage-account-name>
      accountkey: <azure-storage-account-key>
      container: <azure-storage-container-name>
----
+
For Amazon S3, the process is similar. The `imageChartStorage` section in the `harbor-values.yaml` file will look like this:
+
.harbor-values.yaml
[source,yaml]
----
persistence:
...
  imageChartStorage:
    type: s3
      region: <aws-region>
      bucket: <aws-s3-bucket-name>
      accesskey: <aws-account-access-key>
      secretkey: <aws-account-secret-key>
----

. (Optional) Configure high-availability parameters
+
By default, {spr} uses a replica count (i.e. number of redundant pods providing the same service) value of 1 for all its components.
To have a highly-available deployment, configure a `ReplicaCount` value of at least 2 for enabled services in the `harbor-values.yaml` file:
+
.harbor-values.yaml
[source,yaml]
----
portal:
  replicas: 3
core:
  replicas: 3
# Only enabled when using a LoadBalancer instead of Ingress to expose services
nginx:
  replicas: 3
jobservice:
  replicas: 3
registry:
  replicas: 3
trivy:
  replicas: 3
notary:
  server:
    replicas: 3
  signer:
    replicas: 3
----
+
[WARNING]
====
A {kube} `StorageClass` with `ReadWriteMany` access mode is required to enable high-availability for some {spr} components:

* The `jobservice` component
* The registry component, when a {kube} persistent volume is used as the storage back-end for OCI artifacts

If a `StorageClass` with `ReadWriteMany` access is not available for your {kube} cluster, setting the replica count to a value higher than 1 for these components will result in installation failure.
Furthermore, using `helm upgrade` to apply subsequent configuration changes or to perform upgrades will also result in failures without a `ReadWriteMany` access mode `StorageClass`.
To prevent that, ensure the `updateStrategy.type` option is set to `Recreate` in the `harbor-values.yaml` file:

.harbor-values.yaml
[source,yaml]
----
# The update strategy for deployments with persistent volumes(jobservice, registry
# and chartmuseum): "RollingUpdate" or "Recreate"
# Set it as "Recreate" when "RWM" for volumes isn't supported
updateStrategy:
  type: Recreate
----
====

. [[install-external-database]] (Optional) External Database Setup
+
An external database is recommended to deploy {spr} in a fully highly-available and scalable setup.
This section assumes a managed PostgreSQL database instance has already been setup, either in Azure or AWS, as covered in the <<requirements-external-postgres>>.
+
[loweralpha]
.. Connect to an Azure PostgreSQL database
+
Add the following section to the `harbor-values.yaml` file and fill it with information reflecting the Azure PostgreSQL database instance previously configured as an external database:
+
.harbor-values.yaml
[source,yaml]
----
database:
  type: external
  external:
    host: <database-fully-qualified-hostname>
    port: "5432"
    username: <admin-user>@<database-hostname>
    password: <admin-password>
    # "disable" - No SSL
    # "require" - Always SSL (skip verification)
    # "verify-ca" - Always SSL (verify that the certificate presented by the
    # server was signed by a trusted CA)
    # "verify-full" - Always SSL (verify that the certification presented by the
    # server was signed by a trusted CA and the server host name matches the one
    # in the certificate)
    sslmode: "verify-full"
----

.. Connect to an AWS PostgreSQL database
+
Add the following section to the `harbor-values.yaml` file and fill it with information reflecting the AWS PostgreSQL database instance previously configured as an external database:
+
.harbor-values.yaml
[source,yaml]
----
database:
  type: external
  external:
    host: <database-fully-qualified-hostname>
    port: "5432"
    username: <admin-user>@<database-hostname>
    password: <admin-password>
    # "disable" - No SSL
    # "require" - Always SSL (skip verification)
    # "verify-ca" - Always SSL (verify that the certificate presented by the
    # server was signed by a trusted CA)
    # "verify-full" - Always SSL (verify that the certification presented by the
    # server was signed by a trusted CA and the server host name matches the one
    # in the certificate)
    sslmode: "verify-full"
----

. [[install-redis-operator]] (Optional) Install Redis Operator
+
As mentioned above, Redis Operator provides High Availability to the Redis component of {spr}. It can be installed into the same {kube} cluster as {spr}. The installation of Redis operator is also done via a Helm chart, and must happen before the installation of {spr}.
+
// Preliminary instructions!
+
[loweralpha]
... Install Redis operator using the Helm chart:
+
[source,bash]
----
export HELM_EXPERIMENTAL_OCI=1
helm chart pull registry.suse.com/harbor/redis-operator:3.1
helm chart export registry.suse.com/harbor/redis-operator:3.1
helm -n registry install harbor-redis ./redis-operator
----

... Create a secret:
+
Save the password for Redis to a file and create a {kube} secret with this new password.
+
[IMPORTANT]
====
The file containing the password must be literally be called `password`.
====
+
[source,bash]
----
echo -n "securepassword" > password
kubectl -n registry create secret generic redis-auth --from-file=password
rm password
----

... Install `RedisFailover` object:
+
The Redis HA configuration needs to be specified as a `RedisFailover` {kube} CRD object.
The following is an example configuration:
+
[source,yaml]
----
apiVersion: databases.spotahome.com/v1
kind: RedisFailover
metadata:
  name: harbor-redis
spec:
  sentinel:
    replicas: 3
    image: registry.suse.com/harbor/harbor-redis:2.1.0
    customConfig:
      - "dir /data"
    resources:
      requests:
        cpu: 100m
      limits:
        memory: 100Mi
  redis:
    replicas: 3
    image: registry.suse.com/harbor/harbor-redis:2.1.0
    customConfig:
      - "dir /data"
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
      limits:
        cpu: 400m
        memory: 500Mi
    storage:
      keepAfterDeletion: true
      persistentVolumeClaim:
        metadata:
          name: harbor-redis-data
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 10Gi
  auth:
    secretPath: redis-auth
----
+
At a minimum, the configuration options below should be customized as desired.
For more configuration options, the link:https://github.com/spotahome/redis-operator#usage[Redis Operator Documentation] may be used.
+
--
[lowerroman]
. Number of replicas for sentinel and Redis. Use a number higher than 2 for a highly-available installation
. The storage size for the `persistentVolumeClaim`
--
+
The `RedisFailover` configuration can be deployed using `kubectl`.
For example, assuming `redis.yaml` is the file containing the `RedisFailover` object properties, the command would be:
+
[source,bash]
----
kubectl create -n registry -f redis.yaml
----
... Wait until all the objects are prepared, specifically `rfs-harbor-redis`, the Sentinel deployment:
+
[source,bash]
----
> kubectl get deployments -l app.kubernetes.io/component=sentinel -n registry
NAME                READY   UP-TO-DATE   AVAILABLE   AGE
rfs-harbor-redis    3/3     3            3           7m57s
----

... Configure {spr} to be connected to the external Redis
+
Extend the `harbor-values.yaml` file with the configuration specified below.
+
.harbor-values.yaml
[source,yaml]
----
redis:
  type: external
  external:
    addr: rfs-harbor-redis:26379
    sentinelMasterSet: mymaster
    password: securepassword // <1>
----
<1> Replace the value for password key with the password prepared in <<install-redis-operator>>.

. [[install-external-redis]] (Optional) External Redis Setup
+
An external Redis is recommended to deploy {spr} in a fully highly-available and scalable setup.
When deployed in AKS or EKS, as an alternative to using the Redis Operator, {spr} may instead be connected to a managed Redis instance running in public cloud.
This section assumes a managed Redis instance has already been setup, either in Azure or AWS, as covered in the External Redis requirements section.

.. Connect to an Azure Cache for Redis instance
+
Add the following section to the `harbor-values.yaml` file and fill it with information reflecting the Azure Cache for Redis instance previously prepared.
As mentioned above in the <<requirements-redis-azure>>, the address will have the form of `<azure-redis-cache>.redis.cache.windows.net`.
+
.harbor-values.yaml
[source,yaml]
----
redis:
  type: external
  external:
    addr: "192.168.0.2:6379"
    password: "securepassword" // <1>
----
<1> Replace `securepassword` with the password you configured in the Azure Cache for Redis instance.
// TODO AI (Dirk Mueller) needs to be reviewed
.. Connect to an Amazon ElastiCache Redis service
+
Add the following section to the `harbor-values.yaml` file and fill it with information reflecting the Amazon ElastiCache Redis instance previously prepared:
+
.harbor-values.yaml
[source,yaml]
----
redis:
  type: external
  external:
    addr: "192.168.0.2:6379"
    password: "" // <1>
----
<1> Add password if configured manually (not the default) in AWS ElastiCache.

// .. Connect to an Amazon ElastiCache Redis service
// TODO - AI jsuchome - Add AWS method for Redis

. [[install-resource-limits]] (Optional) Setup Resource Requests and Limits
+
It is a good practice to specify resource requests and limit values.
For each Harbor component it is possible to specify minimal resource value, i.e. the amount of CPU units and memory it should get, as well as a limit value so that Kubernetes knows the resources given to a component cannot exceed the such limit.
These per-component values are used for all containers that are created for given Harbor component.
+
For example, add the following section to the `harbor-values.yaml` to specify that the containers from the core component should get at least 0.1 cpu and 256MiB of RAM and not more than 1 CPU and 1GIB of memory:
+
.harbor-values.yaml
[source,yaml]
----
core:
  resources:
    requests:
      memory: 256Mi
      cpu: 100m
    limits:
      cpu: 1
      memory: 1Gi
----
+
Read more about Resource management in the link:https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/[upstream documentation].

. [[install-passwords]] Set up the passwords for deployment
+
By default, all passwords are automatically generated when installing {spr} with the Helm chart. They can be retrieved post-installation from the created {kube} secrets objects. For example, to retrieve the Harbor adminstrator password necessary to log in into the Harbor Portal UI as admin user, run this command after the deployment is finished:
+
[source,bash]
----
kubectl get secret suse-registry-harbor-core -n registry -o jsonpath="{.data.HARBOR_ADMIN_PASSWORD}" | base64 --decode
----
+
To set a custom administrator password before the installation, modify your `harbor-values.yaml` file like this:
+
.harbor-config-values.yaml
[source,yaml]
----
harborAdminPassword: <password-for-admin-user>
----
+
Similarly, custom passwords may be set before the installation for the database and Redis services, if configured as internal services:
+
.harbor-config-values.yaml
[source,yaml]
----
database:
  ...
  internal:
    password: <password-for-redis>

redis:
  ...
  internal:
    password: <password-for-redis>
----

. Finally, deploy helm to install {spr}
+
To install {spr} as a suse-registry release into the registry namespace with the custom configuration prepared in the `harbor-values.yaml` file in the previous steps, run the following command:
+
[source,bash]
----
helm -n registry install suse-registry ./harbor -f harbor-values.yaml
----
+
Once the installation is complete, Helm will provide the information about the location of the newly installed registry, e.g.:
+
[source,bash]
----
NAME: suse-registry
LAST DEPLOYED: Fri Jul 24 10:34:53 2020
NAMESPACE: registry
STATUS: deployed
REVISION: 1
NOTES:
Please wait for several minutes for Harbor deployment to complete.
Then you should be able to visit the Harbor portal at https://core.harbor.domain // <1>
----
<1> You will see your `<core_fqdn>` instead of `https://core.harbor.domain`.

. Check the installation
+
You can check the status of created artifacts and see if everything is running correctly:
+
[source,bash]
----
> kubectl -n registry get deployments
NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
suse-registry-harbor-core         1/1     1            1           17h
suse-registry-harbor-jobservice   1/1     1            1           17h
suse-registry-harbor-portal       1/1     1            1           17h
suse-registry-harbor-registry     1/1     1            1           17h
----
+
[source,bash]
----
> kubectl -n registry get pods
NAME                                                  READY   STATUS    RESTARTS   AGE
suse-registry-harbor-core-c787885b6-2l7lz             1/1     Running   1          105m
suse-registry-harbor-database-0                       1/1     Running   0          105m
suse-registry-harbor-jobservice-698fb5bb44-88mc5      1/1     Running   1          105m
suse-registry-harbor-nginx-b4f7748c5-8v2rp            1/1     Running   0          105m
suse-registry-harbor-portal-bff5898cc-tt9ss           1/1     Running   0          105m
suse-registry-harbor-redis-0                          1/1     Running   0          105m
suse-registry-harbor-registry-7f65b6f87b-sqhzt        2/2     Running   0          105m
suse-registry-harbor-trivy-0                          1/1     Running   0          105m
----

After the installation is complete, please proceed with <<administration>> and configure an authentication method.
