:docinfo:

:localdate:

// Document Variables
:slesProdVersion: 15
//

= SAP Convergent Mediation ControlZone High Availability Cluster: Setup Guide

// Revision {Revision} from {docdate}
// Standard SUSE includes
// include::common_copyright_gfdl.adoc[]

// :toc:

include::Variables_s4_2101.adoc[]
//

////
TODO PRIOx: example
////

== About this guide

The following sections focus on background information and the purpose of the document at hand.

=== Introduction

{sles4sapReg} is the optimal platform to
run {sapReg} applications with high availability. Together with a redundant layout
of the technical infrastructure, single points of failure can be eliminated.

{sapBSReg} is a sophisticated application platform for large enterprises
and mid-size companies. Many critical business environments require the highest
possible {sapReg} application availability.

The described cluster solution can be used for {sapReg} {sapS4pl}.

TODO

[id="sec.resource"]
=== Additional documentation and resources

Several chapters in this document contain links to additional documentation resources that
are either available on the system or on the Internet.

For the latest product documentation updates, see https://documentation.suse.com/.

More whitepapers, guides and best practices documents referring to SUSE Linux Enterprise Server and SAP can be
found and downloaded at the SUSE Best Practices Web page:
https://documentation.suse.com/sbp/sap/

Here you can access guides for {SAPHANA} system replication
automation and High Availability (HA) scenarios for {SAPNw} and {s4hana}.

Additional resources, such as customer references, brochures or flyers, can be found at the SUSE Linux
Enterprise Server for SAP Applications resource library:
https://www.suse.com/products/sles-for-sap/resource-library/.

Supported high availability solutions by {sles4sap} overview:
https://documentation.suse.com/sles-sap/sap-ha-support/html/sap-ha-support/article-sap-ha-support.html

Lastly, there are manual pages shipped with the product.


// Standard SUSE includes
=== Feedback
include::common_intro_feedback.adoc[]

//=== Documentation Conventions
//TODO PRIO3: work on SUSE doc standard conventions file
//include::common_intro_typografie.adoc[]

== Scope of this document

The document at hand explains how to:

TODO

== Overview

TODO

=== Prerequisites

TODO


=== IP addresses and virtual names

Check if the file _/etc/hosts_ contains at least the following address resolutions.
Add those entries if they are missing.

[subs="attributes"]
----
{myIPNode1}  {myNode1}
{myIP2nd1}  {my2nd1}
{myIP2nd2}  {my2nd2}

{myVipAAscs}  sap{mySidLc}as
{myVipAErs}  sap{mySidLc}er
{myVipADb}  sap{mySidLc}db
{myVipAPas}  sap{mySidLc}d1
{myVipAAas}  sap{mySidLc}d2
----


=== Mount points and NFS shares

TODO

=== Linux user and group number scheme

TODO


=== Preparing the operating system  and installing the cluster softwaren

- Set up and enable *chrony* with *YaST*.

- Install the RPM pattern *ha_sles* on both cluster nodes.
+
[subs="attributes"]
----
# zypper in -t pattern ha_sles
----

=== Configuring the cluster base

.Tasks

- To configure the cluster base, you can use either YaST or the interactive
command line tool *ha-cluster-init*. The example below uses the command line wizard.

- Install and configure the watchdog device on the first machine.

Instead of deploying the software based solution, a hardware-based watchdog device should preferably be used.
The following example uses the software device but can be easily adapted to the hardware device.

[subs="attributes"]
----
# modprobe softdog
# echo softdog > /etc/modules-load.d/watchdog.conf
# systemctl restart systemd-modules-load
# lsmod | egrep "(wd|dog|i6|iT|ibm)"
----

- Install and configure the cluster stack on the first machine

[subs="attributes"]
----
# ha-cluster-init -u -s  {myDevPartSbd}
----

- Join the second node

On the second node, perform some preparation steps.

[subs="attributes"]
----
# modprobe softdog
# echo softdog > /etc/modules-load.d/watchdog.conf
# systemctl restart systemd-modules-load
# lsmod | egrep "(wd|dog|i6|iT|ibm)"
----

To configure the cluster base, you can use either YaST or the interactive
command line tool *ha-cluster-join*. The example below uses the command line wizard.

[subs="attributes"]
----
# ha-cluster-join -c {my2nd1}
----

- The _crm_mon -1r_ output should look as follows:

[subs="attributes"]
----
Stack: corosync
Current DC: {my2nd1} (version 1.1.18+20180430.b12c320f5-1.14-b12c320f5) - partition with quorum
Last updated: Mon Jan 28 13:10:37 2019
Last change: Wed Jan 23 09:52:57 2019 by root via cibadmin on {my2nd1}

2 nodes configured
1 resource configured

Online: [ {my2nd1} {my2nd2} ]

stonith-sbd	(stonith:external/sbd):	Started {my2nd1}
----

=== Configuring cluster resources

TODO


==== Activating the cluster

The last step is to end the cluster maintenance mode and to allow the
cluster to detect already running resources.

As user _root_, type the following command:

[subs="attributes"]
----
# crm configure property maintenance-mode="false"
----


== Administration

=== Dos and don'ts

TODO


==== Migrating the ControlZone instance

TODO

==== Setting ControlZone resources into maintenance mode

TODO

==== Ending ControlZone resources maintenance

TODO

==== Cleaning up resources

TODO


=== Testing the cluster

TODO

==== Manually migrating ControlZone resource

TODO


==== Testing ControlZone migration after operating system failure

TODO

==== Restarting ControlZone instance in-place using _Stop_ and _Start_

TODO

==== Additional tests

TODO


== References

For more information, see the documents listed below.

=== Pacemaker
- Pacemaker documentation online:
https://clusterlabs.org/pacemaker/doc/

:leveloffset: 2
include::SAPNotes-convergent-mediation.adoc[]

++++
<?pdfpagebreak?>
++++

////
############################
#
# APPENDIX
#
############################
////

:leveloffset: 0
== Appendix

=== CRM configuration

TODO

=== Corosync configuration of the two-node cluster

Find below the corosync configuration for one corosync ring. Ideally two rings would be used.

[subs="specialchars,attributes"]
----
{my2nd1}:~ # cat /etc/corosync/corosync.conf
# Read the corosync.conf.5 manual page
totem {
    version: 2
    secauth: on
    crypto_hash: sha1
    crypto_cipher: aes256
    cluster_name: hacluster
    clear_node_high_bit: yes
    token: 5000
    token_retransmits_before_loss_const: 10
    join: 60
    consensus: 6000
    max_messages: 20
    interface {
        ringnumber: 0
        mcastport: 5405
        ttl: 1
    }

    transport: udpu
}

logging {
    fileline: off
    to_stderr: no
    to_logfile: no
    logfile: /var/log/cluster/corosync.log
    to_syslog: yes
    debug: off
    timestamp: on
    logger_subsys {
        subsys: QUORUM
        debug: off
    }

}

nodelist {
    node {
        ring0_addr: {myIP2nd1}
        nodeid: 1
    }

    node {
        ring0_addr: {myIP2nd2}
        nodeid: 2
    }

}

quorum {

    # Enable and configure quorum subsystem (default: off)
    # see also corosync.conf.5 and votequorum.5
    provider: corosync_votequorum
    expected_votes: 2
    two_node: 1
}
----

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
== Legal notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

//
// REVISION 0.1 2024/01
//
