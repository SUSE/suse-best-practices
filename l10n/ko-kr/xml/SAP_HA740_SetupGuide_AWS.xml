<?xml version="1.0"?>
<?asciidoc-toc?><?asciidoc-numbered?><article
xmlns:xi="http://www.w3.org/2001/XInclude" xmlns="http://docbook.org/ns/docbook" xmlns:xl="http://www.w3.org/1999/xlink" xml:base="SAP_HA740_SetupGuide_AWS.xml" version="5.0" xml:lang="ko"><title>AWS 클라우드용 SAP NetWeaver High Availability Cluster 7.40 - 설정 가이드</title>
<info>

<date>2020-02-06</date>
<dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:bugtracker>
        <dm:url>https://github.com/SUSE/suse-best-practices/issues/new</dm:url>
        <dm:product>AWS 클라우드용 SAP NetWeaver High Availability Cluster 7.40 - 설정 가이드</dm:product>
    </dm:bugtracker>
</dm:docmanager>

<!--<title>SAP NetWeaver High Availability Cluster7.40 for the AWS Cloud</title>-->
<!--<subtitle>Setup Guide</subtitle>-->
    <!-- <orgname>SUSE Best Practices</orgname>-->
    <productname>SUSE Linux Enterprise Server for SAP Applications</productname>
    <productnumber>12 SP3 and newer</productnumber>

    <author>
        <personname>
            <firstname>Fabian</firstname>
            <surname>Herschel, 특별 아키텍트 SAP, SUSE</surname>
        </personname>
        <!-- <personname>
            <firstname>Fabian</firstname>
            <surname>Herschel</surname>
            </personname>
            <affiliation>
            <jobtitle>Distinguished Architect SAP</jobtitle>
            <orgname>SUSE</orgname>
            </affiliation>-->
    </author>
    <author>
        <personname>
            <firstname>Bernd</firstname>
            <surname>Schubert, SAP 솔루션 아키텍트, SUSE</surname>
        </personname>
        <!-- <personname>
            <firstname>Bernd</firstname>
            <surname>Schubert</surname>
            </personname>
            <affiliation>
            <jobtitle>SAP Solution Architect</jobtitle>
            <orgname>SUSE</orgname>
            </affiliation>-->
    </author>
    <author>
        <personname>
            <firstname>Stefan</firstname>
            <surname>Schneider, 파트너 솔루션 아키텍트, AWS</surname>
        </personname>
    </author>
    <author>
        <personname>
            <firstname>Manas</firstname>
            <surname>Srivastava, 선임 컨설턴트 SAP, AWS</surname>
        </personname>
    </author>

    <cover role="logos">
        <mediaobject>
                <imageobject>
                        <imagedata fileref="suse.svg" width=""/>
                </imageobject>
        </mediaobject>
        </cover>

<!--<date>August 16, 2019</date>-->


<abstract>

        <para>SUSE® Linux Enterprise Server for SAP Applications는 여러 가지 측면에서 SAP* 응용 프로그램에 최적화되어 있습니다.</para>
    <para>본 문서는 AWS 플랫폼에서의 인큐 복제 시나리오를 위해 SUSE Linux Enterprise Server for SAP Applications 12를 사용하여 고가용성 클러스터 솔루션을 배포하는 방법을 설명합니다. 목표는 SAP NW- HA-CLU 7.40 인증 사양 및 목표를 충족하는 것입니다. 이 문서는 SUSE Linux Enterprise Server for SAP Applications 12 SP3를 기준으로 작성하였습니다. 그러나 SUSE Linux Enterprise Server for SAP Applications 최신 서비스 팩에도 이 개념을 적용할 수 있습니다.</para>
    <para><emphasis role="strong">고지 사항</emphasis>: 본 문서는 SUSE 모범 사례 시리즈의 일부입니다. 이 시리즈의 모든 문서는 SUSE 직원 및 제3자가 자발적으로 기여하여 작성한 것입니다. 문서에서 별도로 설명하는 경우를 제외하고, 문서의 목적은 특정 작업을 수행하는 방법의 한 가지 예만을 제공하는 것입니다. 또한, SUSE는 문서에서 설명되는 작업이 해당 기능을 수행하는지 또는 의도하지 않은 결과가 제공되지 않는지를 확인할 수 없습니다. 본 문서의 모든 정보는 최대한 주의를 기울여 작성되었습니다. 그러나 이것이 문서의 정확성을 보장하지는 않습니다. 따라서 SUSE LLC, 그 계열사, 저자, 번역자 중 어느 누구도 있을 수 있는 오류나 그에 따른 결과에 책임을 지지 않는다고 명확히 진술할 필요가 있습니다.
    </para>

</abstract>
</info>
<section xml:id="id-about-this-guide">
<title>이 가이드 정보</title>
<section xml:id="id-introduction">
<title>소개</title>
<para>SUSE® Linux Enterprise Server for SAP Applications는 고가용성 SAP* 응용 프로그램을 실행하기 위한 최적의 플랫폼입니다. 여기에 중복 레이아웃의 기술 인프라를 사용하면 단일 오류 지점을 제거할 수 있습니다.</para>
<para>SAP* Business Suite는 대기업 및 중견기업을 위한 정교한 응용 프로그램 플랫폼입니다. 핵심 비즈니스 환경에는 최대한의 SAP* 응용 프로그램 가용성이 필요한 경우가 많습니다.</para>
<para>여기서 설명하는 클러스터 솔루션은 SAP* S/4 HANA 및 SAP* SAP NetWeaver용으로 사용할 수 있습니다.</para>
<para>SAP NetWeaver는 SAP 비즈니스 응용 프로그램을 지원하기 위해 사용하는 일반 미들웨어 기능 스택입니다. SAP Enqueue Replication Server는 SAP NetWeaver 스택의 가장 중요한 구성 요소 중 하나인 인큐(enqueue) 서비스를 위한 응용 프로그램 수준 중복을 구성합니다. SUSE Linux Enterprise Server for SAP Applications에 대한 예시에서와 같이, 응용 프로그램 수준 중복과 고가용성 클러스터 솔루션을 결합하면 인큐 중복 메커니즘에서 최적의 효과를 달성할 수 있습니다. 여기서 설명하는 개념은 수년간 다양한 규모 및 지점의 고객에서 실제 운영 작업을 통해 완성도가 입증되었습니다.</para>
</section>
<section xml:id="id-additional-documentation-and-resources">
<title>추가 문서 및 리소스</title>
<para>본 추가 문서의 각 장에는 시스템 또는 인터넷에서 사용할 수 있는 추가 문서 리소스로 연결되는 링크가 수록되어 있습니다.</para>
<para>최신 문서 업데이트는 <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://documentation.suse.com">https://documentation.suse.com</link>을 참조하십시오.</para>
<para><link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://www.suse.com/products/sles-for-sap/resource-library/">https://www.suse.com/products/sles-for-sap/resource-library/</link>의 SUSE Linux Enterprise Server for SAP Applications 리소스 라이브러리에서도 여러 백서 및 기타 리소스를 살펴볼 수 있습니다.</para>
<para>이 가이드 및 기타 SAP 모범 사례는 <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://documentation.suse.com/sbp/all/">https://documentation.suse.com/sbp/all/</link>에서 다운로드할 수 있습니다. 여기에서 SAP NetWeaver 및 SAP S/4 HANA용 SAP HANA 시스템 복제 자동화 및 HA 시나리오에 대한 가이드를 찾아볼 수 있습니다.</para>
</section>
<section xml:id="id-feedback">
<title>피드백</title>
<para>여러 피드백 채널을 사용할 수 있습니다.</para>
<variablelist>
<varlistentry>
<term>버그 및 기능 향상 요청</term>
<listitem>
<para>제품과 관련하여 사용 가능한 서비스 및 지원 옵션에 대해서는 <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://www.suse.com/support/">http://www.suse.com/support/</link>를 참조하십시오.</para>
</listitem>
</varlistentry>
</variablelist>
<para>제품 구성 요소의 버그를 보고하려면 <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://scc.suse.com/support/">https://scc.suse.com/support/requests</link>로 이동하여 로그인한 후 <emphasis>새 SR(Service Request: 서비스 요청) 제출</emphasis>을 선택하십시오.</para>
<variablelist>
<varlistentry>
<term>메일</term>
<listitem>
<para>본 제품 문서에 대한 피드백이 있는 경우 <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="mailto:doc-team@suse.com">doc-team@suse.com</link>으로 메일을 보내 주십시오. 문서의 문서 제목, 제품 버전 및 게시 날짜를 포함해야 합니다. 오류를 보고하거나 개선사항을 제안하려면 문제점에 대한 간략한 설명을 제공하고 해당 섹션 번호 및 페이지(또는 URL)를 참조하십시오.</para>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="id-scope-of-this-document">
<title>본 문서의 범위</title>
<para>이 가이드는 다음 작업 방법을 자세히 설명합니다.</para>
<itemizedlist>
<listitem>
<para>SAP Enqueue Replication Server를 포함한 SAP NetWeaver용 SUSE Linux Enterprise High Availability 플랫폼 계획 수립</para>
</listitem>
<listitem>
<para>Linux 고가용성 플랫폼 설정 및 SUSE Linux Enterprise 기반 SAP Enqueue Replication Server 등 기본 SAP NetWeaver 설치</para>
</listitem>
<listitem>
<para>SAP가 인증한 방식으로 sap-suse-cluster-connector를 통해 고가용성 클러스터와 SAP 컨트롤 프레임워크 통합</para>
</listitem>
<listitem>
<para>SAP 노트 2309342(SUSE Linux Enterprise High Availability Extension on AWS)의 설명과 같이 AWS 기반 SAP HANA 데이터베이스용 HA 클러스터 솔루션 설치</para>
</listitem>
</itemizedlist>
<para>이 가이드는 중앙 서비스의 고가용성(HA)을 중심으로 설명합니다. 데이터베이스 및 SAP NetWeaver 인스턴스용 HA 클러스터 솔루션에 대한 설명은 SUSE 모범 사례 문서 웹 페이지(“추가 문서 및 리소스” 섹션 참조)에서 제공되는 모범 사례 “단순 스택”에서 제공됩니다. SAP HANA 시스템 복제의 경우 성능 최적화 또는 비용 최적화 시나리오용 가이드를 따르십시오.</para>
</section>
<section xml:id="id-overview">
<title>개요</title>
<para>이 가이드는 AWS 플랫폼에서의 Enqueue Replication 시나리오를 위해 SUSE Linux Enterprise Server for SAP Applications 12를 사용한 pacemaker 클러스터 설정 방법을 설명합니다. 이 가이드에서는 온프레미스 pacemaker 클러스터 설치 방법은 설명하지 않습니다. 목표는 SAP NW-HA-CLU 7.40 인증 사양 및 목표를 충족하는 것입니다.</para>
<para>목표에는 다음이 포함됩니다.</para>
<itemizedlist>
<listitem>
<para>SAP 시작 프레임워크인 <emphasis>sapstartsrv</emphasis>와 클러스터를 통합하여 유지보수 절차로 인해 클러스터의 안정성이 손상되지 않게 방지</para>
</listitem>
<listitem>
<para>RKS(Rolling Kernel Switch) 인식</para>
</listitem>
<listitem>
<para>표준 SAP 설치로 지원 프로세스 향상</para>
</listitem>
</itemizedlist>
<para>업데이트된 인증 SAP NW-HA-CLU 7.40은 일부 테스트 절차를 재정의하고 특수 조건에서 클러스터의 새 동작 방법을 설명합니다. 이러한 변경 사항을 통해 클러스터 아키텍처를 개선하고 더 편리하게 사용 및 설정할 수 있도록 설계했습니다.</para>
<para>공유 SAP 리소스는 AWS EFS(Elastic File Systems)에서 관리됩니다. SAP 인스턴스 자체는 EFS 파일 시스템에 설치되어 파일 시스템의 전환을 통해 원활하게 기능할 수 있습니다.</para>
<section xml:id="id-using-aws-architectures-in-suse-linux-enterprise-server-pacemaker-clusters">
<title>SUSE Linux Enterprise Server Pacemaker 클러스터에서 AWS 아키텍처 사용</title>
<para>SUSE Linux Enterprise Server pacemaker 클러스터는 AWS 리전에 설치됩니다. AWS 리전은 여러 개의 가용 영역으로 구성됩니다. 가용 영역은 10~50km 떨어진 여러 다른 데이터 센터에 위치합니다. 가용 영역은 플러드 수준, 전기 및 네트워크 훅업이 각기 다릅니다. 그리고 독립적입니다. AWS는 중복 클러스터 노드가 가용 영역(AZ) 전반에 분포하여 고객이 개별 AZ 오류를 극복할 수 있도록 하는 아키텍처 패턴을 권장합니다.</para>
<para>AWS VPC(Virtual Private Cloud)는 모든 가용 영역에 두루 걸쳐 있습니다. 고객이 다음과 같다고 가정합니다.</para>
<itemizedlist>
<listitem>
<para>사용된 2개의 가용 영역을 확인</para>
</listitem>
<listitem>
<para>SUSE Linux Enterprise High Availability Extension 클러스터의 노드 2개를 호스팅할 수 있는 2개의 가용 영역에 서브넷 생성</para>
</listitem>
<listitem>
<para>서브넷 2개에 연결된 라우팅 테이블 사용</para>
</listitem>
<listitem>
<para>선택 사항: VPC에서 이름을 관리하는 Route53 프라이빗 호스팅 이름 지정 영역 호스트</para>
</listitem>
<listitem>
<para>클러스터의 모든 구성 요소는 동일한 Amazon 계정에 있어야 합니다. 다른 계정의 라우팅 테이블(공유 VPC 설정)과 같은 네트워킹 구성 요소 사용은 클러스터 리소스 에이전트에서 지원되지 않습니다. 다중 계정 환경이 필요한 경우에는 AWS 담당자에게 문의하여 교차 계정/VPC 액세스를 위한 Transit GateWay를 구현하는 것이 좋습니다.</para>
</listitem>
</itemizedlist>
<para>AWS 고유 구성 요소는 2가지 구성으로 설치할 수 있습니다. 두 구성 모두 AWS 오버레이 IP 주소를 사용합니다. 오버레이 IP 주소는 인스턴스가 위치한 가용 영역과 관계없이 네트워크 트래픽을 인스턴스로 전송할 수 있는 AWS 고유 라우팅 항목입니다.</para>
<para>SUSE Linux Enterprise High Availability Extension 클러스터는 필요한 경우 이 라우팅 항목을 업데이트합니다. VPC의 모든 SAP 시스템 구성 요소는 이 오버레이 IP 주소를 통해 VPC 내부에 SAP 시스템 구성 요소가 포함된 AWS 인스턴스에 도달할 수 있습니다.</para>
<para>오버레이 IP 주소의 한 가지 단점은 VPC 외부에 위치한 CIDR 범위에서 제공된다는 점입니다. 그렇지 않으면 서브넷과 특정 가용 영역의 일부가 됩니다.</para>
<para>AWS VPN 게이트웨이는 IP 주소로 트래픽을 라우팅하지 않으므로 SAP GUI 등의 온프레미스 사용자는 이 IP 주소에 도달할 수 없습니다. 이러한 제한을 해결할 수 있도록 고객에게는 두 가지 옵션이 제공됩니다.</para>
<orderedlist numeration="arabic">
<listitem>
<para>VPC에서 SAP 라우터 사용. 온프레미스 사용자가 IP 주소에 도달할 수 있습니다. SAP 라우터는 ASCS 시스템으로 트래픽을 릴레이할 수 있습니다.</para>
</listitem>
<listitem>
<para>추가 Route 53 에이전트 구성. Route 53은 AWS 고유 네임 서비스입니다. 클러스터 에이전트가 지정된 ASCS 서비스 이름에 대한 IP 주소를 변경합니다. 온프레미스 네임 서버는 AWS VPC의 서브 도메인에 대한 요청을 이 네임 서비스에 위임해야 합니다. 온프레미스 SAP GUI 사용자는 이름을 통해 ASCS와 통신합니다. 부록의 TBD 섹션에서는 Route 53과 로컬 이름 지정 서비스의 통합 방법을 설명합니다.</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-prerequisites-for-the-aws-specific-ha-installation">
<title>AWS 고유 HA 설치를 위한 필수 구성 요소</title>
<para>설치를 시작하기 전 충족해야 하는 여러 조건이 있습니다.</para>
<itemizedlist>
<listitem>
<para>AWS 계정이 있어야 함</para>
</listitem>
<listitem>
<para>관리자 권한 또는 최소한으로 다음 권한이 있는 AWS 사용자가 있어야 함</para>
<itemizedlist>
<listitem>
<para>보안 그룹 생성</para>
</listitem>
<listitem>
<para>EFS 파일 시스템 생성</para>
</listitem>
<listitem>
<para>AWS 라우팅 테이블 수정</para>
</listitem>
<listitem>
<para>정책을 만들어 IAM 역할에 연결</para>
</listitem>
<listitem>
<para>Route53 에이전트 설치 시 선택 사항</para>
<itemizedlist>
<listitem>
<para>프라이빗 호스팅 영역에 A 레코드 생성 및 수정</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>환경에 대한 이해</para>
<itemizedlist>
<listitem>
<para>리전 및 AWS 이름 파악</para>
</listitem>
<listitem>
<para>VPC 및 AWS ID 파악</para>
</listitem>
<listitem>
<para>VPC에서 사용할 가용 영역 파악</para>
</listitem>
<listitem>
<para>각 가용 영역에 서브넷 보유</para>
<itemizedlist>
<listitem>
<para>서브넷에 묵시적으로 또는 명시적으로 연결된 2개의 라우팅 테이블 보유</para>
</listitem>
<listitem>
<para>SAP 설치 및 EFS 마운트 지점을 위한 서브넷 2개에 사용 가능한 IP 주소 보유</para>
</listitem>
<listitem>
<para>두 서브넷 간 네트워크 트래픽 허용 서브넷에서 발신 인터넷 액세스 허용</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>선택 사항: 두 서브넷에서 인스턴스용 서브 도메인을 호스팅하는 Route 53 프라이빗 호스팅 영역 보유</para>
</listitem>
<listitem>
<para>SAP 중앙 인스턴스용 이름 및 IP 주소를 갖는 리소스 레코드 보유</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>부록의 체크리스트를 사용하여 필요한 모든 정보를 확인한 후 설치를 시작하십시오.</para>
<section xml:id="id-tagging-the-ec2-instances">
<title>EC2 인스턴스 태그 지정</title>
<para>EC2 인스턴스에는 자동으로 생성되는 호스트 이름이 있습니다. SAP 요구 사항을 준수하는 호스트 이름을 선택하십시오. 이와 관련한 자세한 내용은 SAP 노트 611361에서 확인할 수 있습니다.</para>
<para>클러스터 에이전트는 올바른 방법으로 EC2 인스턴스를 식별할 수 있어야 합니다. 이는 인스턴스 태그를 통해 수행할 수 있습니다.</para>
<para>AWS 명령줄 인터페이스(CLI) 또는 콘솔을 통해 <emphasis>클러스터</emphasis> 및 호스트 이름과 같이 임의로 선택한 태그를 두 EC2 인스턴스에 지정합니다. 이는 <emphasis>uname</emphasis> 명령으로 표시됩니다. 두 인스턴스에서 동일한 태그(예: <emphasis>클러스터</emphasis>) 및 개별 호스트 이름을 사용하십시오. EC2 인스턴스에 태그를 지정하는 방법에 대한 설명은 AWS 문서(<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html</link>)에서 찾을 수 있습니다.</para>
<note>
<para>클러스터 관리 리소스에 지정되는 태그에는 비ASCII 문자를 사용하지 마십시오.</para>
</note>
</section>
<section xml:id="id-security-groups">
<title>보안 그룹</title>
<important>
<para>이 섹션에서는 보안 그룹의 SAP 관련 포트에 대해 설명하지 않으며, SUSE 클러스터 전용으로 사용할 수 있어야 하는 포트만 나열합니다.</para>
</important>
<para>다음 포트 및 프로토콜은 2개의 클러스터 노드 간 통신을 허용하도록 구성해야 합니다.</para>
<itemizedlist>
<listitem>
<para>인바운드 UDP용 포트 5405: corosync 통신 계층을 구성하는 데 사용됩니다. 포트 5405는 공통 예시에서 사용되고 있습니다. corosync 구성에 따라 다른 포트를 사용할 수 있습니다.</para>
</listitem>
<listitem>
<para>인바운드 TCP용 포트 7630: SUSE “hawk” 웹 GUI에서 사용됩니다.</para>
</listitem>
<listitem>
<para>ICMP 활성화: SUSE 클러스터의 AWS IP-이동 에이전트에서 ping 명령을 통해 사용됩니다.</para>
</listitem>
</itemizedlist>
<para>아웃바운드 네트워크 통신에는 제한이 없는 것으로 가정합니다.</para>
</section>
<section xml:id="id-creating-an-aws-cli-profile-on-both-ec2-instances">
<title>두 EC2 인스턴스에 AWS CLI 프로파일 생성</title>
<para>SUSE Linux Enterprise Server 에이전트는 AWS 명령줄 인터페이스(CLI)를 사용합니다. 이 AWS CLI 프로파일을 두 인스턴스의 루트 계정 <emphasis>root</emphasis>에 대해 생성해야 합니다. SUSE 리소스에는 텍스트 형식으로 출력을 생성하는 프로파일이 필요합니다. 프로파일 이름은 임의로 지정할 수 있습니다. 아래 예시에서 선택한 이름은 <emphasis>cluster</emphasis>입니다. 인스턴스의 리전도 추가해야 합니다. 아래 예시에서 <emphasis>region-name</emphasis>이라는 문자열을 대상 리전으로 교체하십시오.</para>
<para>이러한 프로파일을 생성하는 한 가지 방법은 다음과 같은 내용으로 <emphasis>/root/.aws/config</emphasis> 파일을 만드는 것입니다.</para>
<screen>[default]
region = region-name
[profile cluster]
region = region-name
output = text</screen>
<para>다른 방법은 다음과 같이 <emphasis>aws configure</emphasis> CLI 명령을 사용하는 것입니다.</para>
<screen># aws configure
AWS Access Key ID [None]:
AWS Secret Access Key [None]:
Default region name [None]: region-name
Default output format [None]:

# aws configure --profile cluster
AWS Access Key ID [None]:
AWS Secret Access Key [None]:
Default region name [None]: region-name
Default output format [None]: text</screen>
<para>이 명령 시퀀스는 기본 프로파일 및 클러스터 프로파일을 생성합니다.</para>
</section>
<section xml:id="id-configure-http-proxies">
<title>HTTP 프록시 구성</title>
<para>시스템의 인터넷 액세스가 투명한 경우에는 HTTP 프록시를 구성할 필요가 없습니다. 리소스 에이전트는 AWS CLI(명령줄 인터페이스) 명령을 실행합니다. 이러한 명령은 HTTP/HTTPS 요청을 인터넷의 액세스 포인트로 전송합니다. 일반적으로 이러한 액세스 포인트와는 직접 통신할 수 있습니다.</para>
<para>그러나 투명한 인터넷 액세스를 제공하지 않는 시스템의 경우에는 HTTP/HTTPS 프록시를 제공해야 합니다. 프록시 액세스 구성은 AWS 문서에 자세히 설명되어 있습니다.</para>
<para>루트 사용자의 <emphasis>.bashrc</emphasis> 파일에 다음과 같은 환경 변수를 추가하십시오.</para>
<screen>export HTTP_PROXY=http://a.b.c.d:n
export HTTPS_PROXY=http://a.b.c.d:m</screen>
<para>인증이 필요한 경우, 위의 환경 변수가 아닌 다음 변수를 추가합니다.</para>
<screen>export HTTP_PROXY=http://username:password@a.b.c.d:n
export HTTPS_PROXY=http://username:password@a.b.c.d:m</screen>
<para>AWS의 SAP용 데이터 공급자는 인스턴스 메타 데이터 서비스와 직접 통신해야 합니다. 루트 사용자의 <emphasis>.bashrc</emphasis> 파일에 다음과 같은 환경 변수를 추가하십시오.</para>
<screen>export NO_PROXY=169.254.169.254</screen>
</section>
<section xml:id="id-add-a-second-ip-for-each-cluster-instance">
<title>각 클러스터 인스턴스에 보조 IP 추가</title>
<para>클러스터 구성에서는 각 클러스터 인스턴스에 IP 주소 두 개가 필요합니다. 인스턴스에 보조 IP 주소를 추가하면 SUSE 클러스터가 2개의 링 corosync 구성을 구현할 수 있습니다. 2개의 링 corosync 구성을 사용하면 기본 IP 주소를 통해 서로 통신하는 도중 문제가 발생하는 경우 클러스터 노드가 보조 IP 주소를 사용하여 서로 통신할 수 있습니다.</para>
<para>보조 IP 주소 할당 방법은 AWS 문서(<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/MultipleIP.html#assignIP-existing">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/MultipleIP.html#assignIP-existing</link>)를 참조하십시오.</para>
<para>AWS에서 보조 IP 주소를 클러스터 인스턴스에 연결한 후에는 보조 IP 주소를 클러스터 인스턴스에 추가해야 합니다. <emphasis>/etc/sysconfig/network/ifcfg-eth0</emphasis> 파일을 업데이트합니다. XX.XX.XX.XX를 새로운 보조 IP 주소로 바꾸고 ‘XX’를 두 자리 숫자 서브넷 마스크로 바꿉니다.</para>
<screen>IPADDR_1=‘XX.XX.XX.XX/XX'
LABEL_1="1"</screen>
<para>클러스터 인스턴스가 재부팅되면 시스템이 파일을 읽고 보조 IP 주소를 추가합니다. 추가적으로, 루트로 아래 명령을 실행하면 재부팅하지 않고 IP 주소를 클러스터 인스턴스 네트워크 스택에 추가합니다.</para>
<screen>ip address add XX.XX.XX.XX/XX dev eth0</screen>
<para>XX.XX.XX.XX를 새로운 보조 IP 주소로 바꾸고 ‘XX’를 두 자리 숫자 서브넷 마스크로 바꿉니다.</para>
</section>
<section xml:id="id-disable-the-sourcedestination-check-for-the-cluster-instances">
<title>클러스터 인스턴스의 소스/대상 확인 비활성화</title>
<para>소스/대상 확인은 AWS 명령줄 인터페이스(AWS-CLI)를 사용하여 스크립트를 통해 비활성화할 수 있습니다. 두 EC2 인스턴스에서 다음 명령을 1회 실행해야 하며, 이를 통해 오버레이 IP 주소에서 트래픽이 수신됩니다.</para>
<screen># aws ec2 modify-instance-attribute --profile cluster --instance-id EC2-instance --no-source-dest-check</screen>
<para>이 명령이 실행되는 시스템에는 다음 정책이 포함된 역할이 일시적으로 필요합니다.</para>
<screen>{
   "Version": "2012-10-17",
   "Statement": [
   {
      "Sid": "Stmt1424870324000",
      "Effect": "Allow",
      "Action": [ "ec2:ModifyInstanceAttribute"],
      "Resource": [
      "arn:aws:ec2:region-name:account-id:instance/instance-a",
      "arn:aws:ec2:region-name:account-id:instance/instance-b"
      ]
   }
   ]
}</screen>
<para>EC2 인스턴스용 리전, 계정 식별자 및 2개 식별자에 대한 개별 파라미터를 적절한 값으로 변경합니다.</para>
<para>소스/대상 확인은 AWS 콘솔에서도 비활성화할 수 있습니다. 두 EC2 인스턴스 모두에 대해 콘솔에서 다음과 같은 드롭다운 박스를 실행해야 합니다(아래 참조).</para>
<figure>
<title>콘솔에서 소스/대상 확인 비활성화</title>
<mediaobject>
<imageobject>
<imagedata fileref="SourceDestinationCheck.png" width=""/>
</imageobject>
<textobject><phrase>PNG</phrase></textobject>
</mediaobject>
</figure>
</section>
<section xml:id="id-avoid-deletion-of-cluster-managed-ip-address-on-the-eth0-interface">
<title>eth0 인터페이스에서 클러스터 관리 IP 주소 삭제 방지</title>
<para>SUSE Linux Enterprise Server 12 SP3는 cloud-netconfig 패키지를 제공하는 최초의 버전입니다. 이 패키지는 eth0 인터페이스에서 클러스터 에이전트가 관리하는 보조 IP 주소를 제거합니다. 이로 인해 HA 서비스 사용자에게 서비스 중단이 발생할 수 있습니다. 모든 클러스터 노드에서 아래 작업을 수행하십시오.</para>
<para>다음 명령으로 cloud-netconfig-ec2 패키지가 설치되어 있는지 확인합니다.</para>
<screen># zypper info cloud-netconfig-ec2</screen>
<para>이 패키지가 설치된 경우 <emphasis>/etc/sysconfig/network/ifcfg-eth0</emphasis> 파일을 업데이트합니다. 다음 라인을 “no” 설정으로 변경하거나 패키지를 아직 설치하지 않은 경우에는 라인을 추가합니다.</para>
<screen>CLOUD_NETCONFIG_MANAGE='no'</screen>
</section>
<section xml:id="id-aws-roles-and-policies">
<title>AWS 역할 및 정책</title>
<para>SAP ASCS 및 ESR은 SUSE Linux Enterprise Server Pacemaker 소프트웨어와 에이전트를 실행합니다. 이 소프트웨어로 클러스터를 작동하기 위해서는 여러 AWS IAM 권한이 필요합니다. 모든 ASCS/ESR 클러스터에서 사용할 새 역할을 만들고 이 역할을 두 인스턴스에 연결합니다. 아래 설명된 정책을 이 역할에 연결합니다.</para>
<section xml:id="id-aws-data-provider-policy">
<title>AWS 데이터 공급자 정책</title>
<para>모든 클러스터 노드가 SAP 시스템에서 작동합니다. AWS 기반 SAP 시스템에는 “AWS Data Provider for SAP”를 설치해야 합니다. 데이터 공급자는 AWS 리소스에 액세스하기 위한 정책이 필요합니다. “AWS Data Provider for SAP 설치 및 운영 가이드”의 “IAM 역할” 섹션에서 설명하는 정책을 사용하고 이를 인스턴스의 역할에 연결합니다. 이 정책은 모든 SAP 시스템에서 사용할 수 있습니다. 한 AWS 계정에서는 정책을 1개만 사용합니다. 정책에는 인스턴스별 권한이 포함되지 않습니다.</para>
<screen>{
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "EC2:DescribeInstances",
                "EC2:DescribeVolumes"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": "cloudwatch:GetMetricStatistics",
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::aws-data-provider/config.properties"
        }
    ]
}</screen>
</section>
<section xml:id="id-stonith-policy">
<title>STONITH 정책</title>
<para>SUSE 클러스터의 인스턴스에는 클러스터 내의 다른 노드를 시작 및 정지하기 위한 권한이 필요합니다. 다음 내용이 포함된 <emphasis>stonith-policy</emphasis> 같은 이름으로 정책을 만들고 클러스터 역할에 연결합니다.</para>
<screen>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "Stmt1424870324000",
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeInstances",
                "ec2:DescribeInstanceAttribute",
                "ec2:DescribeTags"
            ],
            "Resource": "*"
        },
        {
            "Sid": "Stmt1424870324001",
            "Effect": "Allow",
            "Action": [
                "ec2:ModifyInstanceAttribute",
                "ec2:RebootInstances",
                "ec2:StartInstances",
                "ec2:StopInstances"
            ],
            "Resource": [
                "arn:aws:ec2:region-name:aws-account:instance/i-node1",
                "arn:aws:ec2:region-name:aws-account:instance/i-node2"
            ]
        }
    ]
}</screen>
<para><emphasis>aws-account</emphasis> 변수를 적절한 AWS 계정 식별자로 바꿉니다. <emphasis>i-node1</emphasis> 및 <emphasis>i-node2</emphasis> 변수를 (hacert01) 및 (hacert02) 클러스터 노드 2개의 AWS 인스턴스 ID로 바꿉니다. <emphasis>region-name</emphasis> 변수를 사용 중인 AWS 리전의 이름(예: 노스 버지니아 리전의 경우 us-east-1)으로 바꿉니다. 이 정책은 클러스터의 인스턴스에 따라 다릅니다. 각 클러스터에 별도의 정책을 만들어야 합니다!</para>
</section>
</section>
<section xml:id="id-overlay-ip-agent-policy">
<title>오버레이 IP 에이전트 정책</title>
<para>오버레이 IP 에이전트는 AWS 라우팅 테이블의 라우팅 항목을 변경합니다. 다음과 같이 <emphasis>Manage-Overlay-IP-Policy</emphasis>와 같은 이름의 정책을 생성하여 클러스터 인스턴스의 역할에 연결합니다.</para>
<screen>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": "ec2:ReplaceRoute",
            "Resource": "arn:aws:ec2:region-name:account-id:route-table/rtb-XYZ"
        },
        {
            "Sid": "VisualEditor1",
            "Effect": "Allow",
            "Action": "ec2:DescribeRouteTables",
            "Resource": "*"
        }
    ]
}</screen>
<para>이 정책에서는 사용되는 라우팅 테이블을 에이전트가 업데이트하도록 허용합니다. 다음 변수를 적절한 이름으로 교체하십시오.</para>
<itemizedlist>
<listitem>
<para>region-name : 해당 AWS 리전의 이름</para>
</listitem>
<listitem>
<para>account-id : 정책이 사용되고 있는 AWS 계정의 이름</para>
</listitem>
<listitem>
<para>rtb-XYZ : 업데이트되어야 하는 라우팅 테이블의 식별자</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-route-53-updates">
<title>Route 53 업데이트</title>
<para>선택 사항으로 Route 53 에이전트를 클러스터에 설치할 수 있습니다. 다음 정책은 Route 53 에이전트를 사용할 때만 필요합니다. 이름이 <emphasis>Route53-Update</emphasis>인 정책을 만들고 두 클러스터 노드의 역할에 연결합니다.</para>
<screen>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "Stmt1471878724000",
            "Effect": "Allow",
            "Action": "route53:GetChange",
            "Resource": "arn:aws:route53:::change/*"
        },
        {
            "Sid": "Stmt1471878724001",
            "Effect": "Allow",
            "Action": "route53:ChangeResourceRecordSets",
            "Resource": "arn:aws:route53:::hostedzone/hosted zone ID/full name"
        },
        {
            "Sid": "Stmt1471878724002",
            "Effect": "Allow",
            "Action": [
                "route53:ListResourceRecordSets",
                "route53:ChangeResourceRecordSets"
            ],
            "Resource": "arn:aws:route53:::hostedzone/hosted zone ID"
        }
    ]
}</screen>
<para>이 정책은 호스팅된 영역 및 리소스 레코드 세트에 고유합니다. <emphasis>hosted zone ID</emphasis> 변수를 호스팅된 영역의 AWS ID로 바꿉니다. <emphasis>full name</emphasis>을 항목의 이름으로 바꿉니다. 개별 Route 53 정책을 모든 클러스터에 만들어야 합니다.</para>
</section>
</section>
<section xml:id="id-add-overlay-ip-addresses-to-routing-table">
<title>라우팅 테이블에 오버레이 IP 주소 추가</title>
<para>서브넷 2개에 지정된 라우팅 테이블에 라우팅 항목 2개를 수동으로 추가합니다. IP 주소는 VPC의 CIDR 범위 밖에 있어야 합니다. AWS 콘솔을 사용해 “VPC”를 검색하십시오.</para>
<itemizedlist>
<listitem>
<para>VPC 선택</para>
</listitem>
<listitem>
<para>왼쪽 열에서 “[라우팅 테이블]”을 클릭</para>
</listitem>
<listitem>
<para>SAP ASCS 서브넷에서 사용할 라우팅 테이블 선택</para>
</listitem>
<listitem>
<para>“경로” 탭을 클릭</para>
</listitem>
<listitem>
<para>“[편집]”을 클릭</para>
</listitem>
<listitem>
<para>목록 끝으로 스크롤하여 “[다른 경로 추가]”를 클릭</para>
</listitem>
</itemizedlist>
<section xml:id="id-add-the-service-ip-address-for-your-ascs-service">
<title>ASCS 서비스의 서비스 IP 주소 추가</title>
<para>ASCS 서비스(hacert01 노드)의 서비스 IP 주소를 추가합니다. /32를 필터로 사용하십시오(예: 192.168.10.1/32). 처음에 ASCS로 서비스를 제공하는 인스턴스의 ENI(Elastic Network Interface) 이름을 추가합니다. “[저장]”을 클릭하여 변경 사항을 저장합니다.</para>
<para>이는 이름이 sapha1as인 서비스 IP 주소입니다.</para>
</section>
<section xml:id="id-add-the-service-ip-address-for-your-ers-service">
<title>ERS 서비스의 서비스 IP 주소 추가</title>
<para>ERS 서비스(hacert02 노드)의 서비스 IP 주소를 추가합니다. /32를 필터로 사용하십시오(예: 192.168.10.2/32). 초기에 ERS로 기능하는 인스턴스의 ENI(Elastic Network Interface) 이름을 추가합니다. “[저장]”을 클릭하여 변경 사항을 저장합니다.</para>
<para>이는 이름이 sapha1er인 IP 주소입니다.</para>
</section>
</section>
<section xml:id="id-efs-file-system">
<title>EFS 파일 시스템</title>
<para>클러스터에는 AWS EFS(Elastic File System)에서 제공하는 NFS 파일 시스템이 필요합니다. 파일 시스템이 관리하는 항목은 다음과 같습니다.</para>
<itemizedlist>
<listitem>
<para>ASCS00, ERS10, D02, DVEBMGS01 및 기타 애플리케이션 서버와 SYS 디렉토리를 위한 /usr/sap/HA1 데이터</para>
</listitem>
<listitem>
<para>/sapmnt</para>
</listitem>
</itemizedlist>
<para>VPC의 식별자 및 두 클러스터 노드를 운영할 서브넷의 서브넷 식별자가 필요합니다. 다른 서브넷을 선택할 수도 있습니다. 이러한 서브넷은 두 클러스터 노드로 통신할 수 있고 고가용성을 위해 동일한 가용 영역(AZ)에 있어야 합니다. “일반 용도” 옵션으로 충분합니다.</para>
<para>사용 중인 EFS 서버의 DNS 이름을 적어 둡니다. AWS 네임 서비스는 이 DNS 이름을 가용 영역에 있는 IP 주소의 VPC에 내부적으로 확인합니다. 파일 시스템을 마운트해야 하는 경우 이 이름을 “efs-name”으로 부릅니다.</para>
<para>향후 2개의 마운트 지점(/usr/sap/HA1/ASCS00, /usr/SAP/HA1/ESR10)에 파일 시스템 1개를 사용합니다. 이렇게 하면 편리하게 관리하고 처리량을 향상할 수 있습니다. EFS에서의 AWS 처리량은 파일 시스템의 총 크기를 기반으로 합니다.</para>
<para>두 클러스터 노드 중 하나에 로그인한 후 임시 마운트를 통해 EFS 파일 시스템에 디렉토리를 여러 개 만듭니다. <emphasis>root</emphasis> 사용자로 다음 명령을 실행합니다.</para>
<screen># mount efs-name: /mnt
# mkdir -p /mnt/ASCS00 /mnt/ERS10 /mnt/D01 /mnt/DVEBMGS01 /mnt/D02 /mnt/SYS /mnt/sapmnt /mnt/sapcd
# umount /mnt</screen>
<para>다른 애플리케이션 서버를 위한 추가 디렉토리를 만듭니다. 이 NFS 파일 시스템은 모든 디렉토리에서 사용됩니다.</para>
<para>클러스터 노드에 마운트 지점 2개를 마운트합니다. 두 클러스터 노드에서 루트로 다음 명령을 실행합니다.</para>
<screen># mkdir -p /sapmnt /usr/sap/HA1/SYS</screen>
<para>SAP ASCS 및 ERS 서비스를 실행할 두 인스턴스에서 다음 두 라인을 /etc/fstab 파일에 추가합니다.</para>
<screen>efs-name:SYS     /usr/sap/HA1/SYS    nfs4       rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2    0 0
efs-name:sapmnt  /sapmnt             nfs4       rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2    0 0</screen>
<para><emphasis>efs-name</emphasis>을 해당 DNS 이름으로 바꿉니다.</para>
<para>다음 명령을 사용하여 루트로 파일 시스템을 마운트합니다.</para>
<screen># mount /usr/sap/HA1/SYS
# mount /sapmnt</screen>
<section xml:id="id-enable-cluster-instances-to-use-the-overlay-ip-address">
<title>클러스터 인스턴스를 활성화하여 오버레이 IP 주소 사용</title>
<para>두 클러스터 인스턴스에는 표준 인터페이스 <emphasis>eth0</emphasis>에서 보조 IP 주소로 구성할 오버레이 IP 주소가 필요합니다. 이 작업은 다음 명령으로 수행할 수 있습니다.</para>
<screen># ip address add OVERLAY-IP dev eth0</screen>
<para>루트 권한을 사용하여 두 인스턴스에서 다음 명령을 실행합니다. ASCS 노드 hacert01에 ASCS IP 주소를 추가합니다. ERS 노드 hacert02에 인큐 복제 주소를 추가합니다.</para>
</section>
</section>
<section xml:id="id-differences-to-previous-cluster-architecture">
<title>이전 클러스터 아키텍처와의 차이점</title>
<para>마스터-슬레이브 아키텍처를 사용하는 기존 스택과는 개념이 다릅니다. 새로운 인증을 통해 기본이 포함된 더 단순한 모델로 전환합니다. 즉, 한 시스템에는 자체 리소스가 포함된 ASCS가 위치하고 다른 시스템에는 자체 리소스가 포함된 ERS가 위치합니다.</para>
</section>
<section xml:id="id-five-systems-for-ascs-ers-database-and-additional-sap-instances">
<title>ASCS, ERS, 데이터베이스 및 추가 SAP 인스턴스를 위한 시스템 5개</title>
<para>이 가이드는 SAP 분산 시스템을 5개 시스템에 설치하는 방법을 설명합니다. 이 설정에서는 클러스터 내에 2개의 시스템만 위치합니다. 클러스터에 노드 3개를 추가하거나 기존 노드 중 하나에 데이터베이스를 설치하여 데이터베이스 및 SAP dialog 인스턴스도 클러스터에 추가할 수 있습니다. 그러나 데이터베이스는 별도의 클러스터에 설치하는 것이 좋습니다.</para>
<note>
<para>이 가이드에서는 SAP NW-HA-CLU 7.40 인증에 집중하므로 SAP 인스턴스 ASCS 및 ERS만 클러스터에서 관리됩니다.</para>
</note>
<para>사용 중인 데이터베이스가 SAP HANA인 경우에는 자동화 솔루션인 SAPHanaSR을 사용하여 성능 최적화 시스템 복제 시나리오를 설정하는 것이 좋습니다. SAPHanaSR 자동화는 자체 2 노드 클러스터에 설정되어야 합니다. 이 설정에 대한 설명은 SUSE 모범 사례 페이지( <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://www.suse.com/products/sles-for-sap/resource-library/sap-best-practices/">https://www.suse.com/products/sles-for-sap/resource-library/sap-best-practices/</link>)의 별도의 모범 사례에서 제공됩니다.</para>
<figure>
<title>인증 설정을 위한 5개 시스템</title>
<mediaobject>
<imageobject>
<imagedata fileref="sles4sap_nw740_5nodes.svg" width=""/>
</imageobject>
<textobject><phrase>SVG</phrase></textobject>
</mediaobject>
</figure>
<itemizedlist>
<title>클러스터 시스템</title>
<listitem>
<para>ASCS용 시스템 1개(hacert01), 호스트 이름: sapha1as</para>
</listitem>
<listitem>
<para>ERS용 시스템 1개(hacert02), 호스트 이름: sapha1er</para>
</listitem>
</itemizedlist>
<itemizedlist>
<title>비클러스터 시스템</title>
<listitem>
<para>DB용 시스템 1개, 호스트 이름: sapha1db</para>
</listitem>
<listitem>
<para>PAS용 시스템 1개, 호스트 이름: sapha1ci</para>
</listitem>
<listitem>
<para>AAS용 시스템 1개, 호스트 이름: sapha1d2</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-high-availability-for-the-database">
<title>데이터베이스 고가용성</title>
<para>데이터베이스가 고가용성을 지원하도록 설계되지 않은 경우에는 필요에 따라 데이터베이스의 가용성을 늘릴 수 있습니다.</para>
<section xml:id="id-sap-hana-system-replication">
<title>SAP HANA 시스템 복제</title>
<para>이 문서에서 설명되는 5 노드 시나리오의 완벽한 향상은 SAP HANA 시스템 복제(SR) 자동화를 구현하는 것입니다.</para>
<figure>
<title>중앙 서비스용 클러스터 1개, SAP HANA SR용 1개</title>
<mediaobject>
<imageobject>
<imagedata fileref="sles4sap_awsnw740_cs+hanasr.svg" width=""/>
</imageobject>
<textobject><phrase>SVG</phrase></textobject>
</mediaobject>
</figure>
<para>이 시나리오에서 지원되는 데이터베이스는 다음과 같습니다.</para>
<itemizedlist>
<listitem>
<para>SAP HANA DATABASE 1.0</para>
</listitem>
<listitem>
<para>SAP HANA DATABASE 2.0</para>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="id-integration-of-sap-netweaver-into-the-cluster-using-the-cluster-connector">
<title>클러스터 커넥터를 사용하여 SAP NetWeaver를 클러스터에 통합</title>
<para>sap-suse-cluster-connector를 사용하여 SAP 컨트롤 프레임워크를 통해 HA 클러스터를 통합하는 것은 특별한 관심사입니다. SAP Kernel versions 6.40 이상부터는 sapstartsrv가 SAP 인스턴스를 제어합니다. 고가용성 환경에서 SAP 인스턴스를 실행할 때 나타나는 한 가지 전형적인 문제는 SAP 관리자가 클러스터 소프트웨어에서 제공되는 인터페이스를 사용하지 않고 SAP 인스턴스의 상태(시작/중지)를 변경할 경우 클러스터 프레임워크가 이를 오류 상태로 인식하고 SAP 인스턴스를 시작하거나 중지하여 SAP 인스턴스를 기존 상태로 전환하는 것입니다. 이로 인해 SAP 유지보수 작업 중에 클러스터가 SAP 인스턴스의 상태를 변경하면 매우 위험한 상황이 발생할 수 있습니다. 새로 업데이트된 이 솔루션을 사용하면 중앙 구성 요소인 sapstartsrv가 클러스터 소프트웨어에 상태 변경을 보고하므로 이전에 설명한 위험한 상황을 방지할 수 있습니다. (<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://blogs.sap.com/2014/05/08/using-sapvendorclusterconnector-for-interaction-between-cluster-framework-and-sapstartsrv/comment-page-1/">https://blogs.sap.com/2014/05/08/using-sapvendorclusterconnector-for-interaction-between-cluster-framework-and-sapstartsrv/comment-page-1/</link>의 “클러스터 프레임워크와 sapstartsrv 의 상호 작용에 sap_vendor_cluster_connector 사용” 문서도 참조).</para>
<figure>
<title>클러스터와 SAP 시작 프레임워크를 통합하기 위한 클러스터 커넥터</title>
<mediaobject>
<imageobject>
<imagedata fileref="sles4sap_clusterconnector.svg" width=""/>
</imageobject>
<textobject><phrase>SVG</phrase></textobject>
</mediaobject>
</figure>
<note>
<para>이 시나리오에서는 클러스터 프레임워크와 sapstartsrv 간 통신을 위해 API 버전 3을 구현하는 업데이트된 sap-suse-cluster-connector 버전을 사용합니다.</para>
</note>
<para>sap-suse-cluster-connector 새 버전을 사용하면 SAP 인스턴스를 시작, 중지 및 ‘마이그레이션’할 수 있습니다. 클러스터 소프트웨어와 sapstartsrv를 통합하면 명령줄 도구 sapcontrol 또는 SAP 관리 콘솔(SAP MMC 또는 SAP MC)을 사용하여 HA 설정 확인을 실행하도록 있도록 구현할 수도 있습니다.</para>
</section>
<section xml:id="id-disks-and-partitions">
<title>디스크 및 파티션</title>
<para>모든 SAP 파일 시스템에는 EFS 파일 시스템뿐만 아니라 XFS를 사용합니다.</para>
<section xml:id="id-efs-file-systems-for-cluster-ascs-and-ers">
<title>클러스터 ASCS 및 ERS를 위한 EFS 파일 시스템</title>
<para>두 클러스터 노드에서 루트로 다음 하위 디렉토리를 만듭니다.</para>
<screen># mkdir -p /usr/sap/HA1/ASCS00 /usr/sap/HA1/ERS10</screen>
<para>ASCS 및 ERS 인스턴스를 위한 파일 시스템은 클러스터 노드 hacert01 및 hacert02에 공유 및 할당되어야 합니다. EFS 파일 시스템을 만듭니다.</para>
<para>SAP를 설치하는 중에 hacert01에 <emphasis>/usr/sap/HA1/ASCS00</emphasis> 파일 시스템을, hacert02에 <emphasis>/usr/sap/HA1/ERS10</emphasis>을 마운트해야 합니다.</para>
<screen>hacert01: efs-name:/ASCS00 /usr/sap/HA1/ASCS00
hacert02: efs-name:/ERS10 /usr/sap/HA1/ERS10</screen>
<para><emphasis>efs-name</emphasis> 변수를 EFS 파일 시스템의 해당 DNS 이름으로 바꿉니다.</para>
<note>
<para>hacert01과 hacert02는 서로 다른 가용 영역에서 작동하며, 이름이 “efs-name”이고 가용 영역에 고유한 마운트 지점이 필요합니다. AWS에서 제공하는 DNS 이름을 사용하십시오. DNS 이름은 지정된 가용 영역에 로컬인 파일 시스템 마운트 지점을 가리킵니다. SAP를 설치하는 중에 hacert01에 /usr/sap/HA1/ASCS00을, hacert02에 /usr/sap/HA1/ERS10을 마운트해야 합니다.</para>
</note>
</section>
</section>
<section xml:id="id-ip-addresses-and-virtual-names">
<title>IP 주소 및 가상 이름</title>
<para><emphasis>/etc/hosts</emphasis> 파일에 다음 주소 확인 중 하나 이상이 포함되었는지 확인하십시오. 없는 경우 해당 항목을 추가하십시오. 아래 예시에서 10.0.0.0 주소는 VPC CIDR 블록의 기본 IP 주소입니다. 192.168.201.0 주소는 가상 서비스의 오버레이 IP 주소입니다. 아래 목록은 데이터베이스 서버용 가상 IP 주소입니다. 가상 데이터베이스 서버 주소에 대해 SAP 시스템을 설치하면 데이터베이스 서버를 나중에 보호 클러스터 서비스로 업그레이드할 수 있습니다.</para>
<screen>10.0.0.111  hacert01
10.0.0.112  hacert02
10.0.0.113  hacert03
10.0.0.114  sapha1ci
10.0.0.115  sapha1d2
192.168.201.116  sapha1as
192.168.201.117  sapha1er
192.168.201.118  sapha1db</screen>
</section>
<section xml:id="id-mount-points-and-nfs-shares">
<title>마운트 지점 및 NFS 공유</title>
<para>현재 설정에서는 <emphasis>/usr/sap</emphasis> 디렉토리가 루트 파일 시스템의 일부입니다. 물론, 해당 영역을 위한 전용 파일 시스템을 만들고 시스템 부팅 중에 <emphasis>/usr/sap</emphasis>를 마운트할 수도 있습니다. 또한, <emphasis>/usr/sap</emphasis>에는 SAP 제어 파일 <emphasis>sapservices</emphasis> 및 saphostagent가 있으므로, 해당 디렉토리는 클러스터 노드 간 공유 파일 시스템에 위치하지 않아야 합니다.</para>
<para>SAP 리소스를 실행할 수 있는 모든 노드에 디렉토리 구조를 만들어야 합니다. SYS 디렉토리는 모든 노드에 대한 NFS 공유에 있게 됩니다.</para>
<itemizedlist>
<listitem>
<para>마운트 지점을 생성하고 모든 노드에 NFS 공유를 마운트합니다.</para>
</listitem>
<listitem>
<para><emphasis>efs-name</emphasis>을 해당 DNS 이름으로 바꿉니다.</para>
</listitem>
</itemizedlist>
<example>
<title>SAP NetWeaver 7.4</title>
<screen># mkdir -p /sapmnt
# mkdir -p /usr/sap/HA1/{ASCS00,D02,DVEBMGS01,ERS10,SYS}
# mount -t nfs efs-name:/sapmnt /sapmnt
# mount -t nfs efs-name:/SYS    /usr/sap/HA1/SYS
# mount -t nfs efs-name:/sapcd /sapcd</screen>
</example>
<example>
<title>SAP NetWeaver 7.5</title>
<screen># mkdir -p /sapmnt
# mkdir -p /usr/sap/HA1/{ASCS00,D01,D02,ERS10,SYS}
# mount -t nfs efs-name:/sapmnt    /sapmnt
# mount -t nfs efs-name:/SYS /usr/sap/HA1/SYS
# mount -t nfs efs-name:/sapcd /sapcd</screen>
</example>
<itemizedlist>
<listitem>
<para>HANA만 해당: hacert03에 데이터베이스용 마운트 지점 만들기:</para>
</listitem>
</itemizedlist>
<screen># mkdir -p /hana/{shared,data,log}</screen>
<figure>
<title>NFS 공유 포함 파일 시스템 레이아웃</title>
<mediaobject>
<imageobject>
<imagedata fileref="sles4sap_nw740_5nodes.svg" width=""/>
</imageobject>
<textobject><phrase>SVG</phrase></textobject>
</mediaobject>
</figure>
<para>SAP 분산 설치용 서버 3개가 다음과 같이 준비되었습니다.</para>
<itemizedlist>
<listitem>
<para>서버 1(hacert01)은 ASCS SAP 인스턴스를 설치하기 위해 사용됩니다.</para>
</listitem>
<listitem>
<para>서버 2(hacert02)는 ERS SAP 인스턴스를 설치하기 위해 사용됩니다.</para>
</listitem>
<listitem>
<para>서버 3(hacert03)은 데이터베이스를 설치하기 위해 사용됩니다.</para>
</listitem>
<listitem>
<para>서버 4(sapha1ci)는 PAS SAP 인스턴스를 설치하기 위해 사용됩니다.</para>
</listitem>
<listitem>
<para>서버 5(sapha1d2)는 AAS SAP 인스턴스를 설치하기 위해 사용됩니다.</para>
</listitem>
<listitem>
<para>인스턴스 및 데이터베이스 파일 시스템을 특정 노드 1개에 마운트</para>
</listitem>
</itemizedlist>
<itemizedlist>
<listitem>
<para>그 결과, <emphasis>/usr/sap/HA1/</emphasis> 디렉토리는 다음과 같아야 합니다.</para>
</listitem>
</itemizedlist>
<screen># ls -l /usr/sap/HA1/
total 0
drwxr-xr-x 1 ha1adm sapsys 70 28. Mär 17:26 ./
drwxr-xr-x 1 root   sapsys 58 28. Mär 16:49 ../
drwxr-xr-x 7 ha1adm sapsys 58 28. Mär 16:49 ASCS00/
drwxr-xr-x 1 ha1adm sapsys  0 28. Mär 15:59 D02/
drwxr-xr-x 1 ha1adm sapsys  0 28. Mär 15:59 DVEBMGS01/
drwxr-xr-x 1 ha1adm sapsys  0 28. Mär 15:59 ERS10/
drwxr-xr-x 5 ha1adm sapsys 87 28. Mär 17:21 SYS/</screen>
<note>
<para>디렉토리 및 파일의 소유자는 SAP 설치 중에 변경됩니다. 기본적으로 모든 디렉토리 및 파일은 루트 소유입니다.</para>
</note>
</section>
</section>
<section xml:id="id-testing-the-aws-agents">
<title>AWS 에이전트 테스트</title>
<para>AWS 에이전트를 테스트한 후 클러스터를 시작하십시오. 테스트는 AWS 역할 및 정책이 올바르게 구성되었는지 여부를 보여줍니다. 테스트에서는 AWS CLI 오류가 발생하지 않아야 합니다.</para>
<section xml:id="id-testing-the-overlay-ip-agents">
<title>오버레이 IP 에이전트 테스트</title>
<para>명령에서 다음 변수를 바꿉니다.</para>
<itemizedlist>
<listitem>
<para><emphasis>ip_address</emphasis>: ASCS 및 ERS 시스템의 서비스 IP 주소입니다.</para>
</listitem>
<listitem>
<para><emphasis>rtb-table</emphasis> : 오버레이 IP 주소의 AWS 라우팅 테이블 이름입니다.</para>
</listitem>
<listitem>
<para><emphasis>cluster</emphasis> : 필요한 경우 AWS CLI 프로파일 이름을 바꿉니다.</para>
</listitem>
</itemizedlist>
<para>변수는 나중에 OCF 기본의 변수와 일치해야 합니다!</para>
<para>두 시스템에서 루트로 다음 명령을 실행합니다.</para>
<screen>OCF_RESKEY_address=ip_address \
OCF_RESKEY_routing_table=rtb-table \
OCF_RESKEY_interface=eth0 OCF_ROOT=/usr/lib/ocf OCF_RESKEY_profile=cluster \
/usr/lib/ocf/resource.d/suse/aws-vpc-move-ip start

OCF_RESKEY_address=ip_address \
OCF_RESKEY_routing_table=rtb-table \
OCF_RESKEY_interface=eth0 OCF_ROOT=/usr/lib/ocf OCF_RESKEY_profile=cluster \
/usr/lib/ocf/resource.d/suse/aws-vpc-move-ip monitor

OCF_RESKEY_address=ip_address \
OCF_RESKEY_routing_table=rtb-table \
OCF_RESKEY_interface=eth0 OCF_ROOT=/usr/lib/ocf OCF_RESKEY_profile=cluster \
/usr/lib/ocf/resource.d/suse/aws-vpc-move-ip stop</screen>
<para>AWS CLI 액세스 문제를 확인하고 AWS 정책을 수정합니다. AWS 콘솔을 사용하여 <emphasis>start</emphasis> 이후에 IP 주소가 추가되었는지 확인합니다. AWS 콘솔을 사용하여 <emphasis>stop</emphasis> 이후에 IP 주소가 제거되었는지 확인합니다. 다른 클러스터 노드를 사용하여 몇 가지 액세스 명령(ping, SSH 등)을 실행합니다. 다시 확인한 후 이를 통해 해결이 되지 않으면 모든 네트워크 관련 설정을 수정합니다.</para>
</section>
<section xml:id="id-testing-agents-mounting-efs-file-sytems">
<title>EFS 파일 시스템 마운트 에이전트 테스트</title>
<para>우선 모니터링 기능을 테스트합니다. 명령에서 다음 변수를 바꿉니다.</para>
<itemizedlist>
<listitem>
<para><emphasis>efs-name</emphasis>: EFS 파일 시스템의 이름</para>
</listitem>
</itemizedlist>
<para>두 클러스터 노드에서 루트로 다음 명령을 실행합니다.</para>
<screen>OCF_RESKEY_device="efs-name:ASCS00" \
OCF_RESKEY_directory="/usr/sap/HA1/ASCS00" OCF_RESKEY_fstype=nfs4 \
OCF_RESKEY_options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
OCF_ROOT=/usr/lib/ocf /usr/lib/ocf/resource.d/heartbeat/Filesystem start

OCF_RESKEY_device="efs-name:ERS10" \
OCF_RESKEY_directory="/usr/sap/HA1/ERS10" OCF_RESKEY_fstype=nfs4 \
OCF_RESKEY_options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
OCF_ROOT=/usr/lib/ocf /usr/lib/ocf/resource.d/heartbeat/Filesystem start

df -k

OCF_RESKEY_device="efs-name:ASCS00" \
OCF_RESKEY_directory="/usr/sap/HA1/ASCS00" OCF_RESKEY_fstype=nfs4 \
OCF_RESKEY_options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
OCF_ROOT=/usr/lib/ocf /usr/lib/ocf/resource.d/heartbeat/Filesystem stop

OCF_RESKEY_device="efs-name:ERS10" \
OCF_RESKEY_directory="/usr/sap/HA1/ERS10" OCF_RESKEY_fstype=nfs4 \
OCF_RESKEY_options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
OCF_ROOT=/usr/lib/ocf /usr/lib/ocf/resource.d/heartbeat/Filesystem stop

df -k</screen>
<para><emphasis>df -k</emphasis> 명령으로 파일 시스템이 마운트 및 마운트 해제되었는지 확인합니다. 잘못된 <emphasis>efs-name</emphasis> 명령을 사용하거나 하위 디렉토리가 없으면 오류가 발생할 수 있습니다.</para>
</section>
<section xml:id="id-optional-testing-route-53-agents">
<title>선택 사항: Route 53 에이전트 테스트</title>
<para>Route 53 에이전트를 사용하는 경우에는 이 테스트를 수행해야 합니다. 우선 모니터링 기능을 테스트합니다. 명령에서 다음 변수를 바꿉니다.</para>
<itemizedlist>
<listitem>
<para><emphasis>hosted zone id</emphasis>: 호스팅된 프라이빗 영역의 ID입니다.</para>
</listitem>
<listitem>
<para><emphasis>fullname</emphasis> : 하위 도메인과 마지막 점을 포함하여 서비스 이름의 전체 이름입니다.</para>
</listitem>
<listitem>
<para><emphasis>cluster</emphasis> : 필요한 경우 AWS CLI 프로파일 이름을 바꿉니다.</para>
</listitem>
</itemizedlist>
<para>변수는 나중에 OCF 기본의 변수와 일치해야 합니다!</para>
<para>두 시스템에서 루트로 다음 명령을 실행합니다.</para>
<screen>OCF_RESKEY_hostedzoneid=hosted zone id OCF_RESKEY_ttl=10 \
    OCF_RESKEY_fullname=fullname OCF_ROOT=/usr/lib/ocf \
    OCF_RESKEY_profile=cluster \
    /usr/lib/ocf/resource.d/heartbeat/aws-vpc-route53 monitor

OCF_RESKEY_hostedzoneid=hosted zone id OCF_RESKEY_ttl=10 \
    OCF_RESKEY_fullname=fullname OCF_ROOT=/usr/lib/ocf \
    OCF_RESKEY_profile=cluster \
    /usr/lib/ocf/resource.d/heartbeat/aws-vpc-route53 start

OCF_RESKEY_hostedzoneid=hosted zone id OCF_RESKEY_ttl=10 \
  OCF_RESKEY_fullname=fullname OCF_ROOT=/usr/lib/ocf \
  OCF_RESKEY_profile=cluster \
  /usr/lib/ocf/resource.d/heartbeat/aws-vpc-route53 stop</screen>
<para>우선 모니터링에서 모든 문제를 수정합니다. 두 번째 테스트로 시작하고 마지막 테스트로 중지하십시오.</para>
</section>
</section>
<section xml:id="id-sap-installation">
<title>SAP 설치</title>
<para>SAP 분산 시스템을 설치하기 위한 전체 절차는 다음과 같습니다.</para>
<itemizedlist>
<listitem>
<para>중앙 서비스용 ASCS 인스턴스 설치</para>
</listitem>
<listitem>
<para>복제된 인큐 시나리오를 가져오기 위해 ERS 설치</para>
</listitem>
<listitem>
<para>클러스터 인수를 위한 ASCS 및 ERS 설치 준비</para>
</listitem>
<listitem>
<para>데이터베이스 설치</para>
</listitem>
<listitem>
<para>기본 애플리케이션 서버 인스턴스(PAS) 설치</para>
</listitem>
<listitem>
<para>추가 애플리케이션 서버 인스턴스(AAS) 설치</para>
</listitem>
</itemizedlist>
<para>결과적으로 SAP 분산 설치는 다음과 같습니다.</para>
<figure>
<title>SAP 시스템 분산 설치</title>
<mediaobject>
<imageobject>
<imagedata fileref="sles4sap_awsnw740_distInstall.svg" width=""/>
</imageobject>
<textobject><phrase>SVG</phrase></textobject>
</mediaobject>
</figure>
<section xml:id="id-linux-user-and-group-number-scheme">
<title>Linux 사용자 및 그룹 번호 체계</title>
<para>SAP 소프트웨어 프로비저닝 관리자(SWPM)가 사용할 Linux 사용자 ID 또는 그룹 ID를 입력하라고 요청할 때마다 다음 테이블을 예시로 참조하십시오.</para>
<screen>Group sapinst      1000
Group sapsys       1001
Group sapadm       3000
Group sdba         3002

User  ha1adm       3000
User  sdb          3002
User  sqdha1       3003
User  sapadm       3004
User  h04adm       4001</screen>
</section>
<section xml:id="id-installing-ascs-on-hacert01">
<title>hacert01에 ASCS 설치</title>
<para>설치 프로그램에서 확인하거나 사용해야 하므로 나중에 클러스터에서 사용할 서비스 IP 주소를 로컬 IP로 임시로 설정하십시오. 각 설치 단계에서 올바른 가상 호스트 이름을 사용하는지 확인합니다. efs-name:/ASCS00 및 /sapcd/와 같은 파일 시스템을 마운트해야 할 수도 있으므로 유의하십시오.</para>
<screen># ip address add 192.168.201.116/32 dev eth0
# mount efs-name:/ASCS00 /usr/sap/HA1/ASCS00
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME=sapha1as</screen>
<itemizedlist>
<listitem>
<para>SWPM 옵션은 SAP NetWeaver 버전 및 아키텍처에 따라 다릅니다.</para>
<itemizedlist>
<listitem>
<para>설치: SAP NetWeaver 7.40 SR2 → MaxDB → SAP-Systems → Application Server ABAP → 고가용성 시스템 → ASCS 인스턴스</para>
</listitem>
<listitem>
<para>설치: SAP NetWeaver 7.5 → SAP HANA 데이터베이스 → 설치 → Application Server ABAP → 고가용성 시스템 → ASCS 인스턴스</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>SID id HA1</para>
</listitem>
<listitem>
<para>인스턴스 번호 00 사용</para>
</listitem>
<listitem>
<para>FQDN을 사용하여 선택 취소</para>
</listitem>
<listitem>
<para>모든 비밀번호: SuSE1234 사용</para>
</listitem>
<listitem>
<para>파라미터 검토 중에 가상 이름 <emphasis role="strong">sapha1as</emphasis>가 사용되는지 재확인</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-installing-ers-on-hacert02">
<title>hacert02에 ERS 설치</title>
<para>설치 프로그램에서 확인하거나 사용해야 하므로 나중에 클러스터에서 사용할 서비스 IP 주소를 로컬 IP로 임시로 설정하십시오. 각 설치 단계에서 올바른 가상 호스트 이름을 사용하는지 확인합니다.</para>
<screen># ip address add 192.168.201.117/32 dev eth0
# mount efs-name:/ERS10 /usr/sap/HA1/ERS10
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME=sapha1er</screen>
<itemizedlist>
<listitem>
<para>SWPM 옵션은 SAP NetWeaver 버전 및 아키텍처에 따라 다릅니다.</para>
<itemizedlist>
<listitem>
<para>설치: SAP NetWeaver 7.40 SR2 → MaxDB → SAP-Systems → Application Server ABAP → 고가용성 시스템 → Enqueue Replication Server 인스턴스</para>
</listitem>
<listitem>
<para>설치: SAP NetWeaver 7.5 → SAP HANA 데이터베이스 → 설치 → Application Server ABAP → 고가용성 시스템 → Enqueue Replication Server 인스턴스</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>인스턴스 번호 10 사용</para>
</listitem>
<listitem>
<para>FQDN을 사용하여 선택 취소</para>
</listitem>
<listitem>
<para>파라미터 검토 중에 가상 이름 <emphasis role="strong">sapha1er</emphasis>가 사용되는지 재확인</para>
</listitem>
<listitem>
<para>설치 중에 권한과 관련한 오류가 발생하면 ERS 디렉토리의 소유권 변경</para>
</listitem>
</itemizedlist>
<screen># chown -R ha1adm:sapsys /usr/sap/HA1/ERS10</screen>
<itemizedlist>
<listitem>
<para>ASCS 인스턴스를 수동으로 중지/시작하라는 메시지가 표시되면 ha1adm 사용자로 hacert01에 로그인하고 sapcontrol을 호출합니다.</para>
</listitem>
</itemizedlist>
<screen># sapcontrol -nr 00 -function Stop    # to stop the ASCS
# sapcontrol -nr 00 -function Start   # to start the ASCS</screen>
</section>
<section xml:id="id-poststeps-for-ascs-and-ers">
<title>ASCS 및 ERS의 이후 단계</title>
<section xml:id="id-stopping-ascs-and-ers">
<title>ASCS 및 ERS 중지</title>
<para><emphasis>hacert01</emphasis></para>
<screen># su - ha1adm
# sapcontrol -nr 00 -function Stop
# sapcontrol -nr 00 -function StopService</screen>
<para><emphasis>hacert02</emphasis></para>
<screen># su - ha1adm
# sapcontrol -nr 10 -function Stop
# sapcontrol -nr 10 -function StopService</screen>
</section>
<section xml:id="id-maintaining-sapservices">
<title><emphasis/>sapservices 관리</title>
<para>두 클러스터 노드 모두에서 <emphasis>/usr/sap/sapservices</emphasis> 파일에 두 항목(ASCS 및 ERS)이 있는지 확인합니다. 두 호스트에서 누락된 항목을 복사하여 파일을 수정합니다. 아니면 <emphasis>sapstartsrv</emphasis>를 사용하여 누락된 명령 문자열을 추가합니다.</para>
<para>ERS 호스트의 ASCS 프로파일을 위한 예시 단계:</para>
<screen># cd /usr/sap/hostctrl/exe
# export LD_LIBRARY_PATH=.
# ./sapstartsrv pf=/usr/sap/HA1/SYS/profile/HA1_ASCS00_sapha1as -reg</screen>
<para>이를 통해 sapstartsrv 클라이언트가 다음과 같이 서비스를 시작할 수 있습니다.</para>
<para><emphasis>ha1adm 사용자로</emphasis></para>
<screen># sapcontrol -nr 10 -function StartService HA1</screen>
<para><emphasis>/usr/sap/sapservices</emphasis>는 다음과 같이 표시됩니다(일반적으로 인스턴스당 라인 1개).</para>
<screen>#!/bin/sh
LD_LIBRARY_PATH=/usr/sap/HA1/ASCS00/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/HA1/ASCS00/exe/sapstartsrv pf=/usr/sap/HA1/SYS/profile/HA1_ASCS00_sapha1as -D -u ha1adm
LD_LIBRARY_PATH=/usr/sap/HA1/ERS10/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/HA1/ERS10/exe/sapstartsrv pf=/usr/sap/HA1/ERS10/profile/HA1_ERS10_sapha1er -D -u ha1adm</screen>
</section>
<section xml:id="id-integrating-the-cluster-framework-using-the-sap-suse-cluster-connector-package">
<title>sap-suse-cluster-connector 패키지를 사용하여 클러스터 프레임워크 통합</title>
<para>리포지토리에서 <emphasis role="strong">sap-suse-cluster-connector</emphasis> 패키지 버전 3.0.0 이상을 설치합니다.</para>
<screen># zypper install sap-suse-cluster-connector</screen>
<note>
<para>sap-suse-cluster-connector 패키지의 3.0.x 버전은 SUSE SAP API 버전 3을 구현합니다. SAP RKS(Rolling Kernel Switch) 및 ASCS 마이그레이션 등의 새로운 기능은 이 새 버전에서만 지원됩니다.</para>
</note>
<para>ERS 및 ASCS 인스턴스의 경우, <emphasis>/usr/sap/HA1/SYS/profile/</emphasis> 프로파일 디렉토리에서 HA1_ASCS00_sapha1as 및 HA1_ERS10_sapha1er 인스턴스 프로파일 파일을 편집합니다.</para>
<para>sapstartsrv 서비스에 HA 스크립트 커넥터 라이브러리를 로드하고 sap-suse-cluster-connector를 사용하도록 명령합니다.</para>
<screen>service/halib = $(DIR_CT_RUN)/saphascriptco.so
service/halib_cluster_connector = /usr/bin/sap_suse_cluster_connector</screen>
<para>ha1adm 사용자를 unix 사용자 그룹 haclient를 추가합니다.</para>
<screen># usermod -a -G haclient ha1adm</screen>
</section>
<section xml:id="id-adapting-sap-profiles-to-match-the-sap-nw-ha-clu-7-40-certification">
<title>SAP 프로파일을 조정하여 SAP NW-HA-CLU 7.40 인증 일치</title>
<para>ASCS의 경우, 인큐 서버(enserver)에 대한 시작 명령을 <emphasis>Restart_Program_xx</emphasis>에서 <emphasis>Start_Program_xx</emphasis>로 변경합니다. 이렇게 하면 SAP 시작 프레임워크가 인큐 프로세스를 자체적으로 다시 시작하지 <emphasis role="strong">않습니다 </emphasis>. 이러한 재시작은 잠금 손실을 초래합니다.</para>
<formalpara>
<title>파일 /usr/sap/HA1/SYS/profile/HA1_ASCS00_sapha1as</title>
<para>
<screen>Start_Program_01 = local $(_EN) pf=$(_PF)</screen>
</para>
</formalpara>
<para>선택 사항으로 서비스 재시작 횟수를 제한할 수 있습니다(ASCS의 경우, 이로 인해 메시지 서버의 재시작이 제한됨).</para>
<para>ERS 인스턴스의 경우 인큐 복제 서버(enrepserver)에 대한 시작 명령을 <emphasis>Restart_Program_xx</emphasis>에서 <emphasis>Start_Program_xx</emphasis>로 변경합니다.</para>
<formalpara>
<title>파일 /usr/sap/HA1/SYS/profile/HA1_ERS10_sapha1er</title>
<para>
<screen>Start_Program_00 = local $(_ER) pf=$(_PFL) NR=$(SCSID)</screen>
</para>
</formalpara>
</section>
<section xml:id="id-starting-ascs-and-ers">
<title>ASCS 및 ERS 시작</title>
<para><emphasis>hacert01</emphasis></para>
<screen># su - ha1adm
# sapcontrol -nr 00 -function StartService HA1
# sapcontrol -nr 00 -function Start</screen>
<para><emphasis>hacert02</emphasis></para>
<screen># su - ha1adm
# sapcontrol -nr 10 -function StartService HA1
# sapcontrol -nr 10 -function Start</screen>
</section>
</section>
<section xml:id="id-installing-db-on-hacert03-example-sap-hana">
<title>hacert03에 DB 설치(예시 SAP HANA)</title>
<para>HANA DB의 HW 요구 사항은 매우 엄격합니다. 저장소 크기 조정은 여러 지표에 따라 달라집니다. <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://www.sap.com/documents/2015/03/74cdb554-5a7c-0010-82c7-eda71af511fa.html">SAP HANA 하드웨어 디렉토리</link> 및 <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://www.sap.com/documents/2015/03/74cdb554-5a7c-0010-82c7-eda71af511fa.html">SAP HANA TDI</link>에서 지원되는 구성을 확인하십시오.</para>
<para><link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://docs.aws.amazon.com/quickstart/latest/sap-hana/planning.html">AWS 클라우드에 AWS SAP HANA의 “배포 계획 수립”: 빠른 시작 참조 배포</link> 섹션의 설명과 같이 HANA 파일 시스템을 설치합니다.</para>
<para>서비스 IP 주소처럼 작동하는 오버레이 IP 주소로 데이터베이스를 설치하는 것을 고려하십시오. 이를 통해 SAP 클러스터용 SUSE Linux Enterprise High Availability Extension에서 실행할 데이터베이스를 업그레이드할 수 있습니다.</para>
<screen># ip address add 192.168.201.118/32 dev eth0
# mount /dev/sdc1 /hana/shared
# mount /dev/sdc2 /hana/log
# mount /dev/sdc3 /hana/data
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME=sapha1db</screen>
<itemizedlist>
<listitem>
<para>설치: SAP NetWeaver 7.5 → SAP HANA 데이터베이스 → 설치 → Application Server ABAP → 고가용성 시스템 → 데이터베이스 인스턴스</para>
</listitem>
<listitem>
<para>프로파일 디렉토리 /sapmnt/HA1/profile</para>
</listitem>
<listitem>
<para>FQDN을 사용하여 선택 취소</para>
</listitem>
<listitem>
<para>데이터베이스 파라미터: DBSID는 H04, 데이터베이스 호스트는 sapha1db, 인스턴스 번호는 00</para>
</listitem>
<listitem>
<para>데이터베이스 시스템 ID: 인스턴스 번호는 00, SAP 마운트 디렉토리는 /hana/shared</para>
</listitem>
<listitem>
<para>계정 파라미터: 사용자 지정 값이 필요한 경우 변경</para>
</listitem>
<listitem>
<para>정리: <emphasis role="strong">예</emphasis>를 선택하고 ’sapinst’ 그룹에서 운영 체제 사용자를 제거합니다.</para>
</listitem>
<listitem>
<para>파라미터 검토 중에 가상 이름 <emphasis role="strong">sapha1db</emphasis>가 사용되는지 재확인</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-installing-the-primary-application-server-pas-on-sapha1ci">
<title>sapha1ci에 기본 애플리케이션 서버(PAS) 설치</title>
<para>다음 마운트 지점을 sapha1ci 호스트의 <emphasis>/etc/fstab</emphasis> 파일에 추가합니다. “efs_fs_local_az” 문자열을 가용 영역에 있는 EFS 서비스의 IP 주소로 바꿉니다.</para>
<screen>efs-name:sapcd     /sapcd    nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
efs-name:sapmnt    /sapmnt   nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
efs-name:DVEBMGS01 /usr/sap/HA1/DVEBMGS01  nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
efs-name:SYS       /usr/sap/HA1/SYS        nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0</screen>
<para><emphasis>efs-name</emphasis> 변수를 해당 DNS 이름으로 바꿉니다.</para>
<para>마운트 디렉토리를 만들고 파일 시스템을 마운트합니다.</para>
<screen># mkdir -p /sapcd /sapmnt /usr/sap/HA1/SYS /usr/sap/HA1/DVEBMGS01
# mount -a</screen>
<para><emphasis>sapinst</emphasis> 도구를 사용하여 PAS 서버를 설치합니다.</para>
<itemizedlist>
<listitem>
<para>SWPM 옵션은 SAP NetWeaver 버전 및 아키텍처에 따라 다릅니다.</para>
<itemizedlist>
<listitem>
<para>설치: SAP NetWeaver 7.40 SR2→ MaxDB → SAP-Systems → Application Server ABAP → 고가용성 시스템 → 기본 애플리케이션 서버 인스턴스(PAS)</para>
</listitem>
<listitem>
<para>설치: SAP NetWeaver 7.5 → SAP HANA Database → 설치 → Application Server ABAP → 고가용성 시스템 → 기본 애플리케이션 서버 인스턴스(PAS)</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>인스턴스 번호 01 사용</para>
</listitem>
<listitem>
<para>FQDN을 사용하여 선택 취소</para>
</listitem>
<listitem>
<para>실습 설정에서는 기본 보안 스토어 키를 사용하십시오.</para>
</listitem>
<listitem>
<para>Diagnostic Agent를 설치하지 않음</para>
</listitem>
<listitem>
<para>SLD 없음</para>
</listitem>
<listitem>
<para>파라미터 검토 중에 가상 이름 <emphasis role="strong">sapha1ci</emphasis>가 사용되는지 재확인</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-installing-an-additional-application-server-aas-on-sapha1d2">
<title>sapha1d2에 추가 애플리케이션 서버(AAS) 설치</title>
<para>다음 마운트 지점을 sapha1d2 호스트의 <emphasis>/etc/fstab</emphasis> 파일에 추가합니다. “efs_fs_local_az” 문자열을 가용 영역에 있는 EFS 서비스의 IP 주소로 바꿉니다.</para>
<screen>efs-name:sapcd     /sapcd            nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
efs-name:sapmnt    /sapmnt           nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
efs-name:D02       /usr/sap/HA1/D02  nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
efs-name:SYS       /usr/sap/HA1/SYS  nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0</screen>
<para><emphasis>efs-name</emphasis> 변수를 해당 DNS 이름으로 바꿉니다. 마운트 디렉토리를 만들고 파일 시스템을 마운트합니다.</para>
<screen># mkdir -p /sapcd /sapmnt /usr/sap/HA1/SYS /usr/sap/HA1/D02
# mount -a</screen>
<para><emphasis>sapinst</emphasis> 도구를 사용하여 AAS 서버를 설치합니다.</para>
<itemizedlist>
<listitem>
<para>SWPM 옵션은 SAP NetWeaver 버전 및 아키텍처에 따라 다릅니다.</para>
<itemizedlist>
<listitem>
<para>설치: SAP NetWeaver 7.40 SR2→ MaxDB → SAP-Systems → Application Server ABAP → 고가용성 시스템 → 추가 애플리케이션 서버 인스턴스(AAS)</para>
</listitem>
<listitem>
<para>설치: SAP NetWeaver 7.5 → SAP HANA Database → 설치 → Application Server ABAP → 고가용성 시스템 → 추가 애플리케이션 서버 인스턴스(AAS)</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>인스턴스 번호 02 사용</para>
</listitem>
<listitem>
<para>FQDN을 사용하여 선택 취소</para>
</listitem>
<listitem>
<para>Diagnostic Agent를 설치하지 않음</para>
</listitem>
<listitem>
<para>파라미터 검토 중에 <emphasis role="strong">sapha1d2</emphasis> 이름이 사용되는지 재확인</para>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="id-implementing-the-cluster">
<title>클러스터 구현</title>
<para>클러스터를 구현하기 위한 주요 절차는 다음과 같습니다.</para>
<itemizedlist>
<listitem>
<para>운영 체제 설치 시 클러스터 소프트웨어가 설치되지 않은 경우에 설치</para>
</listitem>
<listitem>
<para>클러스터 통신 프레임워크 corosync 구성</para>
</listitem>
<listitem>
<para>클러스터 리소스 관리자 구성</para>
</listitem>
<listitem>
<para>클러스터 리소스 구성</para>
</listitem>
</itemizedlist>
<note>
<para>클러스터 설정을 계속 진행하기 전, 우선 모든 SAP 인스턴스를 중지하고 클러스터 노드에서 (수동으로 추가한) IP 주소를 제거한 후 나중에 클러스터에서 제어할 파일 시스템의 마운트를 해제합니다.</para>
</note>
<orderedlist numeration="arabic">
<title>작업</title>
<listitem>
<para>NTP를 설정합니다(YaST 사용 권장). 모든 EC2 인스턴스에서 액세스할 수 있는 169.254.169.123에서 AWS 타임 서비스를 사용합니다. 진행 중인 동기화를 활성화합니다.</para>
</listitem>
<listitem>
<para><emphasis>ha_sles</emphasis> 패턴을 두 클러스터 노드에 설치합니다.</para>
</listitem>
</orderedlist>
<screen># zypper install -t pattern ha_sles</screen>
<para>Public Cloud Module을 활성화하여 AWS CLI(명령줄 인터페이스)를 업데이트합니다.</para>
<screen># SUSEConnect --list-extensions
# SUSEConnect -p sle-module-public-cloud/12/x86_64</screen>
<para>다음 명령으로 패키지를 업데이트합니다.</para>
<screen># zypper update</screen>
<section xml:id="id-configuring-the-cluster-base">
<title>클러스터 베이스 구성</title>
<itemizedlist>
<title>작업</title>
<listitem>
<para>첫 번째 시스템에 클러스터 스택 설치 및 구성</para>
</listitem>
</itemizedlist>
<section xml:id="id-configuration-of-system-logging">
<title>시스템 로깅 구성</title>
<para>SUSE는 SUSE 클러스터에서 rsyslogd를 사용하여 로깅하는 것을 권장합니다. 이는 기본 구성입니다. 그러나 일부 AWS AMI에서는 syslogd 로깅을 사용합니다. 모든 클러스터 노드에서 루트로 다음 명령을 실행합니다.</para>
<screen># zypper install rsyslog</screen>
<para>옵션 1을 사용합니다(경쟁 소프트웨어인 syslogd 설치 제거). 두 노드를 재부팅합니다.</para>
</section>
<section xml:id="id-corosync-configuration">
<title>Corosync 구성</title>
<section xml:id="id-configuration-of-the-corosync-conf-file">
<title><emphasis>corosync.conf</emphasis> 파일 구성</title>
<para>구성에서는 node-1 노드에 대해 ip-node-1 및 ip2-node-1라는 IP 주소가 있습니다. node-2 노드는 ip-node-2 및 ip2-node-2라는 IP 주소를 가집니다.</para>
<para>모든 클러스터 노드에는 다음과 같이 구성되는 로컬 구성 파일 <emphasis>/etc/corosync/corosync.conf</emphasis>가 있어야 합니다.</para>
<para>관련 정보는 인터페이스와 노드 목록을 설명하는 두 섹션에서 제공됩니다. 나머지 항목은 특정 구현에 필요한 경우 구성할 수 있습니다.</para>
<note>
<para>AWS는 특정 수동 corosync 구성을 요구합니다.</para>
</note>
<para>두 클러스터 노드의 <emphasis>/etc/corosync/corosync.conf</emphasis> 파일에서 다음 구성을 사용하십시오.</para>
<screen># Read the corosync.conf.5 manual page
totem {
  version: 2
  rrp_mode: passive
  token: 30000
  consensus: 32000
  token_retransmits_before_loss_const: 10
  max_messages: 20
  crypto_cipher: none
  crypto_hash: none
  clear_node_high_bit: yes
  interface {
    ringnumber: 0
    bindnetaddr: &lt;ip-local-node&gt;
    mcastport: 5405
    ttl: 1
  }
  transport: udpu
}
 logging {
      fileline: off
      to_logfile: yes
      to_syslog: yes
      logfile: /var/log/cluster/corosync.log
      debug: off
      timestamp: on
      logger_subsys {
         subsys: QUORUM
         debug: off
     }
}
nodelist {
  node {
  ring0_addr: &lt;ip-node-1&gt;
  ring1_addr: &lt;ip2-node-1&gt;
  nodeid: 1
  }
  node {
  ring0_addr: &lt;ip-node-2&gt;
  ring1_addr: &lt;ip2-node-2&gt;
  nodeid: 2
  }
}

quorum {
  # Enable and configure quorum subsystem (default: off)
  # see also corosync.conf.5 and votequorum.5
  provider: corosync_votequorum
  expected_votes: 2
  two_node: 1
}</screen>
<para><emphasis>ip-node-1</emphasis> / <emphasis>ip2-node-1</emphasis> 및 <emphasis>ip-node-2</emphasis> / <emphasis>ip2-node-2</emphasis> 변수를 두 클러스터 인스턴스의 IP 주소로 바꿉니다. <emphasis>ip-local-node</emphasis>를 파일이 생성될 서버의 IP 주소로 바꿉니다.</para>
<para><emphasis>crypto_cipher</emphasis> 및 <emphasis>crypto_hash</emphasis>용으로 선택한 설정은 AWS의 클러스터에 적합합니다. 그리고 강력한 클러스터 통신 암호화가 필요한 경우에는 SUSE 문서에 따라 수정할 수 있습니다.</para>
</section>
</section>
<section xml:id="id-starting-the-cluster">
<title>클러스터 시작</title>
<para>다음 단계는 두 노드에서 다음 명령을 사용하여 클러스터를 시작하는 것입니다.</para>
<screen># systemctl start pacemaker</screen>
</section>
<section xml:id="id-checking-the-configuration">
<title>구성 확인</title>
<para>구성을 확인하기 위한 명령은 다음과 같습니다.</para>
<screen># corosync-cfgtool -s</screen>
<para>그러면 IP 주소가 10.0.0.111인 클러스터 노드를 위한 다음과 같은 결과가 제공됩니다.</para>
<screen>Printing ring status.
Local node ID 1
RING ID 0
id = 10.0.0.111
status = ring 0 active with no faults</screen>
<para>해당 클러스터는 링 0을 사용하며 노드의 ID는 1입니다.</para>
<itemizedlist>
<listitem>
<para><emphasis>crm_mon -1</emphasis> 출력은 다음과 같아야 합니다.</para>
</listitem>
</itemizedlist>
<screen>Stack: corosync
Current DC: hacert01 (version 1.1.15-19.15-e174ec8) - partition with quorum
Last updated: Wed Dec  6 16:02:42 2017
Last change: Wed Dec  6 15:44:45 2017 by hacluster via crmd on hacert01

2 nodes configured
0 resources configured

Online: [ hacert01 hacert02 ]

Full list of resources:</screen>
</section>
</section>
<section xml:id="id-configuring-cluster-resources">
<title>클러스터 리소스 구성</title>
<para>더 이상 마스터-슬레이브 구조를 사용하지 <emphasis role="strong">않고</emphasis> 클러스터와 더 유사한 구조로 변화된 변경된 SAP NetWeaver용 SAPInstance 리소스 에이전트가 필요합니다. 이를 통해 마스터-슬레이브 전체가 <emphasis role="strong">아니라</emphasis> ASCS 및 ERS 자체를 시작 및 정지할 수 있습니다.</para>
<para>이를 위해 ASCS가 ERS를 준수하기 위해 필요한 새로운 기능이 있습니다. 잠금 손실을 방지하려면 ASCS가 ERS의 공유 메모리 테이블을 마운트해야 합니다.</para>
<figure>
<title>리소스 및 제약 조건</title>
<mediaobject>
<imageobject>
<imagedata fileref="sles4sap_nw740_resources.svg" width=""/>
</imageobject>
<textobject><phrase>SVG</phrase></textobject>
</mediaobject>
</figure>
<para>RA 내에서 새 플래그인 “runs_ers_$SID”를 사용하여 구현할 수 있으며, 리소스 파라미터 “IS_ERS=TRUE”를 사용하여 활성화할 수 있습니다.</para>
<para>Route 53 에이전트를 추가하는 옵션이 제공됩니다. 그러면 아키텍처는 다음과 같게 됩니다.</para>
<figure>
<title>리소스 및 제약 조건</title>
<mediaobject>
<imageobject>
<imagedata fileref="sles4sap_nw740_awsr53_resources.svg" width=""/>
</imageobject>
<textobject><phrase>SVG</phrase></textobject>
</mediaobject>
</figure>
<section xml:id="id-preparing-the-cluster-for-adding-the-resources">
<title>리소스를 추가할 수 있도록 클러스터 준비</title>
<para>클러스터가 부분적으로 정의된 리소스를 시작하는 것을 방지하려면 클러스터를 유지보수 모드로 설정하십시오. 이렇게 하면 모든 모니터링 작업이 비활성화됩니다.</para>
<para><emphasis>root 사용자로</emphasis></para>
<screen># crm configure property maintenance-mode="true"</screen>
</section>
<section xml:id="id-configuring-aws-specific-settings">
<title>AWS 고유 설정 구성</title>
<para>두 클러스터 노드 중 하나에서 다음 명령을 실행하십시오.</para>
<screen># vi crm-bs.txt</screen>
<para><emphasis>crm-bs.txt</emphasis> 파일에 다음 정보를 입력합니다.</para>
<screen>property cib-bootstrap-options: \
    stonith-enabled="true" \
    stonith-action="poweroff" \
    stonith-timeout="600s"
rsc_defaults rsc-options: \
        resource-stickiness=1 \
        migration-threshold=3
op_defaults op-options: \
        timeout=600 \
        record-pending=true</screen>
<para><emphasis>poweroff</emphasis> 설정은 에이전트가 인스턴스를 강제로 종료하도록 합니다. 이는 AWS에서 Split Brain 상황을 방지하는 데 도움이 됩니다.</para>
<para>구성을 클러스터에 추가합니다.</para>
<screen># crm configure load update crm-bs.txt</screen>
</section>
<section xml:id="id-configuration-of-aws-specific-stonith-resource">
<title>AWS 고유 Stonith 리소스 구성</title>
<para>다음과 같은 내용으로 파일을 만듭니다.</para>
<screen>primitive res_AWS_STONITH stonith:external/ec2 \
op start interval=0 timeout=180 \
op stop interval=0 timeout=180 \
op monitor interval=120 timeout=60 \
params tag=pacemaker profile=cluster</screen>
<para>EC2 태그 <emphasis>pacemaker</emphasis> 항목은 EC2 인스턴스에 대해 선택한 태그와 일치해야 합니다. 이 태그의 값에는 호스트 이름이 포함됩니다. 프로파일 이름(이 예에서 <emphasis>cluster</emphasis>)은 이전에 구성한 AWS 프로파일과 일치해야 합니다.</para>
<para>이 파일의 이름을 예를 들어 <emphasis>aws-stonith.txt</emphasis>로 지정한 후 이 파일을 구성에 추가합니다. 루트로 다음 명령을 실행해야 합니다. 여기서 파일 이름 <emphasis>aws-stonith.txt</emphasis>가 사용됩니다.</para>
<screen># crm configure load update aws-stonith.txt</screen>
</section>
<section xml:id="id-configuring-the-resources-for-the-ascs">
<title>ASCS용 리소스 구성</title>
<para>우선 파일 시스템, IP 주소 및 SAP 인스턴스를 위한 리소스를 구성합니다. 물론, 환경에 따라 파라미터를 조정해야 합니다.</para>
<para>원하는 편집기를 사용하여 이름이 <emphasis>aws-ascs.txt</emphasis>인 파일을 만듭니다. ASCS 기본 및 ASCS 그룹을 이 파일에 추가합니다. 변경 사항을 저장합니다.</para>
<example>
<title>ASCS 기본</title>
<screen>primitive rsc_fs_HA1_ASCS00 Filesystem \
    params  device="efs-name:/ASCS00" directory="/usr/sap/HA1/ASCS00" \
            fstype="nfs4" \
            options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
    op start timeout=60s interval=0 \
    op stop timeout=60s interval=0 \
    op monitor interval=200s timeout=40s
primitive rsc_ip_HA1_ASCS00 ocf:suse:aws-vpc-move-ip \
    params  address=192.168.201.116 routing_table=rtb-table \
            interface=eth0 profile=cluster \
    op start interval=0 timeout=180 \
    op stop interval=0 timeout=180 \
    op monitor interval=60 timeout=60
primitive rsc_sap_HA1_ASCS00 SAPInstance \
    operations $id=rsc_sap_HA1_ASCS00-operations \
    op monitor interval=120 timeout=60 on-fail=restart \
    params InstanceName=HA1_ASCS00_sapha1as \
        START_PROFILE="/sapmnt/HA1/profile/HA1_ASCS00_sapha1as" \
        AUTOMATIC_RECOVER=false \
    meta resource-stickiness=5000 failure-timeout=60 \
        migration-threshold=1 priority=10</screen>
</example>
<para><emphasis>efs-name</emphasis> 변수를 EFS 서버의 이름으로 바꿉니다.</para>
<para><emphasis>rtb-table</emphasis> 변수를 서브넷의 해당 AWS 라우팅 테이블 식별자로 바꿉니다. AWS CLI 프로파일 이름(이 예에서 <emphasis>cluster</emphasis>)은 이전에 구성한 AWS 프로파일과 일치해야 합니다.</para>
<example>
<title>ASCS 그룹</title>
<screen>group grp_HA1_ASCS00 \
  rsc_ip_HA1_ASCS00 rsc_fs_HA1_ASCS00 rsc_sap_HA1_ASCS00 \
        meta resource-stickiness=3000</screen>
</example>
<para>원하는 텍스트 편집기를 사용하여 txt 파일 <emphasis>aws_ascs.txt</emphasis>를 만들고 이 파일에 두 예시(기본 및 그룹)를 입력한 후 구성을 클러스터 관리자 구성에 로드합니다.</para>
<screen># crm configure load update aws_ascs.txt</screen>
</section>
<section xml:id="id-optional-including-route-53">
<title>선택 사항: Route 53 포함</title>
<para>이 파일의 이름을 예를 들어 <emphasis>aws-route53.txt</emphasis>로 지정한 후 이 파일을 구성에 추가합니다. 루트로 다음 명령을 실행해야 합니다. 여기서 파일 이름 <emphasis>aws-route53.txt</emphasis>가 사용됩니다.</para>
<para>편집기에서 기존에 있는 기본 앞 또는 뒤에 다음 기본을 입력합니다.</para>
<example>
<title>ROUTE53 기본</title>
<screen>primitive rsc_r53_HA1_ASCS00 ocf:heartbeat:aws-vpc-route53 \
   params hostedzoneid=route-53-name ttl=10 fullname=name-full. profile=cluster \
   op start interval=0 timeout=180 \
   op stop interval=0 timeout=180 \
   op monitor interval=300 timeout=180</screen>
</example>
<para><emphasis>route-53-name</emphasis> 변수를 연결된 프라이빗 호스팅 Route 53 영역의 이름으로 바꿉니다.</para>
<para><emphasis>name-full</emphasis> 변수를 프라이빗 호스팅 Route 53 영역과 일치하는 전체 호스트 이름으로 바꿉니다.</para>
<para>이 예시에서 에이전트는 10초의 TTL(time-to-live)을 사용합니다. 필요한 경우 이 파라미터를 변경하십시오.</para>
<para><emphasis>rsc_ip_HA1_ASCS00</emphasis> 다음에 <emphasis>rsc_r53_HA1_ASCS00</emphasis>을 삽입합니다. 그러면 그룹이 Route 53을 오버레이 IP 주소 다음의 두 번째 항목으로 업데이트하게 됩니다.</para>
<example>
<title>ROUTE53 그룹</title>
<screen>group grp_HA1_ASCS00 \
  rsc_ip_HA1_ASCS00 rsc_r53_HA1_ASCS00 \
  rsc_fs_HA1_ASCS00 rsc_sap_HA1_ASCS00 \
        meta resource-stickiness=3000</screen>
<para>원하는 텍스트 편집기를 사용하여 txt 파일 <emphasis>aws-route53.txt</emphasis>를 만들고 이 파일에 두 예시(기본 및 그룹)를 입력한 후 구성을 클러스터 관리자 구성에 로드합니다. 루트로 다음 명령을 사용하고 편집기에서 ASCS 그룹을 수정합니다.</para>
<screen># crm configure load update aws-route53.txt</screen>
</example>
<note>
<para>EC2 메타 데이터의 사용자 데이터 섹션에 “local-ipv4”와 같은 문자열이 포함되면 Route 53 에이전트의 1.0.2 버전이 작동하지 않습니다!</para>
</note>
</section>
<section xml:id="id-configuring-the-resources-for-the-ers">
<title>ERS용 리소스 구성</title>
<para>두 번째로 파일 시스템, IP 주소 및 SAP 인스턴스를 위한 리소스를 구성합니다. 물론, 환경에 따라 파라미터를 조정해야 합니다.</para>
<para><emphasis>efs-name</emphasis>을 EFS 서버의 이름으로 바꿉니다.</para>
<para>특정 파라미터 <emphasis>IS_ERS=true</emphasis>는 ERS 인스턴스에 대해서만 설정해야 합니다.</para>
<example>
<title>ERS 기본</title>
<screen>primitive rsc_fs_HA1_ERS10 Filesystem \
  params device="efs-name:/ERS10" directory="/usr/sap/HA1/ERS10" fstype=nfs4 \
  options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
  op start timeout=60s interval=0 \
  op stop timeout=60s interval=0 \
  op monitor interval=200s timeout=40s
primitive rsc_ip_HA1_ERS10 ocf:suse:aws-vpc-move-ip \
  params address=192.168.201.117 routing_table=rtb-table \
  interface=eth0 profile=cluster \
  op start interval=0 timeout=180 \
  op stop interval=0 timeout=180 \
  op monitor interval=60 timeout=60
primitive rsc_sap_HA1_ERS10 SAPInstance \
  operations $id=rsc_sap_HA1_ERS10-operations \
  op monitor interval=120 timeout=60 on-fail=restart \
  params InstanceName=HA1_ERS10_sapha1er \
         START_PROFILE="/sapmnt/HA1/profile/HA1_ERS10_sapha1er" \
         AUTOMATIC_RECOVER=false IS_ERS=true \
  meta priority=1000</screen>
</example>
<para><emphasis>rtb-table</emphasis> 변수를 서브넷의 해당 AWS 라우팅 테이블 식별자로 바꿉니다. AWS CLI 프로파일 이름(이 예에서 <emphasis>cluster</emphasis>)은 이전에 구성한 AWS 프로파일과 일치해야 합니다.</para>
<example>
<title>ERS 그룹</title>
<screen>group grp_HA1_ERS10 \
  rsc_ip_HA1_ERS10 rsc_fs_HA1_ERS10 rsc_sap_HA1_ERS10</screen>
</example>
<para>원하는 텍스트 편집기를 사용하여 txt 파일(예: <emphasis>aws_crm_ers.txt</emphasis>)을 만들고 이 파일에 두 예시(기본 및 그룹)를 입력한 후 구성을 클러스터 관리자 구성에 로드합니다.</para>
<para><emphasis>root 사용자로</emphasis></para>
<screen># crm configure load update aws_crm_ers.txt</screen>
</section>
<section xml:id="id-configuring-the-colocation-constraints-between-ascs-and-ers">
<title>ASCS와 ERS 간 코로케이션 제약 조건 구성</title>
<para>오류(loc_sap_HA1_failover_to_ers) 발생 후 ASCS 인스턴스가 정확히 ERS 인스턴스가 실행되는 클러스터 노드에서 시작하도록 ASCS와 ERS 인스턴스 사이의 제약 조건을 정의해야 합니다. 이러한 제약 조건에서는 ASCS 인스턴스(또는 노드)에서 오류가 발생한 후 잠금이 손실되지 않는지 확인해야 합니다.</para>
<para>클러스터에 의해 ASCS 인스턴스가 시작된 경우에는 ERS 인스턴스가 “다른” 클러스터 노드(col_sap_HA1_no_both)로 이동해야 합니다. 이 제약 조건은 ERS가 잠금을 재동기화하여 다른 인수에서 클러스터를 사용할 수 있도록 하기 위해 필요합니다.</para>
<example>
<title>위치 제약 조건</title>
<screen>colocation col_sap_HA1_no_both -5000: grp_HA1_ERS10 grp_HA1_ASCS00
location loc_sap_HA1_failover_to_ers rsc_sap_HA1_ASCS00 \
         rule 2000: runs_ers_HA1 eq 1
order ord_sap_HA1_first_start_ascs Optional: rsc_sap_HA1_ASCS00:start \
      rsc_sap_HA1_ERS10:stop symmetrical=false</screen>
</example>
<para>원하는 텍스트 편집기를 사용하여 텍스트 파일(예: <emphasis>crm_col.txt</emphasis>)을 만들고 이 파일에 세 가지 제약 조건을 모두 입력한 후 구성을 클러스터 관리자 구성에 로드합니다.</para>
<para>루트로 다음 명령을 실행합니다.</para>
<screen># crm configure load update crm_col.txt</screen>
</section>
<section xml:id="id-activating-the-cluster">
<title>클러스터 활성화</title>
<para>마지막 단계는 클러스터 유지보수 모드를 종료하고 클러스터가 이미 실행 중인 리소스를 감지하도록 허용하는 것입니다.</para>
<para>루트로 다음 명령을 실행합니다.</para>
<screen># crm configure property maintenance-mode="false"</screen>
<para>클러스터가 이제 ASCS 및 ERS 시스템을 시작합니다. 이 절차는 몇 분 걸립니다. 다음 명령으로 진행 상황을 확인합니다.</para>
<screen># crm status</screen>
</section>
</section>
</section>
<section xml:id="id-administration">
<title>관리</title>
<section xml:id="id-dos-and-donts">
<title>권장 작업 및 금지 작업</title>
<section xml:id="id-never-stop-the-ascs-instance">
<title>ASCS 인스턴스 중지 금지</title>
<para>정상적인 작동을 위해 클러스터 도구 또는 SAP 도구 등의 도구를 사용하여 ASCS SAP 인스턴스를 <emphasis role="strong">중지하지 마십시오</emphasis>. ASCS 인스턴스를 중지하면 인큐 잠금이 손실될 수 있습니다. 새로운 SAP NW-HA-CLU 7.40 인증에 따라 클러스터는 반드시 ASCS의 로컬 재시작을 허용해야 하기 때문입니다. 이 기능은 클러스터를 재구성하지 않고 RKS(Rolling Kernel Switch)를 업데이트하기 위해 필요합니다.</para>
<warning>
<para>ASCS 인스턴스를 중지하면 동일 노드에서 ASCS를 시작하는 동안 SAP 인큐 잠금이 손실될 수 있습니다.</para>
</warning>
</section>
<section xml:id="id-how-to-migrate-ascs">
<title>ASCS 마이그레이션 방법</title>
<para>ASCS SAP 인스턴스를 <emphasis role="strong">마이그레이션</emphasis>하려면, SAP 관리 콘솔 등 SAP 도구를 사용해야 합니다. 이를 통해 sapstartsrv를 트리거하고 sap-suse-cluster-connector를 사용해 ASCS 인스턴스를 마이그레이션할 수 있습니다. <emphasis>ha1adm</emphasis> 사용자로 다음 명령을 호출하여 ASCS를 마이그레이션할 수 있습니다. 마이그레이션을 수행하면 항상 ASCS가 ERS 측으로 마이그레이션되고 SAP 인큐 잠금이 유지됩니다.</para>
<para><emphasis>ha1adm으로</emphasis></para>
<screen># sapcontrol -nr 00 -function HAFailoverToNode ""</screen>
</section>
<section xml:id="id-never-block-resources">
<title>리소스 차단 금지</title>
<para>SAP NW-HA-CLU 7.40에서는 <emphasis role="strong">더 이상 리소스의 수동 제어를 차단할 수 없습니다</emphasis>. 즉, 더 이상 <emphasis>/etc/sysconfig/sap_suse_cluster_connector</emphasis>에서 <emphasis>BLOCK_RESOURCES</emphasis> 변수를 사용할 수 없습니다.</para>
</section>
<section xml:id="id-always-use-unique-instance-numbers">
<title>항상 고유 인스턴스 번호 사용</title>
<para>현재 클러스터로 제어되는 모든 SAP <emphasis role="strong">인스턴스 번호는 고유해야 합니다</emphasis>. D00과 같이 서로 다른 시스템에서 실행되는 여러 dialog 인스턴스가 있어야 하는 경우, 클러스터에 의해 제어되지 않아야 합니다.</para>
</section>
<section xml:id="id-how-to-set-the-cluster-in-maintenance-mode">
<title>유지보수 모드로 클러스터 설정 방법</title>
<para>클러스터를 유지보수 모드로 설정하는 절차는 <emphasis>root</emphasis> 또는 <emphasis>sidadm</emphasis>로 수행할 수 있습니다.</para>
<para><emphasis>root 사용자로</emphasis></para>
<screen># crm configure properties maintenance-mode="true"</screen>
<para><emphasis>ha1adm 사용자로(전체 경로 필요)</emphasis></para>
<screen># /usr/sbin/crm configure properties maintenance-mode="true"</screen>
</section>
<section xml:id="id-procedure-to-end-the-cluster-maintenance">
<title>클러스터 유지보수 종료 절차</title>
<para><emphasis>root 사용자로</emphasis></para>
<screen># crm configure properties maintenance-mode="false"</screen>
</section>
<section xml:id="id-cleanup-resources">
<title>리소스 정리</title>
<para><emphasis role="strong">리소스 오류 정리</emphasis> 방법 ASCS 오류는 자동으로 삭제되어 설정된 시간 이후에 장애 복구가 수행됩니다. 기타 모든 리소스의 경우 오류를 포함한 상태를 정리할 수 있습니다.</para>
<para><emphasis>root 사용자로</emphasis></para>
<screen># crm resource cleanup RESOURCE-NAME</screen>
<warning>
<para>전체 ASCS 리소스 그룹을 정리하지 않아야 합니다. 그러면 ERS 인스턴스가 실행 중인 노드로 전체 그룹을 인수하는 원치 않는 클러스터 동작이 수행될 수 있습니다.</para>
</warning>
</section>
</section>
<section xml:id="id-test-the-cluster">
<title>클러스터 테스트</title>
<para>클러스터로 실제 운영을 진행하기 전에 최소한 다음 테스트를 진행하기를 강력히 권장합니다.</para>
<section xml:id="id-check-product-names-with-hagetfailoverconfig">
<title>HAGetFailoverConfig를 사용하여 제품 이름 확인</title>
<para>SUSE 클러스터 솔루션의 이름이 sapcontrol 또는 SAP 관리 콘솔의 출력에 표시되는지 확인합니다. 이 테스트에서는 SAP NetWeaver 클러스터 통합의 상태를 확인합니다.</para>
<para><emphasis>ha1adm 사용자로</emphasis></para>
<screen># sapcontrol -nr 00 -function HAGetFailoverConfig</screen>
</section>
<section xml:id="id-start-sap-checks-using-hacheckconfig-and-hacheckfailoverconfig">
<title>HACheckConfig 및 HACheckFailoverConfig를 사용한 SAP 확인 시작</title>
<para>HA 구성 테스트에 오류가 표시되지 않는지 확인합니다.</para>
<para><emphasis>ha1adm 사용자로</emphasis></para>
<screen># sapcontrol -nr 00 -function HACheckConfig
# sapcontrol -nr 00 -function HACheckFailoverConfig</screen>
</section>
<section xml:id="id-manually-migrate-ascs">
<title>ASCS 수동 마이그레이션</title>
<para>HA 도구를 사용한 ASCS 수동 마이그레이션이 원활하게 수행되는지 확인합니다.</para>
<para><emphasis>root 사용자로</emphasis></para>
<screen># crm resource migrate rsc_sap_HA1_ASCS00 force
## wait till the ASCS is been migrated to the ERS host
# crm resource unmigrate rsc_sap_HA1_ASCS00</screen>
</section>
<section xml:id="id-migrate-ascs-using-hafailovertonode">
<title>HAFailoverToNode를 사용한 ASCS 마이그레이션</title>
<para>sapcontrol과 같은 SAP 도구를 사용한 ASCS 인스턴스 이동이 원활하게 수행되는지 확인합니다.</para>
<para><emphasis>ha1adm 사용자로</emphasis></para>
<screen># sapcontrol -nr 00 -function HAFailoverToNode ""</screen>
</section>
<section xml:id="id-test-ascs-migration-after-failure">
<title>오류 이후 ASCS 마이그레이션 테스트</title>
<para>노드에서 오류가 발생한 후 ASCS 인스턴스가 올바르게 이동하는지 확인합니다.</para>
<para><emphasis>root 사용자로</emphasis></para>
<screen>## on the ASCS host
# echo b &gt;/proc/sysrq-trigger</screen>
</section>
<section xml:id="id-inplace-restart-of-ascs-using-stop-and-start">
<title>중지 및 시작을 사용하여 제위치에서 ASCS 재시작</title>
<para>제위치에서 SAP 리소스 재시작이 올바르게 수행되는지 확인합니다. SAP 인스턴스는 다른 노드로 장애 조치되지 않아야 하며, 중지했던 동일한 노드에서 시작해야 합니다.</para>
<warning>
<para>이 테스트는 SAP 시스템이 인큐 잠금을 <emphasis role="strong">손실</emphasis>하게 만듭니다. <emphasis role="strong">실제 운영 중에는 이 테스트를 수행하지 않아야 합니다.</emphasis></para>
</warning>
<para><emphasis>ha1adm 사용자로</emphasis></para>
<screen>## example for ASCS
# sapcontrol -nr 00 -function Stop
## wait till the ASCS is completely down
# sapcontrol -nr 00 -function Start</screen>
</section>
<section xml:id="id-additional-tests-to-perform">
<title>수행할 추가 테스트</title>
<itemizedlist>
<listitem>
<para>ASCS의 자동 재시작(RKS 시뮬레이션)</para>
</listitem>
<listitem>
<para>메시지 서버 프로세스의 복구 가능 및 복구 불가 중단을 확인합니다.</para>
</listitem>
<listitem>
<para>SAP 인큐 서버 프로세스의 복구 불가 중단을 확인합니다.</para>
</listitem>
<listitem>
<para>SAP Enqueue Replication Server의 중단을 확인합니다.</para>
</listitem>
<listitem>
<para>sapstartsrv의 중단 및 재시작을 확인합니다.</para>
</listitem>
<listitem>
<para>가능한 경우, RKS(Rolling Kernel Switch) 절차를 확인합니다.</para>
</listitem>
<listitem>
<para>업그레이드의 시뮬레이션을 확인합니다.</para>
</listitem>
<listitem>
<para>클러스터 리소스 오류의 시뮬레이션을 확인합니다.</para>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="id-aws-specific-post-installation-tasks">
<title>AWS 관련 설치 후 작업</title>
<para>선택 사항으로 Route 53 에이전트를 설치하면 필요한 경우 중앙 인스턴스의 DNS 이름을 업데이트합니다. Route 53 명명 테이블은 SAP GUI 사용자와 같은 온프레미스 사용자에게 표시되어야 할 수 있습니다. 이는 Route 53에 이름 확인을 위임하도록 온프레미스 네임 서버를 업데이트할 때 발생합니다. 이러한 이름 확인 요청을 전달하려면 AWS VPC에 추가 구성이 필요합니다.</para>
<para>Active directory 사용자는 [<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://aws.amazon.com/de/blogs/security/how-to-set-up-dns-resolution-between-on-premises-networks-and-aws-using-aws-directory-service-and-amazon-route-53/">https://aws.amazon.com/de/blogs/security/how-to-set-up-dns-resolution-between-on-premises-networks-and-aws-using-aws-directory-service-and-amazon-route-53/</link>]의 설명과 같이 Active Directory Connector를 구성해야 합니다.</para>
<para>DNS 서버 사용자는 [<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="http://www.scalingbits.com/aws/dnsfailover/backpropagation">http://www.scalingbits.com/aws/dnsfailover/backpropagation</link>]의 설명과 같이 바인드 포워딩 EC2 인스턴스를 구현해야 합니다.</para>
</section>
<section xml:id="id-additional-implementation-scenarios">
<title>추가 구현 시나리오</title>
<section xml:id="id-adaptive-server-enterprise-replication-failover-automation-integration">
<title>적응형 서버 엔터프라이즈 복제 장애 조치 자동화 통합</title>
<section xml:id="id-fm-integration-with-suse-linux-enterprise-high-availability-extension-cluster">
<title>SUSE Linux Enterprise High Availability Extension Cluster와 FM 통합</title>
<para>HA 설정을 위한 AWS 기반 표준 SAP는 ASCS, 1개의 AZ 및 해당 ERS에서 실행되는 기본 DB, 동일한 리전의 두 번째 AZ에서 실행되는 보조 DB가 포함된 다중 AZ 배포입니다. 로드를 기준으로 한 기본 애플리케이션 서버 및 추가 애플리케이션 서버가 복원력을 제공하기 위해 두 AZ에 마찬가지로 배포될 수 있습니다. SAP NetWeaver 또는 Business Suite 시스템이 SAP Sybase ASE에서 실행 중인 시나리오를 생각해 보십시오. ABAP 스택(ASCS)을 위한 완전 자동화 HA는 SUSE Linux Enterprise High Availability Extension 클러스터에서 제공됩니다. Sybase ASE DB의 경우 HA 기능은 Always On 구성으로 제공되며 장애 조치 조정은 일반적으로 세 번째 호스트(기본 및 보조 DB 외)에 설치되는 FM(Fault Manager) 유틸리티로 수행됩니다. SAP 환경에서 FM 유틸리티는 SAP DB 종속 커널과 함께 제공되며 ASCS Work 디렉토리(<emphasis>/usr/sap/&lt;SID&gt;/ASCS&lt;instnr&gt;/exe/</emphasis>)에 설치됩니다. ASCS 인스턴스 및 연결 디렉토리(단, Amazon EFS 또는 NFS를 사용하여 공유 파일 시스템에 설치된 경우)의 장애 조치는 SUSE Linux Enterprise High Availability Extension Cluster로 처리됩니다.</para>
</section>
<section xml:id="id-sybase-ase-always-on">
<title>Sybase ASE Always On</title>
<para>SAP Sybase ASE에는 네이티브 HA 및 DR 기능을 제공하는 Always On 기능이 함께 제공됩니다. 항상 사용 옵션은 2개의 SAP ASE 서버로 구성되는 고가용성 및 재해 복구(HADR) 시스템이며, 여기서 사용되는 SAP ASE 서버 중 하나는 주 서버로 모든 트랜잭션이 처리됩니다. 다른 서버는 주 서버를 위한 웜 대기(DR 모드의 경우 “대기 서버”라고 하며 HA 모드에서는 “컴패니언”이라고 함)를 수행하며 주 서버에서 지정된 데이터베이스의 사본이 포함됩니다. 장애 조치 조정은 Fault Manager라는 ASE 제공 유틸리티에 의해 수행됩니다. Fault Manager는 Replication Management Agent(RMA), ASE, Replication Server, 응용 프로그램, 데이터베이스 및 운영 체제 등 HADR 환경의 다양한 구성 요소를 모니터링합니다. Fault Manager의 기본적인 작업은 최소한의 수동 작업으로 자동 장애 조치를 시작하여 ASE 클러스터의 고가용성(장애 조치 중 데이터 손실 없음)을 보장하는 것입니다. SAP Stack에서 장애 관리자 유틸리티(sybdbfm)는 SAP 커널에 종속적인 DB(Sybase ASE)의 일부로 제공됩니다. HA 모드에서의 Sybase ASE DB 설정에 대해서는 SAP 표준 ASE HA-DR 가이드(<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://help.sap.com/viewer/efe56ad3cad0467d837c8ff1ac6ba75c/16.0.3.6/en-US/a6645e28bc2b1014b54b8815a64b87ba.html">https://help.sap.com/viewer/efe56ad3cad0467d837c8ff1ac6ba75c/16.0.3.6/en-US/a6645e28bc2b1014b54b8815a64b87ba.html</link>)를 참조하십시오.</para>
<important>
<para>다음 섹션에서는 몇 가지 예시와 일반 예시를 사용합니다. 일반 예시에서는 &lt;SID&gt;, &lt;instance nr&gt;와 같은 용어가 사용되며, 사용 중인 환경에 따라 조정되어야 합니다. 예를 들어, <emphasis>su - &lt;sid&gt;adm</emphasis>은 <emphasis>su - ha7adm</emphasis>을 의미하며 <emphasis>cd /usr/sap/&lt;SID&gt;/ASCS&lt;instance nr&gt;/work</emphasis>에서 대문자로 된 부분은 <emphasis>cd /usr/sap/HA7/ASCS00/work</emphasis>를 의미합니다.</para>
</important>
</section>
<section xml:id="id-database-host-preparation">
<title>데이터베이스 호스트 준비</title>
<para>본 가이드는 공식 HADR 문서와 동일하지 않습니다. 다음 절차는 사용자가 관심을 가져야 하는 주요 사항을 설명합니다.</para>
<formalpara>
<title>32비트 환경 설치</title>
<para>
<screen># zypper install glibc-32bit libgcc_s1-32bit</screen>
</para>
</formalpara>
<para>예를 들어, 이 소프트웨어 스택은 다음과 같이 사용됩니다.</para>
<itemizedlist>
<listitem>
<para>SL TOOLSET 1.0 — SWPM → NW 7.0x 이상용 1.0 SP25</para>
</listitem>
<listitem>
<para>saphostagent → 7.21 패치 41</para>
</listitem>
<listitem>
<para>SAP 커널 → 7.53 PL421</para>
</listitem>
<listitem>
<para>SAP 설치 내보내기 → (51051806_1)</para>
</listitem>
<listitem>
<para>Sybase RDBMS→ ASE 16.0.03.06 RDBMS(51053561_1)</para>
</listitem>
</itemizedlist>
<note>
<para>간략한 설치 정보 표는 다음 단계를 준비하는 데 매우 유용합니다. SAP Adaptive Server Enterprise - 설치 워크시트 <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://help.sap.com/viewer/efe56ad3cad0467d837c8ff1ac6ba75c/16.0.3.6/en-US/3fe35550f3814b2bb411d5494976e25a.html">https://help.sap.com/viewer/efe56ad3cad0467d837c8ff1ac6ba75c/16.0.3.6/en-US/3fe35550f3814b2bb411d5494976e25a.html</link></para>
</note>
<important>
<para>Fault Manager를 사용하면 이러한 설정 작업이 향상됩니다. 이 시나리오를 지원하는 최소 버전은 * SAP Kernel 749 PL632 * SAP Kernel 753 PL421입니다.</para>
</important>
</section>
<section xml:id="id-database-installation-for-replication-scenario">
<title>복제 시나리오를 위한 데이터베이스 설치</title>
<para>설치는 SAP가 제공하는 SWPM을 사용하여 완료할 수 있습니다.</para>
<itemizedlist>
<title>SWPM을 사용한 기본 데이터베이스 설치:</title>
<listitem>
<para>SWPM 옵션은 SAP NetWeaver 버전 및 아키텍처에 따라 다릅니다.</para>
<itemizedlist>
<listitem>
<para>Software Provisioning Manager 1.0 SP 25 → SAP NetWeaver AS for ABAP 7.52 → SAP ASE → 설치 → Application Server ABAP → 고가용성 시스템 → 데이터베이스 인스턴스</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>마법사에서 요청되는 정보는 다음과 같습니다.</para>
<itemizedlist>
<listitem>
<para>마스터 비밀번호&lt;secure&gt;</para>
</listitem>
<listitem>
<para>SAP 시스템 코드 페이지: 유니코드(기본값)</para>
</listitem>
<listitem>
<para>선택 취소: SAP 시스템용 FQDN 설정</para>
</listitem>
<listitem>
<para>Sybase 데이터베이스 관리자 UID: 2003</para>
</listitem>
<listitem>
<para>테스트 설정에서 선택 취소 -→ sybmgmtdb 데이터베이스용으로 별도의 장치 사용</para>
</listitem>
</itemizedlist>
<para>기본 설치가 완료되면 복제를 위해 기본 데이터베이스를 준비해야 합니다. 우선 <emphasis role="strong">sa</emphasis> 사용자의 잠금을 풀어야 합니다.</para>
<screen># su - syb&lt;sid&gt;
# isql -Usapsso -P &lt;secure password&gt; -S&lt;SID&gt; -X -w1900
# 1&gt; go
# 1&gt; exec sp_locklogin sa, 'unlock'
# 2&gt; go
# Account unlocked.
# (return status = 0)
# 1&gt; quit</screen>
<para>다음 단계에서는 응답 파일과 함께 SRS 소프트웨어를 설치하고 <emphasis>syb&lt;sid&gt;</emphasis> 사용자로 다음 명령을 입력합니다.</para>
<screen># /sapcd/ase-16.0.03.06/BD_SYBASE_ASE_16.0.03.06_RDBMS_for_BS_/SYBASE_LINUX_X86_64/setup.bin -f /sybase/HA7/srs-setup.txt -i silent</screen>
<para>응답 파일과 함께 기본 노드에 HADR을 활성화하고 <emphasis>syb&lt;sid&gt;</emphasis> 사용자로 다음 명령을 입력합니다.</para>
<screen># setuphadr /sybase/HA7/HA7_primary_lin.rs.txt</screen>
<note>
<para>다음과 같은 오류 메시지와 함께 설치가 중지되면 아래에서 설명되는 단계를 실행합니다.</para>
</note>
<screen>Clean up environment.
Environment cleaned up.
Error: Fail to connect to "PRIM" site SAP ASE at "&lt;hostname&gt;:4901".</screen>
<para>호스트 이름 및 포트 번호가 올바르고 데이터베이스 서버가 시작되어 실행 중인지 확인합니다. 모든 항목이 정확하고 네트워크 연결을 사용할 수 있으면 <emphasis>interface</emphasis> 파일을 수정하는 데 도움이 됩니다. <emphasis>/sybase/&lt;SID&gt;/interfaces</emphasis> 파일의 해당 호스트 이름의 IP 주소가 있는 &lt;SID&gt; 섹션에 새 라인을 추가합니다.</para>
<screen># vi /sybase/&lt;SID&gt;/interfaces
...
        master tcp ether &lt;hostname&gt; 4901
        master tcp ether 172.17.1.21 4901
...</screen>
<para>데이터베이스를 위한 보안 스토어 키를 만듭니다.</para>
<screen># /usr/sap/hostctrl/exe/saphostctrl -user sapadm &lt;secure password&gt; -function LiveDatabaseUpdate -dbname &lt;SID&gt; -dbtype syb -dbuser DR_admin -dbpass &lt;Secure password&gt; -updatemethod Execute -updateoption TASK=SET_USER_PASSWORD -updateoption USER=DR_ADMIN</screen>
<itemizedlist>
<title>SWPM을 사용한 컴패니언 데이터베이스 설치:</title>
<listitem>
<para>SWPM 옵션은 SAP NetWeaver 버전 및 아키텍처에 따라 다릅니다.</para>
<itemizedlist>
<listitem>
<para>Software Provisioning Manager 1.0 SP 25 → SAP NetWeaver AS for ABAP 7.52 → SAP ASE → 데이터베이스 복제 → 복제 환경 설정</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>마법사에서 요청되는 정보는 다음과 같습니다.</para>
<itemizedlist>
<listitem>
<para>복제 시스템 파라미터 -→ SID, 마스터 비밀번호, 보조 데이터베이스 인스턴스 설정을 선택</para>
</listitem>
<listitem>
<para>기본 데이터베이스 서버 -→ 호스트 이름 또는 가상 이름</para>
</listitem>
<listitem>
<para>기본 데이터베이스 서버 포트 → 기본값은 4901이지만 주 서버의 설정에 따라 다름</para>
</listitem>
</itemizedlist>
<para>기본 설치가 완료되면 복제를 위해 컴패니언 데이터베이스를 준비해야 합니다. 우선 <emphasis role="strong">sa</emphasis> 사용자의 잠금을 풀어야 합니다.</para>
<screen># su - syb&lt;sid&gt;
# isql -Usapsso -P &lt;secure password&gt; -S&lt;SID&gt; -X -w1900
# 1&gt; go
# 1&gt; exec sp_locklogin sa, 'unlock'
# 2&gt; go
# Account unlocked.
# (return status = 0)
# 1&gt; quit</screen>
<para>다음 단계에서는 컴패니언 사이트에 응답 파일과 함께 SRS 소프트웨어를 설치하고 syb&lt;sid&gt; 사용자로 다음 명령을 입력합니다.</para>
<screen># /sapcd/ase-16.0.03.06/BD_SYBASE_ASE_16.0.03.06_RDBMS_for_BS_/SYBASE_LINUX_X86_64/setup.bin -f /sybase/HA7/srs-setup.txt -i silent</screen>
<para>응답 파일과 함께 컴패니언 노드에 HADR을 활성화하고 syb&lt;sid&gt; 사용자로 다음 명령을 입력합니다.</para>
<screen># setuphadr /sybase/HA7/HA7_companion_lin.rs.txt</screen>
<note>
<para>특정 환경에서는 설치가 실패할 수 있습니다. 이 경우에는 기본 시스템을 다시 설정하고 그 후에 컴패니언을 설치하십시오.</para>
</note>
<note>
<para>시스템을 다시 설치하고 컴패니언 시스템에서 이 디렉토리 <emphasis>/tmp/.SQLAnywhere</emphasis>에 대한 <emphasis role="strong">읽기/쓰기 권한 없음</emphasis>을 표시하면 두 노드에서 권한을 선택합니다. 소유권을 변경해야 하는 경우에는 두 노드에서 설정을 다시 실행합니다. <emphasis role="strong">Primary</emphasis>로 시작하십시오.</para>
</note>
<para>데이터베이스를 위한 보안 스토어 키를 만듭니다.</para>
<screen># /usr/sap/hostctrl/exe/saphostctrl -user sapadm &lt;secure password&gt; -function LiveDatabaseUpdate -dbname &lt;SID&gt; -dbtype syb -dbuser DR_admin -dbpass &lt;Secure password&gt; -updatemethod Execute -updateoption TASK=SET_USER_PASSWORD -updateoption USER=DR_ADMIN</screen>
</section>
<section xml:id="id-fault-manager-installation">
<title>Fault Manager 설치</title>
<para>Fault Manager는 ASCS 호스트에 구성됩니다. 이러한 설정의 장점은 ASCS / ERS 복제용 기존 pacemaker를 사용하여 sybdbfm 서비스를 모니터링 및 추적할 수 있다는 점입니다.</para>
<screen># su - &lt;sid&gt;adm
# cd /usr/sap/&lt;SID&gt;/ASCS00/exe/
# sybdbfm install</screen>
<example>
<title>Fault Manager 설치:</title>
<screen>replication manager agent user DR_admin and password set in Secure Store.
Keep existing values (yes/no)? (yes)
SAPHostAgent connect user sapadm and password set in Secure Store.
Keep existing values (yes/no)? (yes)
Enter value for primary database host: (suse7cl1)
suse7db1
Enter value for primary database name: (HA7)
Enter value for primary database port: (4901)
Enter value for primary site name: (Site1)
Enter value for primary database heart beat port: (13777)
Enter value for standby database host: (suse7cl1)
suse7db2
Enter value for standby database name: (HA7)
Enter value for standby database port: (4901)
Enter value for standby site name : (Site2)
Enter value for standby database heart beat port: (13787)
Enter value for fault manager host: (s7as-service)
Enter value for heart beat to heart beat port: (13797)
Enter value for support for floating database ip: (no)
Enter value for use SAP ASE Cockpit if it is installed and running: (no)</screen>
</example>
<para>환경에 따라 기본 DB 및 컴패니언 DB 호스트 이름, SID &amp; 사이트 이름의 값을 업데이트하십시오. ASCS 호스트에 가상 호스트 이름을 사용하십시오. Fault Manager가 설치될 때, <emphasis>/sapmnt/&lt;SID&gt;/profile</emphasis>에 <emphasis>SYBHA.PFL</emphasis>이라는 이름으로 프로파일이 생성되고 구성 세부 정보가 입력됩니다. ASCS 인스턴스를 다시 시작합니다. 그러면 아래와 같이 시작 프로파일에 추가된 Fault Manager도 시작됩니다.</para>
<note>
<para>재설치이고 기존 값을 100% 알지 못하는 경우에는 <emphasis>sapadm</emphasis> 및 <emphasis>DR_admin</emphasis>을 위한 보안 스토어에서 기존 사용자 이름 및 비밀번호를 덮어쓰는 것이 좋습니다.</para>
</note>
<screen># cat /sapmnt/&lt;SID&gt;/profile/&lt;SID&gt;_ASCS00_&lt;virt. ASCS hostname&gt;
....
#-----------------------------------------------------------------------
# copy sybdbfm and dependent
#-----------------------------------------------------------------------
_CP_SYBDBFM_ARG1 = list:$(DIR_CT_RUN)/instancedb.lst
Execute_06 = immediate $(DIR_CT_RUN)/sapcpe$(FT_EXE) pf=$(_PF) $(_CP_SYBDBFM_ARG1)
_CP_SYBDBFM_ARG2 = list:$(DIR_GLOBAL)/syb/linuxx86_64/cpe_sybodbc.lst
_CP_SYBDBFM_ARG3 = source:$(DIR_GLOBAL)/syb/linuxx86_64/sybodbc
Execute_07 = immediate $(DIR_CT_RUN)/sapcpe$(FT_EXE) pf=$(_PF) $(_CP_SYBDBFM_ARG2) $(_CP_SYBDBFM_ARG3)
#-----------------------------------------------------------------------
# Start sybha
#-----------------------------------------------------------------------
_SYBHAD = sybdbfm.sap$(SAPSYSTEMNAME)_$(INSTANCE_NAME)
_SYBHA_PF = $(DIR_PROFILE)/SYBHA.PFL
Execute_08 = local rm -f $(_SYBHAD)
Execute_09 = local ln -s -f $(DIR_EXECUTABLE)/sybdbfm$(FT_EXE) $(_SYBHAD)
Restart_Program_02 = local $(_SYBHAD) hadm pf=$(_SYBHA_PF)
#-----------------------------------------------------------------------
....</screen>
<para>FM의 상태는 아래를 수행하여 확인할 수 있습니다. ASCS 작업 디렉토리로 이동한 후 <emphasis>sybdbfm.sap.&lt;SID&gt;_ASCS&lt;instance number&gt; status</emphasis>를 실행합니다.</para>
<para><emphasis>ha1adm 사용자로</emphasis></para>
<screen># cd /usr/sap/&lt;SID&gt;/ASCS&lt;instance number&gt;/work
# ./sybdbfm.sap&lt;SID&gt;_ASCS&lt;instance number&gt; status

fault manager running, pid = 23234, fault manager overall status = OK, currently executing in mode PAUSING
<emphasis role="strong">sanity check report (208)</emphasis>.
node 1: server &lt;DB server1&gt;, site &lt;site name one&gt;.
db host status: OK.
db status OK hadr status PRIMARY.
node 2: server &lt;DB server2&gt;, site &lt;site name two&gt;.
db host status: OK.
db status OK hadr status STANDBY.
replication status: SYNC_OK.</screen>
<para>로그 파일을 확인하는 것도 상태를 확인하는 적절한 방법입니다.</para>
<para><emphasis>ha1adm 사용자로</emphasis></para>
<screen># cd /usr/sap/&lt;SID&gt;/ASCS&lt;instance number&gt;/work
# tail -f dev_sybdbfm
# ...

2019 09/11 15:34:30.523 (23234) ----- Log messages ----

2019 09/11 15:34:30.523 (23234) Info: saphostcontrol: Executing LiveDatabaseUpdate

2019 09/11 15:34:30.523 (23234) Info: saphostcontrol: LiveDatabaseUpdate successfully executed

2019 09/11 15:34:30.524 (23234) call is running.
2019 09/11 15:34:30.534 (23234) call exited (exit code 0).
2019 09/11 15:34:30.534 (23234) db status is:
 DB_OK.
2019 09/11 15:34:42.561 (23234) *** sanity check report (136)***.
2019 09/11 15:34:42.562 (23234) node 1: server &lt;DB server1&gt;, site &lt;site name one&gt;.
2019 09/11 15:34:42.562 (23234) db host status: OK.
2019 09/11 15:34:42.562 (23234) db status OK hadr status PRIMARY.
2019 09/11 15:34:42.562 (23234) node 2: server &lt;DB server2&gt;, site &lt;site name two&gt;.
2019 09/11 15:34:42.562 (23234) db host status: OK.
2019 09/11 15:34:42.562 (23234) db status OK hadr status STANDBY.
2019 09/11 15:34:42.562 (23234) replication status: SYNC_OK.
2019 09/11 15:34:57.688 (23234) *** sanity check report (137)***.
2019 09/11 15:34:57.688 (23234) node 1: server &lt;DB server1&gt;, site &lt;site name one&gt;.
2019 09/11 15:34:57.688 (23234) db host status: OK.
2019 09/11 15:34:57.688 (23234) db status OK hadr status PRIMARY.
2019 09/11 15:34:57.688 (23234) node 2: server &lt;DB server2&gt;, site &lt;site name two&gt;.
2019 09/11 15:34:57.688 (23234) db host status: OK.
2019 09/11 15:34:57.688 (23234) db status OK hadr status STANDBY.
2019 09/11 15:34:57.688 (23234) replication status: SYNC_OK.
2019 09/11 15:35:12.827 (23234) *** sanity check report (138)***.
2019 09/11 15:35:12.827 (23234) node 1: server &lt;DB server1&gt;, site &lt;site name one&gt;.
2019 09/11 15:35:12.827 (23234) db host status: OK.
2019 09/11 15:35:12.827 (23234) db status OK hadr status PRIMARY.
2019 09/11 15:35:12.827 (23234) node 2: server &lt;DB server2&gt;, site &lt;site name two&gt;.
2019 09/11 15:35:12.827 (23234) db host status: OK.
2019 09/11 15:35:12.827 (23234) db status OK hadr status STANDBY.
2019 09/11 15:35:12.827 (23234) replication status: SYNC_OK.
# ...</screen>
<para>장애 조치를 수행하기 위해 <emphasis>SYBHA.PFL</emphasis>에서 업데이트해야 하는 몇 가지 파라미터는 아래와 같습니다.</para>
<screen>ha/syb/support_cluster = 1
ha/syb/failover_if_unresponsive = 1
ha/syb/allow_restart_companion = 1
ha/syb/set_standby_available_after_failover = 1
ha/syb/chk_restart_repserver = 1
ha/syb/cluster_fmhost1 = <emphasis role="strong">Hostname for Node 1 of the ASCS HA Setup</emphasis>
ha/syb/cluster_fmhost2 = <emphasis role="strong">Hostname for Node 2 of the ASCS HA Setup</emphasis>
ha/syb/use_boot_file_always = 1
ha/syb/dbfmhost = <emphasis role="strong">virtual hostname of ASCS instance</emphasis></screen>
<para>모든 FM 파라미터에 대한 세부 정보는 <emphasis role="strong">SAP ASE HA DR 사용자 가이드</emphasis>에서 찾아볼 수 있습니다. 굵게 표시된 항목이 설정에서 사용됩니다. FM은 노드 1에서 노드 2로 장애 조치할 수 있는 ASCS와 함께 설치되므로, <emphasis>ha/syb/cluster_fmhost1</emphasis> 및 <emphasis>ha/syb/cluster_fmhost2</emphasis> 파라미터는 FM 호스트가 실행될 수 있는 두 노드의 물리적 호스트 이름을 제공합니다.</para>
<para>전체 가용 영역(AZ1)이 중단되고 ASCS 및 기본 데이터베이스가 실행되는 시나리오에서는 ASCS 장애 조치가 완료되고 FM이 두 번째 가용 영역(AZ2)에서 시작되어 실행될 때까지 DB 장애조치가 트리거되지 않습니다. 이후에 FM은 부팅 파일을 읽어 ASE DB의 이전 상태를 가져와야 합니다. 이는 FM이 장애 조치를 올바르게 트리거하기 위해 필수입니다. <emphasis>ha/syb/use_boot_file_always=1</emphasis> 파라미터는 FM이 항상 작업 디렉토리(ASCS 및 FM에서 동일)의 일부인 부팅 파일에서 읽고 FM과 함께 장애 조치를 수행하도록 합니다.</para>
</section>
</section>
<section xml:id="id-cluster-integration-of-fault-manager">
<title>Fault Manager의 클러스터 통합</title>
<para>pacemaker 환경에서 FM은 ASCS 인스턴스의 일부로 구현됩니다.</para>
<example>
<title>FM은 ASCS와 함께 제공되는 포함 서비스로 통합됩니다.</title>
<para><emphasis>primitive rsc_sap<emphasis>_</emphasis>&lt;SID&gt;</emphasis><emphasis>_ASCS&lt;instance number&gt;</emphasis>를 위한 클러스터 구성을 다음과 같이 수정해야 합니다. 예시에서 사용되는 항목은 다음과 같습니다.</para>
<itemizedlist>
<listitem>
<para>&lt;SID&gt; ⇒ HA1</para>
</listitem>
<listitem>
<para>&lt;instance number&gt; ⇒ 00</para>
</listitem>
<listitem>
<para>가상 호스트 이름 ⇒ sapha1as</para>
</listitem>
</itemizedlist>
<screen># crm configure edit rsc_sap_HA1_ASCS00</screen>
<screen>primitive rsc_sap_HA1_ASCS00 SAPInstance \
        operations $id=rsc_sap_HA1_ASCS00-operations \
        op monitor interval=11 timeout=60 on-fail=restart \
        params InstanceName=HA1_ASCS00_sapha1as START_PROFILE="/sapmnt/HA1/profile/HA1_ASCS00_sapha1as" \
        AUTOMATIC_RECOVER=false MONITOR_SERVICES="sybdbfm|msg_server|enserver" \
        meta resource-stickiness=5000 failure-timeout=60 migration-threshold=1 priority=10</screen>
<para>FM 서비스는 기본으로 감시되는 SAP 인스턴스 서비스의 일부가 아닙니다. <emphasis role="strong">MONITOR_SERVICES</emphasis>를 지정하면 모든 기본 설정을 명명된 서비스로 덮어씁니다. 즉, <emphasis>sapcontrol -nr 00 -function GetProcessList</emphasis> 명령의 결과로 표시되는 모든 서비스의 수를 계산해야 합니다. 위의 예는 ENSA1 구성용입니다.</para>
<note>
<para>클러스터 구성은 ENSA1 및 ENS2 설치에서 다릅니다. MONITOR_SERVICES의 이름은 이 두 버전에서 다릅니다.</para>
</note>
</example>
</section>
<section xml:id="id-operating-a-pacemaker-controlled-and-fm-monitored-ase-replication-setup">
<title>Pacemaker 제어 및 FM 모니터링 ASE 복제 설정 작동</title>
<para>FM용 pacemaker가 통합된 FM 제어 ASE 복제 설정을 구현하려면 몇 가지 특수 규칙을 따라야 합니다. 우선 복제 및 FM 자체의 상태를 확인하는 방법에 대한 세부 정보가 필요합니다. 이 장에서는 인수 시간 향상 방법 및 해당 환경의 제어 방법에 대한 지침도 제공됩니다.</para>
<example>
<title>데이터베이스 환경의 상태 확인</title>
<para>상태를 확인하고 기본 사이트를 찾습니다. ASCS 호스트에서 <emphasis>ha1adm 사용자로</emphasis></para>
<screen># cd /usr/sap/&lt;SID&gt;/ASCS&lt;instance nr&gt;/work
# ./sybdbfm.sap&lt;SID&gt;_ASCS&lt;instance nr&gt; status</screen>
<para>그리고 <emphasis>dev_sybdbfm</emphasis> 로그 파일을 확인합니다.</para>
<screen>2019 09/13 09:58:52.200 (3290) *** sanity check report (2)***.
2019 09/13 09:58:52.200 (3290) node 1: server sapdb1, site LeonRot.
2019 09/13 09:58:52.200 (3290) db host status: OK.
2019 09/13 09:58:52.200 (3290) db status OK hadr status STANDBY.
2019 09/13 09:58:52.200 (3290) node 2: server sapdb2, site Orlando.
2019 09/13 09:58:52.201 (3290) db host status: OK.
2019 09/13 09dbs_syb_server sapdb1:sapdb2:58:52.201 (3290) db status OK hadr status PRIMARY.
2019 09/13 09:58:52.201 (3290) replication status: SYNC_OK.</screen>
<para>데이터베이스 호스트에서 <emphasis>root 사용자로</emphasis></para>
<screen># /usr/sap/hostctrl/exe/saphostctrl -user sapadm &lt;secure password&gt; -dbname &lt;SID&gt; -dbtype syb -function GetDatabaseSystemStatus
# /usr/sap/hostctrl/exe/saphostctrl -user sapadm &lt;secure password&gt; -dbname &lt;SID&gt; -dbtype syb -function GetDatabaseStatus
# /usr/sap/hostctrl/exe/saphostctrl -user sapadm &lt;secure password&gt; -dbname &lt;SID&gt; -dbtype syb -function LiveDatabaseUpdate -updatemethod Check -updateoption TASK=REPLICATION_STATUS</screen>
<para>데이터베이스 호스트에서 <emphasis>syb&lt;sid&gt; 사용자로</emphasis></para>
<screen>#  isql -UDR_admin -P &lt;secure password&gt; -S&lt;db host&gt;:4909 -X -w 1000
1&gt; sap_status active_path
2&gt; go</screen>
</example>
<para>DB 장애 조치 상황(인수)에 맞춰 애플리케이션 서버(PAS 및 AAS) 환경을 조정해야 합니다. Dialog 서버(PAS, AAS)를 제공하는 각 호스트에서 <emphasis>.dbenv.sh</emphasis> 및/또는 <emphasis>.dbenv.csh</emphasis> 파일을 확장해야 합니다.</para>
<example>
<title>Dialog Server에서 DB 환경 설정 수정</title>
<para>Dialog 애플리케이션 서버를 실행하는 각 호스트에서 아래와 같이 누락된 값을 추가하고 설정을 확장합니다.</para>
<para><emphasis>ha1adm 사용자로</emphasis></para>
<screen># vi .dbenv.csh
...
setenv dbs_syb_server &lt;server1:server2&gt;
setenv dbs_syb_ha 1
...</screen>
<para><emphasis>ha1adm 사용자로</emphasis></para>
<screen># vi .dbenv.sh
...
dbs_syb_server=&lt;server1:server2&gt;
export dbs_syb_server
dbs_syb_ha=1
export dbs_syb_ha
...</screen>
</example>
<important>
<para>인스턴스를 다시 시작하여 변경 사항을 적용해야 합니다.</para>
</important>
<example>
<title>기본 DB 호스트 중단 후 대응 시간을 향상하기 위한 OS 설정</title>
<para>기본 tcp_retries 값은 높음이며 이로 인해 인수 시간이 매우 오래 걸리게 됩니다. ASE16 PL7으로 동작이 수정됩니다. 이 패치까지는 아래와 같이 변경하여 인수 시간을 향상할 수 있습니다.</para>
<para><emphasis>root 사용자로</emphasis></para>
<screen># echo 3 &gt;/proc/sys/net/ipv4/tcp_retries2
## makes the changes online
# vi /etc/sysctl.conf
...
net.ipv4.tcp_retries2 = 3
...
## makes the changes reboot persistent</screen>
</example>
<example>
<title>복제 모드에서 SAP 시스템 및 데이터베이스 시작 및 중지</title>
<para>Fault Manager가 기본 및 컴패니언 데이터베이스를 모니터링하고 Pacemaker에서 Fault Manager를 모니터링하는 경우에는 시스템을 시작 및 중지하기 위한 특별한 절차가 필요합니다.</para>
<itemizedlist>
<title>일반적으로 이러한 단계는 시스템을 <emphasis role="strong">시작</emphasis>하는 데 중요합니다.</title>
<listitem>
<para>컴패니언 데이터베이스 + 복제 서버 시작</para>
</listitem>
<listitem>
<para>기본 데이터베이스 + 복제 서버 시작</para>
</listitem>
<listitem>
<para>클러스터 유지보수 모드를 false로 변경</para>
<itemizedlist>
<listitem>
<para>FM과 함께 ASCS 시작(자동)</para>
</listitem>
<listitem>
<para>ERS 시작(자동)</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>PAS 및 AAS 인스턴스 시작</para>
</listitem>
<listitem>
<para>선택 사항: SAP 시스템을 수동으로 시작한 경우에는 클러스터 유지보수 모드를 해제합니다.</para>
<itemizedlist>
<listitem>
<para>파일 시스템을 마운트하고 IP를 수동으로 설정해야 합니다.</para>
</listitem>
<listitem>
<para><emphasis>&lt;sid&gt;adm</emphasis> 사용자로 <emphasis>sapcontrol -nr &lt;instance number&gt; -function StartSystem</emphasis></para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>컴패니언 데이터베이스 호스트에서 <emphasis>root 사용자로</emphasis></para>
<screen># /usr/sap/hostctrl/exe/saphostctrl -function StartDatabase -dbname &lt;SID&gt; -dbtype syb
# /usr/sap/hostctrl/exe/saphostctrl -function StartDatabase -dbname &lt;SID&gt;_REP -dbtype syb</screen>
<para>기본 데이터베이스 호스트에서 <emphasis>root 사용자로</emphasis></para>
<screen># /usr/sap/hostctrl/exe/saphostctrl -function StartDatabase -dbname &lt;SID&gt; -dbtype syb
# /usr/sap/hostctrl/exe/saphostctrl -function StartDatabase -dbname &lt;SID&gt;_REP -dbtype syb</screen>
<para>ASCS 및 ERS용 Pacemaker 호스트 중 하나에서 <emphasis>root 사용자로</emphasis></para>
<screen># crm configure property maintenance-mode=false</screen>
<para>PAS 또는 AAS용 호스트에서 <emphasis>&lt;sid&gt;adm 사용자로</emphasis></para>
<screen># sapcontrol -nr &lt;instance number&gt; -function StartSystem</screen>
<note>
<para>시스템을 하나씩 시작해야 하는 경우에는 <emphasis>sapcontrol -nr &lt;instance number&gt; -function StartSystem</emphasis> 명령을 사용합니다. 방향은 ASCS, ERS, PAS, AAS여야 합니다.</para>
</note>
<itemizedlist>
<title>일반적으로 이러한 단계는 시스템을 <emphasis role="strong">중지</emphasis>하는 데 중요합니다.</title>
<listitem>
<para>클러스터 유지보수 모드를 <emphasis>true</emphasis>로 설정</para>
</listitem>
<listitem>
<para>PAS 및 AAS 인스턴스 중지</para>
</listitem>
<listitem>
<para>FM과 함께 ASCS 중지</para>
</listitem>
<listitem>
<para>ERS 중지</para>
</listitem>
<listitem>
<para>기본 데이터베이스 + 복제 서버 중지</para>
</listitem>
<listitem>
<para>컴패니언 데이터베이스 + 복제 서버 중지</para>
</listitem>
</itemizedlist>
<para><emphasis>root 사용자로</emphasis></para>
<screen># crm configure property maintenance-mode=true
# crm status</screen>
<para>ASCS 및 ERS 또는 PAS/AAS용 Pacemaker 호스트 중 하나에서 <emphasis>&lt;sid&gt;adm 사용자로</emphasis></para>
<screen># sapcontrol -nr &lt;instance number&gt; -function StopSystem</screen>
<note>
<para>시스템을 하나씩 중지해야 하는 경우에는 각 인스턴스 호스트에서 <emphasis>sapcontrol -nr &lt;instance number&gt; -function Stop</emphasis> 명령을 사용합니다. 방향은 AAS, PAS, ASCS, ERS여야 합니다.</para>
</note>
<para>기본 데이터베이스 호스트에서 <emphasis>root 사용자로</emphasis></para>
<screen># /usr/sap/hostctrl/exe/saphostctrl -function StopDatabase -dbname &lt;SID&gt; -dbtype syb
# /usr/sap/hostctrl/exe/saphostctrl -function StopDatabase -dbname &lt;SID&gt;_REP -dbtype syb</screen>
<para>컴패니언 데이터베이스 호스트에서 <emphasis>root 사용자로</emphasis></para>
<screen># /usr/sap/hostctrl/exe/saphostctrl -function StopDatabase -dbname &lt;SID&gt; -dbtype syb
# /usr/sap/hostctrl/exe/saphostctrl -function StopDatabase -dbname &lt;SID&gt;_REP -dbtype syb</screen>
<important>
<para>Pacemaker 제어 서버를 적절한 방법으로 중지해야 합니다. 구현된 stonith 방법에 따라 다른 절차를 이용할 수 있습니다.</para>
</important>
<para>한 클러스터 노드에서 <emphasis>root 사용자로</emphasis></para>
<screen># crm cluster run "crm cluster stop"</screen>
<para>각 노드에서 <emphasis>root 사용자로</emphasis></para>
<screen># reboot
## or
# poweroff</screen>
</example>
<section xml:id="id-testing-the-replication-and-fm-cluster-integration">
<title>복제 및 FM 클러스터 통합 테스트</title>
<para>각 고가용성 솔루션에서 중요한 것은 포괄적인 테스트 절차입니다. 이를 통해 오류 발생 시 솔루션이 예상된 대로 작동할 수 있습니다.</para>
<example>
<title>FM이 작동하는 경우 데이터베이스 장애 조치 및 모니터링 트리거</title>
<para>상태를 확인하고 기본 사이트를 찾습니다. ASCS 호스트에서 <emphasis>ha1adm 사용자로</emphasis></para>
<screen># cd /usr/sap/&lt;SID&gt;/ASCS&lt;instance nr&gt;/work
# ./sybdbfm.sap&lt;SID&gt;_ASCS&lt;instance nr&gt; status</screen>
<para>그리고 <emphasis>dev_sybdbfm</emphasis> 로그 파일을 확인합니다.</para>
<screen>2019 09/13 09:58:52.200 (3290) *** sanity check report (2)***.
2019 09/13 09:58:52.200 (3290) node 1: server sapdb1, site LeonRot.
2019 09/13 09:58:52.200 (3290) db host status: OK.
2019 09/13 09:58:52.200 (3290) db status OK hadr status STANDBY.
2019 09/13 09:58:52.200 (3290) node 2: server sapdb2, site Orlando.
2019 09/13 09:58:52.201 (3290) db host status: OK.
2019 09/13 09dbs_syb_server sapdb1:sapdb2:58:52.201 (3290) db status OK hadr status PRIMARY.
2019 09/13 09:58:52.201 (3290) replication status: SYNC_OK.</screen>
<itemizedlist>
<listitem>
<para>이제 기본 데이터베이스 서버를 제거합니다.</para>
</listitem>
<listitem>
<para>ASCS 호스트의 FM에서 인수 프로세스를 모니터링합니다.</para>
</listitem>
</itemizedlist>
<para>ASCS 호스트에서 <emphasis>ha1adm 사용자로</emphasis></para>
<screen># cd /usr/sap/&lt;SID&gt;/ASCS&lt;instance nr&gt;/work
# tail -f  dev_sybdbfm</screen>
</example>
<example>
<title>인수 프로세스에서 선택된 출력</title>
<screen>...
    2019 09/13 11:08:38.301 (3290) <emphasis role="strong"> </emphasis>* sanity check report (270)<emphasis role="strong">* </emphasis>.
    2019 09/13 11:08:38.301 (3290) node 1: server sapdb1, site LeonRot.
    2019 09/13 11:08:38.301 (3290) db host status: OK.
    2019 09/13 11:08:38.301 (3290) db status OK hadr status STANDBY.
    2019 09/13 11:08:38.301 (3290) node 2: server sapdb2, site Orlando.
    2019 09/13 11:08:38.301 (3290) db host status: OK.
    2019 09/13 11:08:38.301 (3290) db status OK hadr status PRIMARY.
    2019 09/13 11:08:38.301 (3290) replication status: SYNC_OK.
    2019 09/13 11:08:50.416 (3290) ERROR in function SimpleFetch (1832) (SQLExecDirect failed): (30046) [08S01] [SAP][ASE ODBC Driver]Connection to the server has been lost. Unresponsive Connection was disconnected during command timeout. Check the server to determine the status of any open transactions.
    2019 09/13 11:08:50.416 (3290) ERROR in function SimpleFetch (1832) (SQLExecDirect failed): (30149) [HYT00] [SAP][ASE ODBC Driver]The command has timed out.
    2019 09/13 11:08:50.416 (3290) execution of statement master..sp_hadr_admin get_request, '1' failed.
    2019 09/13 11:08:50.416 (3290) ERROR in function SimpleFetch (1824) (SQLAllocStmt failed): (30102) [HY010] [SAP][ASE ODBC Driver]Function sequence error
    2019 09/13 11:08:50.416 (3290) execution of statement select top 1 convert( varchar(10), @@hadr_mode ) || ' ' || convert( varchar(10), @@hadr_state ) from sysobjects failed.
    2019 09/13 11:08:50.416 (3290) disconnect connection
    2019 09/13 11:09:22.505 (3290) ERROR in function SQLConnectWithRetry (1341) (SQLConnectWithRetry failed): (30293) [HY000] [SAP][ASE ODBC Driver]The socket failed to connect within the timeout specified.
    2019 09/13 11:09:22.505 (3290) ERROR in function SQLConnectWithRetry (1341) (SQLConnectWithRetry failed): (30012) [08001] [SAP][ASE ODBC Driver]Client unable to establish a connection
    2019 09/13 11:09:22.505 (3290) connected with warnings (555E69805100)
    2019 09/13 11:09:22.505 (3290) ERROR in function SimpleFetch (1824) (SQLAllocStmt failed): (30293) [HY000] [SAP][ASE ODBC Driver]The socket failed to connect within the timeout specified.
    2019 09/13 11:09:22.505 (3290) ERROR in function SimpleFetch (1824) (SQLAllocStmt failed): (30012) [08001] [SAP][ASE ODBC Driver]Client unable to establish a connection
    2019 09/13 11:09:22.505 (3290) execution of statement select top 1 convert( varchar(10), @@hadr_mode ) || ' ' || convert( varchar(10), @@hadr_state ) from sysobjects failed.
    2019 09/13 11:09:22.505 (3290) disconnect connection
    2019 09/13 11:09:22.505 (3290) primary site unusable.
...
    2019 09/13 11:09:22.984 (3290) primary site unusable.
    2019 09/13 11:09:22.984 (3290) <emphasis role="strong"> </emphasis>* sanity check report (271)<emphasis role="strong">* </emphasis>.
    2019 09/13 11:09:22.984 (3290) node 1: server sapdb1, site LeonRot.
    2019 09/13 11:09:22.984 (3290) db host status: OK.
    2019 09/13 11:09:22.984 (3290) db status OK hadr status STANDBY.
    2019 09/13 11:09:22.984 (3290) node 2: server sapdb2, site Orlando.
    2019 09/13 11:09:22.984 (3290) db host status: UNUSABLE.
    2019 09/13 11:09:22.984 (3290) db status DB INDOUBT hadr status UNREACHABLE.
    2019 09/13 11:09:22.984 (3290) replication status: SYNC_OK.
    2019 09/13 11:09:23.047 (3290) doAction: Primary database is declared dead or unusable.
    2019 09/13 11:09:23.047 (3290) disconnect connection
    2019 09/13 11:09:23.047 (3290) database host cannot be reached.
    <emphasis role="strong">2019 09/13 11:09:23.047 (3290) doAction: failover.</emphasis>
...
    2019 09/13 11:11:55.497 (3290) <emphasis role="strong"> </emphasis>* sanity check report (273)<emphasis role="strong">* </emphasis>.
    2019 09/13 11:11:55.497 (3290) node 1: server sapdb1, site LeonRot.
    2019 09/13 11:11:55.497 (3290) db host status: OK.
    <emphasis role="strong">2019 09/13 11:11:55.497 (3290) db status OK hadr status PRIMARY.</emphasis>
    2019 09/13 11:11:55.497 (3290) node 2: server sapdb2, site Orlando.
    2019 09/13 11:11:55.497 (3290) db host status: UNUSABLE.
    2019 09/13 11:11:55.498 (3290) db status DB INDOUBT hadr status UNREACHABLE.
    2019 09/13 11:11:55.498 (3290) replication status: UNKNOWN.
    2019 09/13 11:11:55.555 (3290) doAction: Standby database is declared dead or unusable.
    2019 09/13 11:11:55.555 (3290) disconnect connection
    <emphasis role="strong">2019 09/13 11:11:55.555 (3290) doAction: Companion db host is declared unusable.</emphasis>
    2019 09/13 11:11:55.555 (3290) doAction: no action defined.
    2019 09/13 11:11:58.568 (3290) Error: NIECONN_REFUSED (No route to host), NiRawConnect failed in plugin_fopen()
...
<emphasis role="marked"><emphasis role="marked"/><emphasis role="marked"/></emphasis><emphasis role="marked"> host is coming back online </emphasis><emphasis role="marked"><emphasis role="marked"/><emphasis role="marked"/></emphasis><emphasis role="marked">#</emphasis>#
    2019 09/13 11:18:45.579 (3290) call is running.
    2019 09/13 11:18:45.589 (3290) call exited (exit code 0).
    2019 09/13 11:18:45.589 (3290) db status is: DB_OK.
    2019 09/13 11:18:45.589 (3290) doAction: Standby database is declared dead or unusable.
    2019 09/13 11:18:45.589 (3290) disconnect connection
    <emphasis role="strong">2019 09/13 11:18:45.589 (3290) doAction: Companion db host is declared ok.</emphasis>
    <emphasis role="strong">2019 09/13 11:18:45.589 (3290) doAction: restart database.</emphasis>
    2019 09/13 11:18:45.805 (3290) Webmethod returned successfully
...
    2019 09/13 11:22:43.677 (3290) <emphasis role="strong"> </emphasis>* sanity check report (286)<emphasis role="strong">* </emphasis>.
    2019 09/13 11:22:43.677 (3290) node 1: server sapdb1, site LeonRot.
    2019 09/13 11:22:43.677 (3290) db host status: OK.
    2019 09/13 11:22:43.677 (3290) db status OK hadr status PRIMARY.
    2019 09/13 11:22:43.677 (3290) node 2: server sapdb2, site Orlando.
    2019 09/13 11:22:43.677 (3290) db host status: OK.
    <emphasis role="strong">2019 09/13 11:22:43.677 (3290) db status OK hadr status STANDBY.</emphasis>
    <emphasis role="strong">2019 09/13 11:22:43.677 (3290) replication status: SYNC_OK.</emphasis>
...</screen>
<para><emphasis>root 사용자로</emphasis></para>
<screen># /usr/sap/hostctrl/exe/saphostctrl -user sapadm &lt;secure password&gt; -dbname &lt;SID&gt; -dbtype syb -function LiveDatabaseUpdate -updatemethod Check -updateoption TASK=REPLICATION_STATUS

Webmethod returned successfully
Operation ID: 5254001F87CB1EE9B5C34755C99DDDFA

----- Response data ----
TASK_NAME=REPLICATION_STATUS
REPLICATION_STATUS=active
PRIMARY_SITE=&lt;site1&gt;
STANDBY_SITE=&lt;site2&gt;
REPLICATION_MODE=sync
ASE transaction log backlog (MB)=0
Replication queue backlog (MB)=0
TASK_STATUS=OK
----- Log messages ----
Info: saphostcontrol: Executing LiveDatabaseUpdate
Info: saphostcontrol: LiveDatabaseUpdate successfully executed</screen>
</example>
<example>
<title>FM 오류 트리거</title>
<para>FM 프로세스를 6회 이상 종료하면 pacemaker가 작동합니다. 5회까지는 saphostagent가 SAP 프로세스를 관리합니다. 특정 기간 내에 실패 횟수에 도달하면 서비스가 다시 시작되지 않습니다.</para>
<para><emphasis>ha1adm 사용자로</emphasis></para>
<screen># pkill -9 sybdbfm
## check that the PID has changed
# sapcontrol -nr 00 -function GetProcessList
# pkill -9 sybdbfm
...
# sapcontrol -nr 00 -function GetProcessList
...
sybdbfm, , GRAY, Stopped, , , 11154
...</screen>
<para>이제 pacemaker가 우선 로컬에서 FM 인스턴스를 다시 시작합니다. <emphasis>root 사용자로</emphasis></para>
<screen># crm_mon -1rfn
...
Migration Summary:
* Node &lt;hostname&gt;:
rsc_sap_WAS_ASCS00: migration-threshold=3 fail-count=1 last-failure='Fri Sep 13 13:46:39 2019
...</screen>
<note>
<para><emphasis role="strong">fail-count</emphasis>가 정의된 임계값에 도달하면 ASCS가 해당 호스트에서 이동됩니다.</para>
</note>
</example>
</section>
</section>
</section>
<section xml:id="id-appendix">
<title>부록</title>
<section xml:id="id-crm-configuration">
<title>CRM 구성</title>
<para>SAP 시스템 HA1용 전체 crm 구성</para>
<screen>#
# nodes
#
node 1084753931: hacert01
node 1084753932: hacert02
#
# primitives for ASCS and ERS
#
primitive res_AWS_STONITH stonith:external/ec2 \
        op start interval=0 timeout=180 \
        op stop interval=0 timeout=180 \
        op monitor interval=120 timeout=60 \
        params tag=pacemaker profile=cluster
primitive rsc_fs_HA1_ASCS00 Filesystem \
  params device="efs-name:/ASCS00" \
         directory="/usr/sap/HA1/ASCS00" fstype=nfs4 \
         options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
        op start timeout=60s interval=0 \
        op stop timeout=60s interval=0 \
        op monitor interval=200s timeout=40s
primitive rsc_fs_HA1_ERS10 Filesystem \
  params device="efs-name:/ERS10" \
    directory="/usr/sap/HA1/ERS10" fstype=nfs4 \
    options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
        op start timeout=60s interval=0 \
        op stop timeout=60s interval=0 \
        op monitor interval=20s timeout=40s
primitive rsc_ip_HA1_ASCS00 ocf:suse:aws-vpc-move-ip \
        params address=192.168.201.116 routing_table=rtb-table-name \
        interface=eth0 profile=cluster \
        op start interval=0 timeout=180 \
        op stop interval=0 timeout=180 \
        op monitor interval=120 timeout=60
primitive rsc_ip_HA1_ERS10 ocf:suse:aws-vpc-move-ip \
        params address=192.168.201.117 routing_table=rtb-table-name \
        interface=eth0 profile=cluster \
        op start interval=0 timeout=180 \
        op stop interval=0 timeout=180 \
        op monitor interval=120 timeout=60
primitive rsc_r53_HA1_ASCS00 aws-vpc-route53 \
        params hostedzoneid=hosted-zone-id ttl=10 \
        fullname=full-name profile=cluster \
        op start interval=0 timeout=180 \
        op stop interval=0 timeout=180 \
        op monitor interval=300 timeout=180
primitive rsc_sap_HA1_ASCS00 SAPInstance \
        operations $id=rsc_sap_HA1_ASCS00-operations \
        op monitor interval=120 timeout=60 on-fail=restart \
        params InstanceName=HA1_ASCS00_sapha1as \
     START_PROFILE="/sapmnt/HA1/profile/HA1_ASCS00_sapha1as" \
     AUTOMATIC_RECOVER=false \
        meta resource-stickiness=5000 failure-timeout=60 migration-threshold=1 \
       priority=10
primitive rsc_sap_HA1_ERS10 SAPInstance \
        operations $id=rsc_sap_HA1_ERS10-operations \
        op monitor interval=120 timeout=60 on-fail=restart \
        params InstanceName=HA1_ERS10_sapha1er \
    START_PROFILE="/sapmnt/HA1/profile/HA1_ERS10_sapha1er" \
    AUTOMATIC_RECOVER=false IS_ERS=true \
        meta priority=1000
#
# group definitions for ASCS and ERS
#
group grp_HA1_ASCS00 rsc_ip_HA1_ASCS00 \
   rsc_r53_HA1_ASCS00 rsc_fs_HA1_ASCS00 \
   rsc_sap_HA1_ASCS00 \
         meta resource-stickiness=3000
group grp_HA1_ERS10 rsc_ip_HA1_ERS10 \
   rsc_fs_HA1_ERS10 rsc_sap_HA1_ERS10

#
# constraints between ASCS and ERS
#
colocation col_sap_HA1_not_both -5000: grp_HA1_ERS10 grp_HA1_ASCS00
location loc_sap_HA1_failover_to_ers rsc_sap_HA1_ASCS00 \
        rule 2000: runs_ers_HA1 eq 1
order ord_sap_HA1_first_ascs Optional: rsc_sap_HA1_ASCS00:start rsc_sap_HA1_ERS10:stop symmetrical=false
#
# crm properties and more
#
property cib-bootstrap-options: \
    have-watchdog=false \
    dc-version=1.1.15-21.1-e174ec8 \
    cluster-infrastructure=corosync \
    stonith-enabled=true \
    stonith-action=poweroff \
    stonith-timeout=600s \
    last-lrm-refresh=1513844735
rsc_defaults rsc-options: \
    resource-stickiness=1 \
    migration-threshold=3
op_defaults op-options: \
    timeout=600 \
        record-pending=true</screen>
</section>
<section xml:id="id-checklist-aws-installation">
<title>AWS 설치 체크리스트</title>
<para>설치를 시작하기 전, 우선 AWS 구성을 확인한 후 다음 AWS 항목을 수집하십시오.</para>
<informaltable frame="all" rowsep="1" colsep="1">
<?dbhtml table-width="85%"?>
<?dbfo table-width="85%"?>
<?dblatex table-width="85%"?>
<tgroup cols="2">
<colspec colname="col_1" colwidth="180.625*"/>
<colspec colname="col_2" colwidth="180.625*"/>
<thead>
<row>
<entry align="center" valign="top" namest="col_1" nameend="col_2">AWS 설치 체크리스트</entry>
</row>
</thead>
<tbody>
<row>
<entry align="center" valign="top"><para><emphasis role="strong">항목</emphasis></para></entry>
<entry align="center" valign="top"><para><emphasis role="strong">상태/값</emphasis></para></entry>
</row>
<row>
<entry align="center" valign="top" namest="col_1" nameend="col_2"><para><emphasis role="strong"><emphasis>SLES 구독 및 업데이트 상태</emphasis></emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>모든 시스템에 SAP 구독을 위한 SLES가 있습니다.</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>모든 시스템에 퍼블릭 클라우드 채널이 있습니다.</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>모든 시스템이 최신 패치 수준을 사용하도록 업데이트되었습니다.</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="center" valign="top" namest="col_1" nameend="col_2"><para><emphasis role="strong"><emphasis>설치자를 위한 AWS 사용자 권한</emphasis></emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>EC2 인스턴스 및 EBS 볼륨 생성</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>보안 그룹 생성</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>EFS 파일 시스템 생성</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>AWS 라우팅 테이블 수정</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>정책을 생성하여 IAM 역할에 연결</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>Route53 에이전트 설치 시 선택 사항</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>프라이빗 호스팅 영역에 A 레코드 생성 및 수정</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>잠재적으로 필요한 사항: 서브넷 및 라우팅 테이블 만들기</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="center" valign="top" namest="col_1" nameend="col_2"><para><emphasis role="strong"><emphasis>VPC 및 네트워크</emphasis></emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>VPC ID</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>VPC의 CIDR 범위</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>첫 번째 AZ에서 시스템의 서브넷 ID A</para></entry>
<entry align="left" valign="top"><para>﻿</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>두 번째 AZ에서 시스템의 서브넷 ID B</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>서브넷 A 및 B의 라우팅 테이블 ID</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>라우팅 테이블이 두 서브넷과 연결되었습니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>대안: VPC에 연결되었습니까? 서브넷에 자체 서브넷이 없습니다.</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="center" valign="top" namest="col_1" nameend="col_2"><para><emphasis role="strong"><emphasis>선택 사항: Route 53 구성</emphasis></emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>프라이빗 호스팅 Route 53 영역의 이름</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>DHCP 옵션 세트의 이름(옵션 확인!)</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>옵션 세트가 VPC에 연결되었습니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="center" valign="top" namest="col_1" nameend="col_2"><para><emphasis role="strong"><emphasis>AWS 정책 만들기</emphasis></emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>데이터 공급자 정책 이름</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>STONITH 정책 이름</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>Move IP(오버레이 IP) 정책 이름</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>선택 사항: Route53 정책 이름</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="center" valign="top" namest="col_1" nameend="col_2"><para><emphasis role="strong"><emphasis>첫 번째 클러스터 노드(ASCS 및 ERS)</emphasis></emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>인스턴스 ID</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>ENI ID</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>IP 주소</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>호스트 이름</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>인스턴스가 서브넷 A에 연결되어 있습니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>인스턴스에 3개 또는 4개 정책이 모두 연결되었습니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>EC2 태그 <emphasis>pacemaker</emphasis>가 호스트 이름과 함께 설정되었습니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>AWS CLI 프로파일 <emphasis>cluster</emphasis>가 생성되고 <emphasis>text</emphasis>로 설정되었습니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>소스/대상 확인이 비활성화되었습니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="center" valign="top" namest="col_1" nameend="col_2"><para><emphasis role="strong"><emphasis>두 번째 클러스터 노드(ASCS 및 ERS)</emphasis></emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>인스턴스 ID</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>ENI ID</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>IP 주소</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>호스트 이름</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>인스턴스가 서브넷 B에 연결되어 있습니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>인스턴스에 3개 또는 4개 정책이 모두 연결되었습니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>EC2 태그 <emphasis>pacemaker</emphasis>가 호스트 이름과 함께 설정되었습니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>AWS CLI 프로파일 <emphasis>cluster</emphasis>가 생성되고 <emphasis>text</emphasis>로 설정되었습니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>소스/대상 확인이 비활성화되었습니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="center" valign="top" namest="col_1" nameend="col_2"><para><emphasis role="strong"><emphasis>PAS 시스템</emphasis></emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>IP 주소</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>호스트 이름</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>인스턴스가 서브넷 A 또는 B에 연결되어 있습니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>인스턴스에 연결된 데이터 공급자 정책이 있습니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="center" valign="top" namest="col_1" nameend="col_2"><para><emphasis role="strong"><emphasis>AAS 시스템</emphasis></emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>IP 주소</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>호스트 이름</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>인스턴스가 서브넷 A 또는 B에 연결됨</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>인스턴스에 연결된 데이터 공급자 정책이 있습니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="center" valign="top" namest="col_1" nameend="col_2"><para><emphasis role="strong"><emphasis>DB 시스템(데이터베이스 장애 조치 클러스터의 노드 1일 수 있음)</emphasis></emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>인스턴스 ID</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>ENI ID</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>IP 주소</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>호스트 이름</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>인스턴스가 서브넷 A에 연결되어 있습니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>인스턴스에 연결된 데이터 공급자 정책이 있습니까? 클러스터 노드에 2개 또는 3개 이상의 정책이 연결됨</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="center" valign="top" namest="col_1" nameend="col_2"><para><emphasis role="strong"><emphasis>오버레이 IP 주소: 서비스 ASCS</emphasis></emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>IP 주소</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>라우팅 테이블에 추가되었습니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>첫 번째 노드의 ENI를 가리킵니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="center" valign="top" namest="col_1" nameend="col_2"><para><emphasis role="strong"><emphasis>오버레이 IP 주소: 서비스 ERS</emphasis></emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>IP 주소</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>라우팅 테이블에 추가되었습니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>두 번째 노드의 ENI를 가리킵니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="center" valign="top" namest="col_1" nameend="col_2"><para><emphasis role="strong"><emphasis>선택 사항: 오버레이 IP 주소 DB 서버</emphasis></emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>IP 주소</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>라우팅 테이블에 추가되었습니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>DB 서버의 ENI를 가리킵니까?</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="center" valign="top" namest="col_1" nameend="col_2"><para><emphasis role="strong"><emphasis>선택 사항: Route 53 구성</emphasis></emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Route 53 프라이빗 호스팅 영역에는 ASCS 시스템 이름과 첫 번째 클러스터 노드의 IP 주소가 포함된 A 레코드가 있습니다.</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="center" valign="top" namest="col_1" nameend="col_2"><para><emphasis role="strong"><emphasis>EFS 파일 시스템 만들기</emphasis></emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>EFS 파일 시스템의 DNS 이름</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="center" valign="top" namest="col_1" nameend="col_2"><para><emphasis role="strong"><emphasis>인터넷 액세스</emphasis></emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>모든 인스턴스가 인터넷에 액세스할 수 있습니까? 라우팅 테이블 확인</para></entry>
<entry align="left" valign="top"> </entry>
</row>
<row>
<entry align="left" valign="top"><para>대안: 데이터 공급자 및 클러스터 소프트웨어를 위한 HTTP 프록시 추가</para></entry>
<entry align="left" valign="top"> </entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section xml:id="id-related-sap-notes">
<title>관련 SAP 노트</title>
<itemizedlist>
<listitem>
<para>953653 - Rolling Kernel Switch (<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://launchpad.support.sap.com/#/notes/953653/E">https://launchpad.support.sap.com/#/notes/953653/E</link>)</para>
</listitem>
<listitem>
<para>1153713 - Problems with SAP Management Console (Java) (<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://launchpad.support.sap.com/#/notes/1153713/E">https://launchpad.support.sap.com/#/notes/1153713/E</link>)</para>
</listitem>
<listitem>
<para>1588667 - SAP on AWS: Overview of related SAP Notes and Web-Links (<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://launchpad.support.sap.com/#/notes/1588667/E">https://launchpad.support.sap.com/#/notes/1588667/E</link>)</para>
</listitem>
<listitem>
<para>1656099 - SAP Applications on AWS: Supported DB/OS and AWS EC2 products (<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://launchpad.support.sap.com/#/notes/1656099/E">https://launchpad.support.sap.com/#/notes/1656099/E</link>)</para>
</listitem>
<listitem>
<para>1656250 - SAP on AWS: Support prerequisites (<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://launchpad.support.sap.com/#/notes/1656250/E">https://launchpad.support.sap.com/#/notes/1656250/E</link>)</para>
</listitem>
<listitem>
<para>1763512 - Support details for SUSE Linux Enterprise for SAP Applications (<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://launchpad.support.sap.com/#/notes/1763512/E">https://launchpad.support.sap.com/#/notes/1763512/E</link>)</para>
</listitem>
<listitem>
<para>1984787 - SUSE LINUX Enterprise Server 12: Installation notes (<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://launchpad.support.sap.com/#/notes/1984787/E">https://launchpad.support.sap.com/#/notes/1984787/E</link>)</para>
</listitem>
<listitem>
<para>2077934 - Rolling kernel switch in HA environments (<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://launchpad.support.sap.com/#/notes/2077934/E">https://launchpad.support.sap.com/#/notes/2077934/E</link>)</para>
</listitem>
<listitem>
<para>2235581 - SAP HANA: Supported Operating Systems (<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://launchpad.support.sap.com/#/notes/2235581/E">https://launchpad.support.sap.com/#/notes/2235581/E</link>)</para>
</listitem>
<listitem>
<para>2254173 - Linux: Rolling Kernel Switch in Pacemaker based NetWeaver HA environments (<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://launchpad.support.sap.com/#/notes/2254173/E">https://launchpad.support.sap.com/#/notes/2254173/E</link>)</para>
</listitem>
<listitem>
<para>2309342 - SUSE Linux Enterprise High Availability Extension on AWS for SAP HANA (<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://launchpad.support.sap.com/#/notes/2309342/E">https://launchpad.support.sap.com/#/notes/2309342/E</link>)</para>
</listitem>
<listitem>
<para>2369910 - SAP Software on Linux: General information (<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://launchpad.support.sap.com/#/notes/2369910/E">https://launchpad.support.sap.com/#/notes/2369910/E</link>)</para>
</listitem>
<listitem>
<para>2777438 - SYB: Database Fault Management (<link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://launchpad.support.sap.com/#/notes/2777438/E">https://launchpad.support.sap.com/#/notes/2777438/E</link>)</para>
</listitem>
</itemizedlist>
<?pdfpagebreak?>
</section>
</section>
<section xml:id="id-legal-notice">
<title>사용권 고지사항</title>
<para>저작권 © 2006–2020 SUSE LLC 및 제공자. All rights reserved. </para>
<para>GNU 무료 설명서 라이선스, 버전 1.2 또는 (사용자 선택에 따라) 버전 1.3의 조항에 따라 본 문서를 복사, 배포 및/또는 수정하는 권한이 허가됩니다. 그리고 각 항목에는 본 저작권 표시 및 라이선스가 설명된 고정(Invariant) 섹션이 있습니다. 라이선스 버전 1.2의 복사본은 “GNU 무료 설명서 라이선스” 섹션에 포함되어 있습니다.</para>
<para>SUSE, SUSE 로고 및 YaST는 미국과 기타 국가에서 SUSE LLC의 등록 상표입니다. SUSE 상표는 <link xmlns:xl="http://www.w3.org/1999/xlink" xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>을 참조하십시오.</para>
<para>Linux는 Linus Torvalds의 등록 상표입니다. 본 문서에 언급된 기타 모든 이름 또는 상표는 해당 소유주의 상표 또는 등록 상표일 수 있습니다.</para>
<para>본 문서는 “SUSE 모범 사례”라는 문서 시리즈 중 일부입니다. 시리즈의 각 문서는 SUSE 직원 및 제3자가 자발적으로 기여하여 작성되었습니다. 문서의 목적은 특정 작업을 수행하는 방법의 한 가지 예만을 제공하는 것입니다.</para>
<para>또한, SUSE는 문서에서 설명되는 작업이 해당 기능을 수행하는지 또는 의도하지 않은 결과가 제공되지 않는지를 확인할 수 없습니다.</para>
<para>본 문서의 모든 정보는 최대한 주의를 기울여 작성되었습니다. 그러나 이것이 문서의 정확성을 보장하지는 않습니다. 따라서 SUSE LLC, 그 계열사, 저자, 번역자 중 어느 누구도 있을 수 있는 오류나 그에 따른 결과에 책임을 지지 않는다고 명확히 진술할 필요가 있습니다. 아래에 명시된 내용은 문서를 게시할 때 준수해야 할 라이선스에 대한 주의를 환기하기 위한 것입니다.</para>
<?pdfpagebreak?>
</section>
<xi:include href="license-gfdl.xml"/>
</article>
