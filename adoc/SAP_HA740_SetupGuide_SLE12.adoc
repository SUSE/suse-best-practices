:docinfo:


// defining article ID
[#art-sapha740-setup-sle12]

:slesProdVersion: 12

= SAP NetWeaver Enqueue Replication 1 High Availability Cluster - SAP NetWeaver 7.40 and 7.50: Setup Guide for SUSE Linux Enterprise Server 12

//Fabian Herschel, Bernd Schubert
//2022/06/28
//:Revision: 1.1b
//Revision {Revision} from {docdate}
// Standard SUSE includes
//include::common_copyright_gfdl.adoc[]
//
//:toc:
include::Variables_HA740.adoc[]

////
Weitere TODOs:
TODO use the correct include files and include "places" for the common files to be compatible for the documentation team
TODO maybe the whole setup could be divided in two parts: Enqueue Replication
cluster which is the core part, and the auxiliary pieces which might come from somewhere?
DONE PRIO1: Document that autostart for SAP instances must be switched-off
REJC p.2 either adding the third node to the cluster -> (would change the basic setup)
TODO p.4 https://blogs.sap.com/2014/05/08/using-sapvendorclusterconnector-for-interaction-between-cluster-framework-and-sapstartsrv/comment-page-1/ -> (check link)
TODO p.7 (somewhere explain that the D02 and DVEBMGS01 on the DB host just to simplify this lab environment)
TODO p.7,8 (only short hostnames in /etc/hosts?)
TODO p.13 TODO/TBD ->
TODO p.14 File /usr/sap/HA1/SYS/profile/HA1_ASCS00_sapha1as. -> (font)
TODO p.14 File /usr/sap/HA1/SYS/profile/HA1_ERS10_sapha1er. -> (font)
TODO p.17 TODO/TBD ->
TODO p.17 <nul> -> refer to sle-ha quickstart guide on our webpage
TODO p.18 <nul> -> (run ha-cluter-join or do something to enable cluster?)
TODO p.18 <nul> -> some notes on adpating resulting config, depending on environment
TODO p.24 /usr/sap/HA1/SYS/exe/uc/linuxx86_64/sapcontrol -> # linuxx86_64/sapcontrol
TODO p.25 As ha1adm. -> (font)
TODO p.25,26,27,28,29 sapcontrol -nr -> # sapcontrol -nr
TODO p.31 in HA-Umgebungen -> in HA environments (links without "D"?)
TODO p.32 ... -> TODO/TBD
TODO p.32 -> SLE-HA release notes https://www.suse.com/releasenotes/x86_64/SLE-HA/12-SP2/
TODO p.32 SLE-HA quick setup guide
TODO p.32 SLE-HA product docu
TODO p.32 -> TODO SLES-for-SAP release note
TODO p.32 -> TODO product docu
TODO -> NFS SAP layout for instance
TODO: non line break with in command or variables
TODO: table of required values
////

== About this guide

=== Introduction

{sles4sapReg} is the optimal platform to
run {sapReg} applications with high availability (HA). Together with a redundant layout
of the technical infrastructure, single points of failure can be eliminated.

{sapBSReg} is a sophisticated application platform for large enterprises
and mid-size companies. Many critical business environments require the highest
possible {sapReg} application availability.

The described cluster solution can be used for {sapReg} S/4 HANA and for
{sapReg} {sapNW}.

{sapNw} is a common stack of middleware functionality used to support the SAP
business applications. The {sapERS} constitutes application
level redundancy for one of the most crucial components of the {sapNw} stack,
the enqueue service. An optimal effect of the enqueue replication mechanism can
be achieved when combining the application level redundancy with a high
availability cluster solution as provided with {sles4sap}. The described
concept has proven its maturity over several years of productive operations for
customers of different sizes and branches.


=== Additional documentation and resources

Chapters in this manual contain links to additional documentation resources that
are either available on the system or on the Internet.

For the latest documentation updates, see https://documentation.suse.com/.

Numerous whitepapers, a best practices guide, and other
resources are provided at the {sles4sap} resource
library: https://www.suse.com/products/sles-for-sap/#resource .

This guide and other SAP-specific best practices documents can be downloaded from 
the documentation portal at https://documentation.suse.com/sbp/sap-12/.

Here you can find guides for {SAPHANA} system replication
automation and HA scenarios for {SAPNw} and {s4hana}.

// Standard SUSE includes
=== Feedback
include::common_intro_feedback.adoc[]

//=== Documentation conventions
//TODO work on SUSE doc standard conventions file
//include::common_intro_typografie.adoc[]

== Scope of This Document

This guide details how to:

- Plan a {sleHA} platform for {sapNw},
  including {sapERS}.
- Set up a {linux} high availability platform and perform a basic {sapNW}
  installation including {sapERS} on {sle}.
- Integrate the high availability cluster with the {sap} control framework via
  {s4sClConnector3}, as certified by {sap}.

This guide focuses on the high availability of the central services. 

////
HA cluster solutions for the database and {sapNW} instances are described in the best
practice "Simple Stack" available on our landing page (see section "Additional
documentation and resources"). 
////

For {saphana} system replication, follow the guides for the performance- or cost-optimized scenario.

== Overview

This guide describes how to set up a pacemaker cluster using {sles4sap}
{slesProdVersion} for the Enqueue Replication scenario. The goal is to match
the {sapCert} certification specifications and goals.

These goals include:

- Integration of the cluster with the {SAP} start framework _sapstartsrv_ to
  ensure that maintenance procedures do not break the cluster stability
- Rolling Kernel Switch (RKS) awareness
- Standard {sap} installation to improve support processes

The updated certification {sapcert} has redefined some of the test procedures
and described new expectations how the cluster should behave in special
conditions. These changes allowed us to improve the cluster architecture and to
design it for easier usage and setup.

Shared SAP resources are on a central NFS server.

The {sap} instances themselves are installed on a shared disk to allow switching over the file
systems for proper functionality. The second need for a shared disk is that we are using the SBD
for the cluster fencing mechanism STONITH.
////
TODO PRIO2: with SAP clarify for NFS layout for instance profile
////

=== Differences to previous cluster architectures

The concept is different to the old stack with the master-slave architecture.
With the new certification we switch to a more simple model with primitives.
This means we have on one machine the ASCS with its own resources and on
the other machine the ERS with its own resources.


=== Three systems for ASCS, ERS, database and additional SAP instances

This guide describes the installation of a distributed {sap} system on three
systems. In this setup, only two systems are in the cluster. The database and
{sap} dialog instances could also be added to the cluster by either adding the
third node to the cluster or by installing the database on either of the
nodes. However we recommend to install the database on a separate
cluster.

NOTE: The cluster in this guide only manages the {sap} instances ASCS and ERS,
because of the focus of the {sapCert} certification.

If your database is {sapHana}, we recommend to set up the performance optimized
system replication scenario using our automation solution {sapHanaSR}. The
{sapHanaSR} automation should be set up in an own two node cluster. The setup is
described in a separate best practices document available at http://documentation.suse.com/sbp/sap-12/.

.Three systems for the certification setup
image::sles4sap_nw740_3nodes.svg[SVG]

.Clustered machines

*    one machine ({myNode1}) for ASCS
**    Hostname:    {myVipNAscs}

*    one machine ({myNode2}) for ERS
**    Hostname:   {myVipNErs}

.Non-Clustered machine

*    one machine ({myNode3}) for DB and DI
**    Hostname:   {myVipNDb}
**    Hostname:   {myVipNPas}
**    Hostname:   {myVipNDSec}

=== High availability for the database

Depending on your needs you can also increase the availability of the database if your
database is not already highly available by design.

==== {SapHana} system replication

A perfect enhancement of the three node scenario described in this document is
to implement an {saphana} system replication (SR) automation.

.One cluster for central services, one for {saphana} SR
image::sles4sap_nw740_cs+hanasr.svg[SVG]

The following Databases are supported in combination with this scenario:

- SAP HANA DATABASE 1.0
- SAP HANA DATABASE 2.0

==== Simple stack

Another option is to implement a second cluster for a database without SR aka
"ANYDB". The cluster resource agent SAPDatabase uses the SAPHOSTAGENT to control
and monitor the database.

.One cluster for the central services and one cluster for the ANY database
image::sles4sap_nw740_cs+anydb.svg[SVG]

.The following OS / Databases combination are examples for this scenario
[width="85%",options="header"]
|=========================================================
2+^|{sles4sap} 12
^| *Intel X86_64* ^|*POWER LITTLE ENDIAN*
|SAP HANA DATABASE 1.0  |
|SAP HANA DATABASE 2.0  |SAP HANA DATABASE 2.0
|DB2 FOR LUW 10.5|
|MaxDB 7.9|
|ORACLE 12.1|
|SAP ASE 16.0 FOR BUS. SUITE |
|=========================================================

NOTE: First version for {sapNW} on Power Little Endian is 7.50. More information about
supported combination of OS and Databases for {sapNW} can be found at the
SAP Product Availability Matrix. (https://apps.support.sap.com/sap/support/pam[SAP PAM])

=== Integration of {SapNW} into the cluster using the Cluster Connector

The integration of the HA cluster through the SAP control framework using the
{s4sClConnector} is of special interest. The {SAPSTARTSRV} controls {sap} instances since
{sap} Kernel versions 6.40. One of the classical problems running
{sap} instances in a highly available environment is the following: If an {sap}
administrator changes the status (start/stop) of an {sap} instance without using
the interfaces provided by the cluster software, the cluster framework will
detect that as an error status and will bring the {sap} instance into the old
status by either starting or stopping the {sap} instance. This can result in
very dangerous situations if the cluster changes the status of an {sap} instance
during some {sap} maintenance tasks. This new updated solution enables the central component
{SAPSTARTSRV} to report state changes to the cluster software, and therefore avoids the
previously described dangerous situations.
(See also blog article "Using sap_vendor_cluster_connector for interaction between cluster
framework and sapstartsrv")
(https://blogs.sap.com/2014/05/08/using-sapvendorclusterconnector-for-interaction-between-cluster-framework-and-sapstartsrv/comment-page-1/).

.Cluster connector to integrate the cluster with the {sap} start framework
image::sles4sap_clusterconnector.svg[SVG]

NOTE: For this scenario we are using an updated version of the {s4sClConnector3}.
This version implements the API version 3 for the communication between the cluster
framework and the {sapstartsrv}.

The new version of the {s4sClConnector3} now allows to start, stop and 'move'
an {sap} instance. The integration between the cluster software and the
{sapstartsrv} also implements the option to run checks of the HA setup using either the
command line tool sapcontrol or the {SAP} management consoles ({SAP} MMC or
{sap} MC).

=== Disks and partitions

For all {sap} file systems beside the file systems on NFS we are using XFS.

==== Shared disk for cluster ASCS and ERS

The disk for the ASCS and ERS instances need to be shared and assigned to the
cluster nodes {myNode1} and {myNode2}. Beside the partitions for the file systems
for the SAP instances the disk also provides the partition to be used as SBD.

On {myNode1} prepare the file systems for the shared disk. Create three partitions on
the shared drive {myDev}:

* partition one ({myDevPartSbd}) for SBD (7M)
* partition two ({myDevPartAscs}) for the first file system (10GB) formatted
with XFS
* partition three ({myDevPartErs}) for the second file system (10GB) formatted
with XFS

You could either use YaST to create partitions or using available command line
tools. The following script could be used for non-interactive setups.

[subs="attributes"]
----
# parted -s {myDev} print
# # we are on the 'correct' drive, right?
# parted -s {myDev} mklabel gpt
# parted -s {myDev} mkpart primary 1049k 8388k
# parted -s {myDev} mkpart primary 8389k 10.7G
# parted -s {myDev} mkpart primary 10.7G 21.5G
# mkfs.xfs {myDevPartAscs}
# mkfs.xfs {myDevPartErs}
----

For these file systems we recommend to use plain partitions to keep the cluster
configuration as easy as possible. However you could also place these file
systems in separate volume groups. In that case you need to add further cluster
resources to control the logical volume groups. This is out of the scope of this
setup guide.

After we have partitioned the shared disk on {myNode1} we need to request a
partition table rescan on {myNode2}.

[subs="attributes"]
----
# partprobe; fdisk -l {myDev}
----

During the SAP installation we need {myMpAscs} to be mounted on {myNode1} and
{myMpErs} to be mounted on {myNode2}.
////
TODO: non line break with in command or variables
////
- {myNode1}:   {myDevPartAscs}   {myMpAscs}
- {myNode2}:   {myDevPartErs}   {myMpErs}

==== Disk for DB and dialog instances (MaxDB Example)

The disk for the database and primary application server is assigned to
{myNode3}. In an advanced setup this disk should be shared between {myNode3}
and an optional additional node building an own cluster.

* partition one ({myDevPartSbd}) for SBD (7M) - not used here but a reservation
for an optional second cluster
* partition two ({myDevPartDb}) for the Database (60GB) formatted with XFS
* partition three ({myDevPartPas}) for the second file system (10GB) formatted
with XFS
* partition four ({myDevPartSec}) for the third file system (10GB)
formatted with XFS

You could either use YaST to create partitions or using available command line
tools. The following script could be used for non-interactive setups.

[subs="attributes"]
----
# parted -s {myDev} print
# # we are on the 'correct' drive, right?
# parted -s {myDev} mklabel gpt
# parted -s {myDev} mkpart primary 1049k 8388k
# parted -s {myDev} mkpart primary 8389k 60G
# parted -s {myDev} mkpart primary 60G 70G
# parted -s {myDev} mkpart primary 70G 80G
# mkfs.xfs {myDevPartDb}
# mkfs.xfs {myDevPartPas}
# mkfs.xfs {myDevPartSec}
----

.To be mounted either by OS or an optional cluster
- {myNode3}:   {myDevPartDb}   {myMpDb}

- {myNode3}:   {myDevPartPas}   {myMpPas74}

- {myNode3}:   {myDevPartSec}   {myMpSec}

NOTE:  {myInstPas750} => Since NetWeaver 7.5, the primary application server instance
directory has been renamed. (D<Instance_Number>)


.NFS server
- {myNfsSrv}:{myNFSExpPath}/{mySid}/sapmnt   /sapmnt

- {myNfsSrv}:{myNFSExpPath}/{mySid}/usrsapsys /usr/sap/{mySid}/SYS

.Media
- {myNfsSrv}:{myNFSExpPathSapMedia740} /sapcd

or

- {myNfsSrv}:{myNFSExpPathSapMedia750} /sapcd

=== IP addresses and virtual names

Check, if the _/etc/hosts_ contains at least the following address resolutions.
Add those entries, if they are missing.

[subs="attributes"]
----
{myIPNode1}  {myNode1}
{myIPNode2}  {myNode2}
{myIPNode3}  {myNode3}
{myVipAAscs}  {myVipNAscs}
{myVipAErs}  {myVipNErs}
{myVipADb}  {myVipNDb}
{myVipAPas}  {myVipNPas}
{myVipADSec}  {myVipNDSec}
----

=== Mount points and NFS shares

In our setup the directory _/usr/sap_ is part of the root file system. You could
of course also create a dedicated file system for that area and mount _/usr/sap_
during the system boot. As _/usr/sap_ also contains the {sap} control file
_sapservices_ and the {saphostagent}, the directory should not be placed on a
shared file system between the cluster nodes.

We need to create the directory structure on all nodes which might be able to
run the SAP resource. The SYS directory will be on an NFS share for all nodes.

- Creating mount points and mounting NFS share on all nodes

.{sapNW} 7.4
==============================================
[subs="attributes"]
----
# mkdir -p /sapcd
# mkdir -p /sapmnt
# mkdir -p /usr/sap/{mySid}/{{myInstAscs},{myInstDSec},{myInstPas740},{myInstErs},SYS}
# mount -t nfs {myNfsSrv}:{myNFSExpPath}/{mySid}/sapmnt    /sapmnt
# mount -t nfs {myNfsSrv}:{myNFSExpPath}/{mySid}/usrsapsys /usr/sap/{mySid}/SYS
# mount -t nfs {myNfsSrv}:{myNFSExpPathSapMedia740} /sapcd
----
==============================================

.{sapNW} 7.5
==============================================
[subs="attributes"]
----
# mkdir -p /sapcd
# mkdir -p /sapmnt
# mkdir -p /usr/sap/{mySid}/{{myInstAscs},{myInstPas750},{myInstDSec},{myInstErs},SYS}
# mount -t nfs {myNfsSrv}:{myNFSExpPath}/{mySid}/sapmnt    /sapmnt
# mount -t nfs {myNfsSrv}:{myNFSExpPath}/{mySid}/usrsapsys /usr/sap/{mySid}/SYS
# mount -t nfs {myNfsSrv}:{myNFSExpPathSapMedia750} /sapcd
----
==============================================

- Only MaxDB:  creating mount points for the database at {myNode3}:

[subs="attributes"]
----
# mkdir -p /sapdb
----

- Only HANA: creating mount points for database at {myNode3}:

[subs="attributes"]
----
# mkdir -p /hana/{shared,data,log}
----

- Other databases: creating mount points based on there installation guide.

As we do not control the NFS shares via the cluster in this setup, you should
add these file systems to _/etc/fstab_ to get the file systems mounted during
the next system boot.

////
review Lee means this looks like a 4 node cluster, adding additional title???
////
.File system layout including NFS shares
image::sles4sap_nw740_fs.svg[SVG]

We prepare the three servers for the distributed {sap} installation. Server 1
({myNode1}) will be used to install the ASCS {sap} instance. Server 2
({myNode2}) will be used to install the ERS {sap} instance. Server 3
({myNode3}) will be used to install the dialog {sap} instances and the database.

- Mounting the instance and database file systems at one specific node:

.{sapNW} 7.40 on x86_64 architecture with MaxDB
=============================================================

[subs="attributes"]
----
(ASCS   {myNode1}) # mount {myDevPartAscs} {myMpAscs}
(ERS    {myNode2}) # mount {myDevPartErs} /usr/sap/{mySid}/{myInstErs}
(DB     {myNode3}) # mount {myDevPartDb} /sapdb
(Dialog {myNode3}) # mount {myDevPartPas} /usr/sap/{mySid}/{myInstPas740}
(Dialog {myNode3}) # mount {myDevPartSec} /usr/sap/{mySid}/{myInstDSec}
----
=============================================================

.{sapNW} 7.50 on PowerLE architecture with HANA
=============================================================

[subs="attributes"]
----
(ASCS   {myNode1}) # mount {myDevPartAscs} {myMpAscs}
(ERS    {myNode2}) # mount {myDevPartErs} /usr/sap/{mySid}/{myInstErs}
(DB     {myNode3}) # mount {bsDevPartDbS} /hana/shared
(DB     {myNode3}) # mount {bsDevPartDbL} /hana/log
(DB     {myNode3}) # mount {bsDevPartDbD} /hana/data
(Dialog {myNode3}) # mount {myDevPartPas} /usr/sap/{mySid}/{myInstPas750}
(Dialog {myNode3}) # mount {myDevPartSec} /usr/sap/{mySid}/{myInstDSec}
----
=============================================================

- As a result the directory _/usr/sap/{mySid}/_ should now look like:

[subs="attributes"]
----
# ls -la /usr/sap/{mySid}/
total 0
drwxr-xr-x 1 {mySidLc}adm sapsys 70 28. Mar 17:26 ./
drwxr-xr-x 1 root   sapsys 58 28. Mar 16:49 ../
drwxr-xr-x 7 {mySidLc}adm sapsys 58 28. Mar 16:49 {myInstAscs}/
drwxr-xr-x 1 {mySidLc}adm sapsys  0 28. Mar 15:59 {myInstDSec}/
drwxr-xr-x 1 {mySidLc}adm sapsys  0 28. Mar 15:59 {myInstPas750}/
drwxr-xr-x 1 {mySidLc}adm sapsys  0 28. Mar 15:59 {myInstErs}/
drwxr-xr-x 5 {mySidLc}adm sapsys 87 28. Mar 17:21 SYS/
----

NOTE: The owner of the directory and files is changed during the {sap} installation. By default all
of them are owned by root.

== SAP installation

The overall procedure to install the distributed SAP is:

- Installing the ASCS instance for the central services
- Installing the ERS to get a replicated enqueue scenario
- Preparing the ASCS and ERS installations for the cluster take-over
- Installing the Database
- Installing the primary application server instance (PAS)
- Installing additional application server instances (AAS)

The result will be a distributed {sap} installation as illustrated here:

.Distributed installation of the {sap} system
image::sles4sap_nw740_distInstall.svg[SVG]

=== Linux user and group number scheme

Whenever asked by the SAP software provisioning manager (SWPM) which Linux User
IDs or Group IDs to use, refer to the following table which is, of course, only
an example.

[subs="attributes"]
----
Group sapinst      1000
Group sapsys       1001
Group sapadm       3000
Group sdba         3002

User  {mysapadm}       3000
User  sdb          3002
User  sqd{mySidLc}       3003
User  sapadm       3004
User  {bsDBadm}       4001
----


=== Installing ASCS on {myNode1}

Temporarily we need to set the service IP address used later in the
cluster as local IP, because the installer wants to resolve or use it.
Make sure to use the right virtual host name for each installation step.
Take care for file systems like {myDevPartAscs} and /sapcd/ which might also need
to be mounted.

[subs="attributes"]
----
# ip a a {myVipAAscs}{myVipNM} dev eth0
# mount {myDevPartAscs} {myMpAscs}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNAscs}
----

* SWPM option depends on {sapNW} version and architecture
** Installing {sapNW} 7.40 SR2 -> MaxDB -> SAP-Systems ->
Application Server ABAP -> High-Availability System -> ASCS Instance
** Installing {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> ASCS Instance
* SID id {mySid}
* Use instance number {myAscsIno}
* Deselect using FQDN
* All passwords: use {mySapPwd}
* Double-check during the parameter review, if virtual name *{myVipNAscs}* is used

=== Installing ERS on {myNode2}

Temporarily we need to set the service IP address used later in the
cluster as local IP, because the installer wants to resolve or use it.
Make sure to use the right virtual host name for each installation step.

[subs="attributes"]
----
# ip a a {myVipAErs}{myVipNM} dev eth0
# mount {myDevPartErs} {myMpErs}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNErs}
----

* SWPM option depends on {sapNW} version and architecture
** Installing {sapNW} 7.40 SR2 -> MaxDB -> SAP-Systems ->
Application Server ABAP -> High-Availability System -> Enqueue Replication
Server Instance
** Installing {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> Enqueue Replication
Server Instance
* Use instance number {myErsIno}
* Deselect using FQDN
* Double-check during the parameter review if virtual name *{myVipNErs}* is used
* If you get an error during the installation about permissions, change the
  ownership of the ERS directory

[subs="attributes"]
----
# chown -R {mysapadm}:sapsys /usr/sap/{mySid}/{myInstErs}
----

* If you get a prompt to manually stop/start the ASCS instance, log in at
{mynode1} as user {mysapadm} and call sapcontrol.

[subs="attributes"]
----
# sapcontrol -nr {myAscsIno} -function Stop    # to stop the ASCS
# sapcontrol -nr {myAscsIno} -function Start   # to start the ASCS
----

=== Poststeps for ASCS and ERS

==== Stopping ASCS and ERS

_On {myNode1}_

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myAscsIno} -function Stop
# sapcontrol -nr {myAscsIno} -function StopService
----

_On {myNode2}_

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myErsIno} -function Stop
# sapcontrol -nr {myErsIno} -function StopService
----

==== Maintaining _sapservices_

Ensure _/usr/sap/sapservices_ hold both entries (ASCS+ERS) on both cluster
nodes. This allows the {sapstartsrv} clients to start the service like
(do not execute this at this point in time).

_As user {mySapAdm}_

[subs="attributes"]
----
# sapcontrol -nr {myErsIno} -function StartService {mySid}
----
The _/usr/sap/sapservices_ looks like (typically one line per instance):

[subs="attributes"]
----
#!/bin/sh
LD_LIBRARY_PATH=/usr/sap/{mySid}/{myInstAscs}/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/{mySid}/{myInstAscs}/exe/sapstartsrv pf=/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs} -D -u {mySapAdm}
LD_LIBRARY_PATH=/usr/sap/{mySid}/{myInstErs}/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/{mySid}/{myInstErs}/exe/sapstartsrv pf=/usr/sap/{mySid}/{myInstErs}/profile/{mySid}_{myInstErs}_{myVipNErs} -D -u {mySapAdm}
----

==== Integrating the cluster framework using {s4sClConnector3}

Install the package *{s4sClConnector3}* version 3.1.0 from our
repositories:

[subs="attributes"]
----
# zypper in {s4sClConnector3}
----

NOTE: Be careful there are two packages available. The package {s4sClConnector}
continues to contain the old version 1.1.0 (SAP API 1). The package
{s4sClConnector3} contains the new version 3.1.x (SAP API 3).
The package {s4sClConnector3} with version 3.1.x implements the SUSE SAP API
version 3. New features like SAP Rolling Kernel Switch (RKS) and the migration
of ASCS are only supported with this new version.

For the ERS and ASCS instance edit the instance profiles
{mySid}_{myInstAscs}_{myVipNAscs} and {mySid}_{myInstErs}_{myVipNErs} in the
profile directory _/usr/sap/{mySid}/SYS/profile/_.

You need to tell the `{sapStartSrv}` to load the HA script connector library and
to use the `{s4sClConnector3}`. Additionally, make sure the feature
_Autostart_ is *not* used.

[subs="attributes"]
----
service/halib = $(DIR_EXECUTABLE)/saphascriptco.so
service/halib_cluster_connector = /usr/bin/sap_suse_cluster_connector
----

Add the user {mySapAdm} to the UNIX user group haclient.

[subs="attributes"]
----
# usermod -aG haclient {mySapAdm}
----

==== Adapting {sap} profiles to match the {sapCert} certification

For the ASCS, change the start command from _Restart_Program_xx_ to
_Start_Program_xx_ for the enqueue server (enserver). This change tells the
{sap} start framework *not* to self-restart the enqueue process. Such a restart
would lead in loss of the locks.

.File /usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs}

[subs="attributes"]
----
Start_Program_01 = local $(_EN) pf=$(_PF)
----

Optionally you could limit the number of restarts of services (in the case of
ASCS this limits the restart of the message server).

For the ERS change instance the start command from _Restart_Program_xx_ to
_Start_Program_xx_ for the enqueue replication server (enrepserver).

.File /usr/sap/{mySid}/SYS/profile/{mySid}_{myInstErs}_{myVipNErs}

[subs="attributes"]
----
Start_Program_00 = local $(_ER) pf=$(_PFL) NR=$(SCSID)
----

==== Starting ASCS and ERS

_On {myNode1}_

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myAscsIno} -function StartService {mySid}
# sapcontrol -nr {myAscsIno} -function Start
----

_On {myNode2}_

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myErsIno} -function StartService {mySid}
# sapcontrol -nr {myErsIno} -function Start
----

=== Installing DB on {myNode3} (Example MaxDB)

The MaxDB needs min.40 GB. We use {myDevPartDb} and mount the partition to
_/sapdb_.

[subs="attributes"]
----
# ip a a {myVipADb}{myVipNM} dev eth0
# mount {myDevPartDb} {myMPDb}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNDb}
----

* We are installing SAP NetWeaver 7.40 SR2 -> MaxDB -> SAP-Systems ->
  Application Server ABAP -> High Availability System -> DB
* Profile directory /sapmnt/{mySid}/profile
* DB ID is {mySid}
* Volume Media Type *keep* File (not raw)
* Deselect using FQDN
* Double-check during the parameter review, if virtual name *{myVipNDb}* is used

=== Installing DB on {myNode3} (Example SAP HANA)

The HANA DB has very strict HW requirements. The storage sizing depends on many
indicators. Check the supported configurations at
https://support.sap.com/en/release-upgrade-maintenance.html#section_1969201630[SAP HANA Hardware Directory]
and https://www.sap.com/documents/2016/05/e8705aae-717c-0010-82c7-eda71af511fa.html[SAP HANA TDI].

[subs="attributes"]
----
# ip a a {myVipADb}{myVipNM} dev eth0
# mount {bsDevPartDbS} {bsMPDb}/shared
# mount {bsDevPartDbL} {bsMPDb}/log
# mount {bsDevPartDbD} {bsMPDb}/data
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNDb}
----

* We are installing {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> Database Instance
* Profile directory /sapmnt/{mySid}/profile
* Deselect using FQDN
* Database parameters: enter DBSID is {bsSidDB}; Database Host is {myVipNDb};
Instance Number is {bsDBIno}
* Database System ID: enter Instance Number is {bsDBIno}; SAP Mount Directory is
{bsMPDb}/shared
* Account parameters: change them in case of custom values needed
* Clean up: select *Yes*, remove operating system users from group'sapinst'....
* Double-check during the parameter review, if virtual name *{myVipNDb}* is
  used

=== Installing the Primary Application Server (PAS) on {myNode3}

[subs="attributes"]
----
# ip a a {myVipAPas}{myVipNM} dev eth0
# mount {myDevPartPas} {myMPPas74}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNPas}
----

or alternatively:

[subs="attributes"]
----
# ip a a {myVipAPas}{myVipNM} dev eth0
# mount {myDevPartPas} {myMPPas75}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNPas}
----

* SWPM option depends on {sapNW} version and architecture
** Installing SAP NetWeaver 7.40 SR2 -> MaxDB -> SAP-Systems ->
Application Server ABAP -> High-Availability System -> Primary Application Server Instance
(PAS)
** Installing {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> Primary Application Server Instance
(PAS)
* Use instance number {myPasIno}
* Deselect using FQDN
* For our hands-on setup use a default secure store key
* Do not install Diagnostic Agent
* No SLD
* Double-check during the parameter review if virtual name *{myVipNPas}* is used

=== Installing the Additional Application Server (AAS) on {myNode3}

[subs="attributes"]
----
# ip a a {myVipADSec}{myVipNM} dev eth0
# mount {myDevPartSec} {myMPSec}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNDSec}
----

* SWPM option depends on {sapNW} version and architecture
** Installing SAP NetWeaver 7.40 SR2 -> MaxDB -> SAP-Systems ->
Application Server ABAP -> High-Availability System -> Additional Application Server
Instance (AAS)
** Installing {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> Additional Application Server
Instance (AAS)
* Use instance number {myDSecIno}
* Deselect using FQDN
* Do not install Diagnostic Agent
* Double-check during the parameter review if virtual name *{myVipNDSec}* is used

== Implementing the cluster

The main procedure to implement the cluster is as follows:

* Install the cluster software if not already done during the installation of
the operating system
* Configure the cluster communication framework corosync
* Configure the cluster resource manager
* Configure the cluster resources
* Tune the cluster timing in special for the SBD.

///////////////////////////////
TODO: Do we really need to stop, unconfigure and unmount?
Maybe we find a way to configure the resources that the cluster just
accepts the already started resource groups - lets see ;-)
///////////////////////////////

NOTE: Before we continue to set up the cluster, we first stop all SAP instances, remove
the (manual added) IP addresses on the cluster nodes and unmount the file systems
which will be controlled by the cluster later.

NOTE: The SBD device/partition need to be created in beforehand. In this setup
guide we already have reserved partition _{myDevPartSbd}_ for SBD usage.

.Tasks

. Setup NTP (best with yast2) and enable it

. Install pattern ha_sles on both cluster nodes

[subs="attributes"]
----
# zypper in -t pattern ha_sles
----

=== Configuring the cluster base

.Tasks

- Install and configure the cluster stack at first machine

You can use either YaST to configure the cluster base or the interactive
command line tool ha-cluster-init. The following script can be used for
automated setups.

[subs="attributes"]
----
# modprobe softdog
# echo "softdog" > /etc/modules-load.d/softdog.conf
# systemctl enable sbd
# ha-cluster-init -y -i eth0 -u -s {myDevPartSbd}
----

Keep in mind that a hardware watchdog is preferred instead of the softdog method.

- Join the second node

Find below some preparation steps on the second node.

[subs="attributes"]
----
# modprobe softdog
# echo "softdog" > /etc/modules-load.d/softdog.conf
# systemctl enable sbd
# rsync {myIpNode1}:/etc/sysconfig/sbd /etc/sysconfig
----

You can use either YaST to configure the cluster base or the interactive
command line tool ha-cluster-join. The following script can be used for
automated setups.

[subs="attributes"]
----
# ha-cluster-join -y -c {myIPNode1} -i {myHaNetIf}
----

- The _crm_mon -1r_ output should look like this:

[subs="attributes"]
----
Last updated: Thu Nov 21 14:25:53 2019		Last change: Thu Nov 21 14:23:21 2019 by {mySidLc}adm via crm_resource on {myNode1}
Stack: corosync
Current DC: {myNode1} (version 1.1.19-20181105.ccd6b5b10) - partition with quorum
2 nodes and 1 resource configured

Online: [ {myNode1} {myNode2} ]

stonith-sbd	(stonith:external/sbd):	Started {myNode1}
----

- After both nodes are listed in the overview, verify the property setting of the basic cluster configuration. 
Very important here is the setting: *record-pending=true*.

[subs="attributes"]
----
# crm configure show
...
property cib-bootstrap-options: \
        have-watchdog=true \
        dc-version=1.1.19-20181105.ccd6b5b10 \
        cluster-infrastructure=corosync \
        cluster-name=hacluster \
        stonith-enabled=true \
        last-lrm-refresh=1494346532
rsc_defaults rsc-options: \
        resource-stickiness=1 \
        migration-threshold=3
op_defaults op-options: \
        timeout=600 \
        record-pending=true

----

=== Configuring cluster resources

We need a changed SAPInstance resource agent for {sapNw} to *not* use
the master-slave construct anymore and to move to a more cluster-like construct to
start and stop the ASCS and the ERS itself and *not* only the complete
master-slave.

For this there is a new functionality for the ASCS needed to follow the ERS.
The ASCS needs to mount the shared memory table of the ERS to avoid the loss of
locks.

.Resources and constraints
image::sles4sap_nw740_resources.svg[SVG]

The implementation is done using the new flag "runs_ers_$SID" within
the RA, enabled with help of the resource parameter "IS_ERS=TRUE".

Another benefit of this concept is that we can now work with local (mountable)
file systems instead of a shared (NFS) file system for the {sap} instance
directories.

==== Preparing the cluster for adding the resources

To avoid that the cluster starts partially defined resources, we set the cluster
to the maintenance mode. This deactivates all monitor actions.

_As user root_

[subs="attributes"]
----
# crm configure property maintenance-mode="true"
----

==== Configuring the resources for the ASCS

First we configure the resources for the file system, IP address and the {sap}
instance. Of course you need to adapt the parameters to your environment.

.ASCS primitive
================================================
[subs="attributes"]
----
primitive rsc_fs_{mySID}_{myInstAscs} Filesystem \
  params device="{myDevPartAscs}" directory="/usr/sap/{mySID}/{myInstAscs}" \
     fstype=xfs \
  op start timeout=60s interval=0 \
  op stop timeout=60s interval=0 \
  op monitor interval=20s timeout=40s
primitive rsc_ip_{mySID}_{myInstAscs} IPaddr2 \
  params ip={myVipAAscs} \
  op monitor interval=10s timeout=20s
primitive rsc_sap_{mySID}_{myInstAscs} SAPInstance \
  operations $id=rsc_sap_{mySID}_{myInstAscs}-operations \
  op monitor interval=11 timeout=60 on-fail=restart \
  params InstanceName={mySID}_{myInstAscs}_{myVipNAscs} \
     START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstAscs}_{myVipNAscs}" \
     AUTOMATIC_RECOVER=false \
  meta resource-stickiness=5000 failure-timeout=60 \
     migration-threshold=1 priority=10
----
================================================

.ASCS group
================================================
[subs="attributes"]
----
group grp_{mySID}_{myInstAscs} \
  rsc_ip_{mySID}_{myInstAscs} rsc_fs_{mySID}_{myInstAscs} rsc_sap_{mySID}_{myInstAscs} \
     meta resource-stickiness=3000
----
================================================

Create a txt file (like crm_ascs.txt) with your preferred text editor, enter
both examples (primitives and group) to that file and load the configuration to
the cluster manager configuration.

_As user root_

[subs="attributes"]
----
# crm configure load update crm_ascs.txt
----

==== Configuring the resources for the ERS

Second, we configure the resources for the file system, IP address and the {sap}
instance. Of course you need to adapt the parameters to your environment.

The specific parameter _IS_ERS=true_ should only be set for the ERS instance.

.ERS primitive
================================================
[subs="attributes"]
----
primitive rsc_fs_{mySID}_{myInstErs} Filesystem \
  params device="{myDevPartErs}" directory="/usr/sap/{mySID}/{myInstErs}" fstype=xfs \
  op start timeout=60s interval=0 \
  op stop timeout=60s interval=0 \
  op monitor interval=20s timeout=40s
primitive rsc_ip_{mySID}_{myInstErs} IPaddr2 \
  params ip={myVipAErs} \
  op monitor interval=10s timeout=20s
primitive rsc_sap_{mySID}_{myInstErs} SAPInstance \
  operations $id=rsc_sap_{mySID}_{myInstErs}-operations \
  op monitor interval=11 timeout=60 on-fail=restart \
  params InstanceName={mySID}_{myInstErs}_{myVipNErs} \
     START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstErs}_{myVipNErs}" \
     AUTOMATIC_RECOVER=false IS_ERS=true \
  meta priority=1000
----
================================================

.ERS group
================================================
[subs="attributes"]
----
group grp_{mySID}_{myInstErs} \
  rsc_ip_{mySID}_{myInstErs} rsc_fs_{mySID}_{myInstErs} rsc_sap_{mySID}_{myInstErs}
----
================================================

Create a txt file (like crm_ers.txt) with your preferred text editor, enter
both examples (primitives and group) to that file and load the configuration to
the cluster manager configuration.

_As user root_

[subs="attributes"]
----
# crm configure load update crm_ers.txt
----

==== Configuring the colocation constraints between ASCS and ERS

The constraints between the ASCS and ERS instance are needed to define that the
ASCS instance starts exactly on the cluster node running the ERS
instance after a failure (loc_sap_{mysid}_failover_to_ers). This constraint is
needed to ensure that the locks are not lost after an ASCS instance (or node)
failure.

If the ASCS instance has been started by the cluster the ERS instance should
be moved to an "other" cluster node (col_sap_{mysid}_no_both). This constraint
is needed to ensure that the ERS will synchronize the locks again and the cluster is
ready for an additional take-over.

.Location constraint
================================================
[subs="attributes"]
----
colocation col_sap_{mysid}_no_both -5000: grp_{mySID}_{myInstErs} grp_{mySID}_{myInstAscs}
location loc_sap_{mysid}_failover_to_ers rsc_sap_{mySID}_{myInstAscs} \
         rule 2000: runs_ers_{mySID} eq 1
order ord_sap_{mysid}_first_start_ascs Optional: rsc_sap_{mySID}_{myInstAscs}:start \
      rsc_sap_{mySID}_{myInstErs}:stop symmetrical=false
----
================================================

Create a txt file (like crm_col.txt) with your preferred text editor, enter
all three constraints to that file and load the configuration to the
cluster manager configuration.

_As user root_

[subs="attributes"]
----
# crm configure load update crm_col.txt
----

==== Activating the cluster

Now the last step is to end the cluster maintenance mode and to allow the
cluster to detect already running resources.

_As user root_

[subs="attributes"]
----
# crm configure property maintenance-mode="false"
----


////
############## REMOVING THIS FROM THE OUTPUT FOR NOW ###########

=== Installing SAP Licenses

Most likely you have your own established method to maintain your {sap} licenses.
This section is only a reminder. You should have installed two license keys as
we need to failover the ASCS {sap} instance.

- Get the HWKEY of both cluster nodes
- Get the license from the SAP launchpad
- Install the two licenses per HWKEY using transaction SLICENSE

###################
////

////
############## REMOVING THIS FROM THE OUTPUT FOR NOW ###########

=== Optional - Installing the HA test tool HATool

This is an optional task. If you like to check your cluster using {sap}s HATool
you might find these notes helpful. Always follow {SAP}s documentation
how to install and use the HATool.

We install the test Client at {myNode3}. The server part is imported
as a SAP Transport. In our environment the {sap} dialog instance is also running
at {myNode3}, so we also store the transport files there.

- Installing the HATool Client (as root)

[subs="attributes"]
----
# cd /root/herschel
# unzip HATool_v213.zip
(creates directories HATool/{bin,check,config,doc,event,server)
----

- Installing other sw parts of the HATool Client (as root).

See also sections 3.2 and 4.2 of the "High Availability Test Tool - Installation
& Operations Guide"

[subs="attributes"]
----
# cd /root/herschel
##
## SAP JVM8 - we skipped that part, because we have Oracle VM8
##
#######
##
## SAP JCO 3.0
##
# mkdir SAPJCO; cd SAPJCO
# Download from Marketplace
# unzip mkdir SAPJCOmkdir SAPJCO
# tar -xvzf sapjco3-linuxx86_64-3.0.16.tgz
# unzip sapjco30P16HF_1-10009485.zipsapjco30P16HF_1-10009485.zip
# ln -s $PWD/sapjco3.jar /root/herschel/HATool/bin
# ln -s $PWD/libsapjco3.so /root/herschel/HATool/bin
##
## d3.min.js - graphic library
##
# mkdir d3.min.js
# wget https://d3js.org/d3.v3.min.js
# mv d3.v3.min.js d3.min.js/d3.min.js
# ln -s /root/herschel/d3.min.js/d3.v3.min.js \
      /root/herschel/HATool/bin/d3.min.js
----

- Importing the HATool server part into the SAP system.
This is described in section 4.3 in the "High Availability Test Tool -
Installation & Operations Guide".

- Creating and configuring user hatool.
This is described in section 4.3 in the "High Availability Test Tool -
Installation & Operations Guide".

- Setting up the first property file for a smoke test.
In the directory /root/herschel/HATool/config, change the file
haQuickStartTemplate.properties like the following patch does. This means set the values for
systemclient, user, password, messagehost and SID.

[subs="attributes"]
----
 --- haQuickStartTemplate.properties     2015-10-14 14:50:48.000000000 +0200
 +++ FHhaQuickStartTemplate.properties   2017-04-24 20:43:41.117512210 +0200
 @@ -53,20 +53,20 @@ resettestdata = 1
  ###################################
  ########## login information for server user ###########
 -systemclient = nnn
 -user = xxxxxxxxxx
 -password = xxxxxxxxx
 +systemclient = 001
 +user = hatool
 +password = {mySapPwd}
  language = en
  ################# RFC ###################
  # login via message server
  -msgserverhost = xxxxxxxxxx
  +msgserverhost = {myVipNAscs}
  # the port information is necessary, if there is no entry for sapms systemid in /etc/services resp. C:\Windows\System32\drivers\etc\services
  #msgserverport = 3600
  # for the logon group, PUBLIC is used by default; if no explicit group was created, use SPACE
  logongroup = SPACE
  # if msgserverport is not set, systemid is mandatory
 -systemid = SID
 +systemid = {mySid}
 ####################
 #   One HA Event   #
----

- Testrun
[subs="attributes"]
 java -cp haTestTool.jar:sapjco3.jar com.sap.test.haload.ClientDriver \
     file=FHhaQuickStartTemplate.properties

- Adapting the test properties
This is described in section 3.2 of the "SAP Application Server HA Interface
Certification" guide.

** Login.properties - set systemclient, user, password, messagehost and SID
** TEC05Event.properties - set eventcall to TEC05Event.sh
** TEC14Event.properties - set eventcall to TEC14Event.sh
** TEC05Event.sh - edit the _sapcontrol_ command

[subs="attributes"]
----
 /usr/sap/{mySid}/SYS/exe/uc/linuxx86_64/sapcontrol -prot NI_HTTP \
   -user {mySapAdm} {mySapPwd} \
   -host {myVipNAscs} -nr {myAscsIno} -function HAFailoverToNode ""
----
** TEC14Event.sh - edit the sapctrl command

[subs="attributes"]
----
 /usr/sap/{mySid}/SYS/exe/uc/linuxx86_64/sapcontrol -prot NI_HTTP \
   -user {mySapAdm} {mySapPwd} \
   -host {myVipNAscs} -nr {myAscsIno} -function UpdateSystem 120 300 1
----

NOTE: Maybe we need to change the rks-host (here {myVipNAscs}) after we get
feedback from SAP which instance should be referenced.

##############
////

////
:leveloffset: 2
include::MaxdbStudio.txt[]

include::MaxdbResizeDB.txt[]

:leveloffset: 0
////

////
############################
#
# ADMINISTRATION
#
############################
////

== Administration

=== Dos and don'ts

==== Never stop the ASCS instance

For normal operation *do not stop* the ASCS {sap} instance with any tool such
as cluster tools or {sap} tools. The stop of the ASCS instance might lead to a loss of enqueue
locks. Because following the new {sapCert} certification the cluster must allow local restarts
of the ASCS. This feature is needed to allow rolling kernel switch (RKS) updates without
reconfiguring the cluster.

WARNING: Stopping the ASCS instance might lead into the loss of {sap} enqueue
  locks during the start of the ASCS on the same node.

==== How to move ASCS

To *move* the ASCS {sap} instance you should use the {sap} tools such as
the {sap} management console. This will trigger {sapStartSrv} to use the
{s4sClConnector3} to move the ASCS instance. As user _{mysapadm}_ you might call
the following command to move-away the ASCS. The move-away will always
move the ASCS to the ERS side which will keep the {sap} enqueue locks.

_As {mysapadm}_

[subs="attributes"]
----
# sapcontrol -nr 00 -function HAFailoverToNode ""
----

==== Never block resources

With {sapCert} it is *not longer allowed to block resources* from being
  controlled manually. This using the variable _BLOCK_RESOURCES_ in
  _/etc/sysconfig/sap_suse_cluster_connector_ is not allowed anymore.

==== Always use unique instance numbers

Currently all {sap} *instance numbers controlled by the cluster must be unique*.
  If you need to have multiple dialog instances such as D00 running on different
  systems they should be not controlled by the cluster.

==== How to set cluster in maintenance mode

The procedure to set the cluster into maintenance mode can be done as _root_ or _sidadm_.

_As user root_

[subs="attributes"]
----
# crm configure property maintenance-mode="true"
----

_As user {mysapadm} (the full path is needed)_

[subs="attributes"]
----
# /usr/sbin/crm configure property maintenance-mode="true"
----

==== Procedure to end the cluster maintenance

_As user root_

[subs="attributes"]
----
# crm configure property maintenance-mode="false"
----

==== Cleaning up resources

How to *clean up resource failures*? Failures of the ASCS will be automatically
  deleted to allow a failback after the configured period of time. For all other
  resources you can clean up the status including the failures:

_As user root_

[subs="attributes"]
----
# crm resource refresh RESOURCE-NAME
----

WARNING: You should not clean up the complete group of the ASCS resource as this
   might lead into an unwanted cluster action to take-over the complete group to
   the node where ERS instance is running.

=== Testing the cluster

We strongly recommend that you at least process the following tests before you
plan going into production with your cluster:

==== Checking Product Names with HAGetFailoverConfig

Check if the name of the SUSE cluster solution is shown in the output of
  sapcontrol or {sap} management console. This test checks the status of the
  {sapNW} cluster integration.

_As user {mysapadm}_

[subs="attributes"]
----
# sapcontrol -nr 00 -function HAGetFailoverConfig
----

==== Starting SAP checks using HACheckConfig and HACheckFailoverConfig

Check if the HA configuration tests are showing no errors.

_As user {mysapadm}_

[subs="attributes"]
----
# sapcontrol -nr 00 -function HACheckConfig
# sapcontrol -nr 00 -function HACheckFailoverConfig
----

==== Manually moving ASCS

Check if manually moving the ASCS using HA tools works properly.

_As user root_

[subs="attributes"]
----
# crm resource move rsc_sap_{mySid}_{myInstAscs} force
## wait until the ASCS is been moved to the ERS host
# crm resource clear rsc_sap_{mySid}_{myInstAscs}
----

==== Migrating ASCS using HAFailoverToNode

Check if moving the ASCS instance using {sap} tools like {sapCtrl} does work properly

_As user {mysapadm}_

[subs="attributes"]
----
# sapcontrol -nr 00 -function HAFailoverToNode ""
----

==== Testing ASCS migration after failure

Check if the ASCS instance moves correctly after a node failure.

_As user root_

[subs="attributes"]
----
## on the ASCS host
# echo b >/proc/sysrq-trigger
----

==== Inplacing restart of ASCS Using Stop and Start

Check if the inplace re-start of the {sap} resources have been processed
  correctly. The {sap} instance should not failover to an other node, it
  must start on the same node where it has been stopped.

WARNING: This test will force the SAP system to *lose* the enqueue locks.
   *This test should not be processed during production.*

_As user {mysapadm}_

[subs="attributes"]
----
## example for ASCS
# sapcontrol -nr 00 -function Stop
## wait until the ASCS is completely down
# sapcontrol -nr 00 -function Start
----

==== Additionally recommended Tests

* Automated restart of the ASCS (simulating RKS)

* Check the recoverable and non-recoverable outage of the message server process

* Check the non-recoverable outage of the {sap} enqueue server process

* Check the outage of the {sapERS}

* Check the outage and restart of {sapStartSrv}

* Check the rolling kernel switch procedure (RKS), if possible

* Check the simulation of an upgrade

* Check the simulation of cluster resource failures


== References

For more information, see the documents listed below.

=== Pacemaker

- Pacemaker 1.1 Configuration Explained:
https://clusterlabs.org/pacemaker/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/

:leveloffset: 2
include::SAPNotes_ha740.adoc[]

++++
<?pdfpagebreak?>
++++

////
############################
#
# APPENDIX
#
############################
////

:leveloffset: 0

== Appendix

=== CRM configuration

The complete crm configuration for {sap} system {mySid} looks as follows:

[subs="attributes"]
----
## nodes

node 1084753931: {myNode1}
node 1084753932: {myNode2}

## primitives for ASCS and ERS

primitive rsc_fs_{mySID}_{myInstAscs} Filesystem \
	params device="{myDevPartAscs}" directory="/usr/sap/{mySID}/{myInstAscs}" fstype=xfs \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=40s
primitive rsc_fs_{mySID}_{myInstErs} Filesystem \
	params device="{myDevPartErs}" directory="/usr/sap/{mySID}/{myInstErs}" fstype=xfs \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=40s
primitive rsc_ip_{mySID}_{myInstAscs} IPaddr2 \
	params ip={myVipAAscs} \
	op monitor interval=10s timeout=20s
primitive rsc_ip_{mySID}_{myInstErs} IPaddr2 \
	params ip={myVipAErs} \
	op monitor interval=10s timeout=20s
primitive rsc_sap_{mySID}_{myInstAscs} SAPInstance \
	operations $id=rsc_sap_{mySID}_{myInstAscs}-operations \
	op monitor interval=11 timeout=60 on-fail=restart \
	params InstanceName={mySID}_{myInstAscs}_{myVipNAscs} \
	 START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstAscs}_{myVipNAscs}" \
	 AUTOMATIC_RECOVER=false \
	meta resource-stickiness=5000 failure-timeout=60 migration-threshold=1 \
	 priority=10
primitive rsc_sap_{mySID}_{myInstErs} SAPInstance \
	operations $id=rsc_sap_{mySID}_{myInstErs}-operations \
	op monitor interval=11 timeout=60 on-fail=restart \
	params InstanceName={mySID}_{myInstErs}_{myVipNErs} \
	 START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstErs}_{myVipNErs}" \
	 AUTOMATIC_RECOVER=false IS_ERS=true \
	meta priority=1000
primitive stonith-sbd stonith:external/sbd \
	params pcmk_delay_max=30s

## group definitions for ASCS and ERS

group grp_{mySID}_{myInstAscs} rsc_ip_{mySID}_{myInstAscs} rsc_fs_{mySID}_{myInstAscs} rsc_sap_{mySID}_{myInstAscs} \
	meta resource-stickiness=3000
group grp_{mySID}_{myInstErs} rsc_ip_{mySID}_{myInstErs} rsc_fs_{mySID}_{myInstErs} rsc_sap_{mySID}_{myInstErs}

## constraints between ASCS and ERS

colocation col_sap_{mySid}_not_both -5000: grp_{mySID}_{myInstErs} grp_{mySID}_{myInstAscs}
location loc_sap_{mySid}_failover_to_ers rsc_sap_{mySID}_{myInstAscs} \
	rule 2000: runs_ers_{mySID} eq 1
order ord_sap_{mySid}_first_ascs Optional: rsc_sap_{mySID}_{myInstAscs}:start rsc_sap_{mySID}_{myInstErs}:stop symmetrical=false

## crm properties and more

property cib-bootstrap-options: \
	have-watchdog=true \
	dc-version=1.1.19-20181105.ccd6b5b10 \
	cluster-infrastructure=corosync \
	cluster-name=hacluster \
	stonith-enabled=true \
	last-lrm-refresh=1494346532
rsc_defaults rsc-options: \
	resource-stickiness=1 \
	migration-threshold=3
op_defaults op-options: \
	timeout=600 \
	record-pending=true
----

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
== Legal notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

////
Version 1.0 - Initial version based on SLE12 and NW 7.40
Version 1.1 - Including SLE15 preparation, NW 7.50, Unicast set-up
////
//
// REVISION 1.1  2018/04
//   - PowerLE
//   - corr: StartService
//   - removed no-quorum-policy=ignore
//   - corr: upper/lowercase of section titles
// Revision 1.1a 2020/04
//   - migration to adoc build process
//   - small adaption based on customer feedback
// Revision 1.1b 2022/06
//   - small adaption based on customer feedback
//
