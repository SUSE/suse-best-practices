:docinfo:

:localdate:

// Document Variables
:slesProdVersion: 15
//

// defining article ID
[#art-saphana-cluster-simplemount]

= SAP S/4 HANA - Enqueue Replication 2 High Availability Cluster With Simple Mount: Setup Guide

// Revision {Revision} from {docdate}
// Standard SUSE includes
// include::common_copyright_gfdl.adoc[]

// :toc:

include::Variables_s4_2021.adoc[]
//

////
TODO PRIOx: example
////

== About this guide

The following sections focus on background information and the purpose of the document at hand.

=== Introduction

{sles4sapReg} is the optimal platform to
run {sapReg} applications with high availability. Together with a redundant layout
of the technical infrastructure, single points of failure can be eliminated.

{sapBSReg} is a sophisticated application platform for large enterprises
and mid-size companies. Many critical business environments require the highest
possible {sapReg} application availability.

The described cluster solution can be used for {sapReg} {sapS4pl}.

{sapS4pl} is a common stack of middleware functionality used to support SAP
business applications. The {sapERS} constitutes application
level redundancy for one of the most crucial components of the {sapS4pl} stack,
the enqueue service. An optimal effect of the enqueue replication mechanism can
be achieved when combining the application level redundancy with a high
availability cluster solution, as provided for example by {sles4sap}. Over
years of productive operations, the components mentioned have proven their
maturity for customers of different sizes and industries.

In contrast to the traditional setups, this setup uses an additional NFS mount
for the {sap} application layer without the need to have dedicated block devices
and cluster-controlled file systems. That greatly simplifies the overall
architecture, implementation and maintenance of a {sleHA} cluster for
{sapS4pl} with {sapERS}. The here described setup is expected to be default
for new deployments on {sles4sap} {slesProdVersion}.

.Certified by SAP
image::SAP_Certi_Integration_SAPS4HANA_R.svg[SVG]

For additional information on the simple mount architecture, also read:

- {SUSE} blog article "Simple Mount Structure for SAP Application Platform"
(https://www.suse.com/c/simple-mount-structure-for-sap-application-platform/).

- {SUSE} knowledge base TID ("Technical Information Document") 00019944 "Use of Filesystem resource for ASCS/ERS HA setup not possible" (https://www.suse.com/support/kb/doc/?id=000019944).

- Manual page ocf_suse_SAPStartSrv(7), shipped with package sapstartsrv-resource-agents.

The former setup with cluster-controlled file system resources as described in
"SAP S/4 HANA - Enqueue Replication 2 High Availability Cluster - Setup Guide"
(https://documentation.suse.com/sbp/sap-15/html/SAP-S4HA10-setupguide-sle15/)
will remain supported.

// TODO PRIO2: for the lifetime of {sles4sap} {slesProdVersion}.

[id="sec.resource"]
=== Additional documentation and resources

Chapters in this manual contain links to additional documentation resources that
are either available on the system or on the Internet.

For the latest {SUSE} product documentation updates, see https://documentation.suse.com.

Find white-papers, best-practices guides, and other resources at the
// TODO PRIO3: check links

- {sles4sap} resource library:
https://www.suse.com/products/sles-for-sap/resource-library/

- {SUSE} best practices web page:
https://documentation.suse.com/sbp/sap-15/

- Supported high availability solutions by {sles4sap} overview:
https://documentation.suse.com/sles-sap/sap-ha-support/html/sap-ha-support/article-sap-ha-support.html

Lastly, there are manual pages shipped with the product.

=== Errata

To deliver urgent smaller fixes and important information in a timely manner,
the Technical Information Document (TID) for this document will be updated,
maintained and published at a higher frequency:

////
TODO PRIO3: create errata for simple-mount guide?
- SAP S/4 HANA - Enqueue Replication 2 High Availability Cluster - Setup Guide - Errata
(https://www.suse.com/support/kb/doc/?id=7023714)
////

- In addition to this guide, check the SUSE SAP Best Practice Guide Errata for other solutions
(https://www.suse.com/support/kb/doc/?id=7023713).

// Standard SUSE includes
=== Feedback
include::common_intro_feedback.adoc[]

//=== Documentation Conventions
//TODO PRIO3: work on SUSE doc standard conventions file
//include::common_intro_typografie.adoc[]


== Scope of this document

The document at hand explains how to:

- plan a {sleHA} platform for {sapS4pl}, including {sapERS}.
- plan and implement an NFS-based storage layout for {sapERS}.
- set up a {linux} high availability platform and perform a basic {sapS4pl}
  installation including {sapERS} on {sle}.
- integrate the high availability cluster with the {sap} control framework via
  *{s4sClConnector3}* version 3 and the new *SAPStartSrv* resource agent,
  to get an {sap} certified setup.

NOTE: This guide implements the cluster architecture for enqueue replication version 2. For {sapS4Pl}
 versions 1909 or newer, enqueue replication version 2 is the default.

This guide focuses on the high availability of the central services. For {saphana} system replication
consult the guides for the performance-optimized or cost-optimized scenario (see <<sec.resource>>).


== Overview

This document describes setting up a pacemaker cluster using {sles4sap}
{slesProdVersion} for the Enqueue Replication scenario. The focus is on matching
the {sapCert} certification specifications and goals. For the setup described in
this document, two or three nodes are used for the ASCS central services instance
and ERS replicated enqueue instance. These nodes are controlled by the {sleHA}
cluster. Additional nodes are used for running the database, and the PAS and AAS
application server instances. Finally, you need a highly available NFS server.

The goals for the setup include:

- Implementation of a cluster with a shared {sap} applications directory
- Integration of the new *SapStartSrv* resource agent
- Integration of the cluster with the native *systemd*-based {sap} start framework
  _sapstartsrv_ to ensure that maintenance procedures do not break the cluster stability
- Rolling Kernel Switch (RKS) awareness
- Standard {sap} installation to improve support processes
- Support of automated HA maintenance mode for SAP resources by implementing support of
  SAP HACheckMaintenanceMode and HASetMaintenanceMode
- Support of more than two cluster nodes for ASCS and ERS instances allowed

The updated certification {sapcert} redefines some of the test procedures
and describes new expectations how the cluster should behave in special
conditions. These changes allowed to improve the cluster architecture and to
design it for easier usage, maintenance procedures and setup.

All shared SAP resources are located on a central NFS server.

Shared disks allow using SBD as the cluster fencing mechanism.

// TODO PRIO2: requirements from ocf_suse_SAPStartSrv(7)


=== Differences to previous cluster architectures

- The described architecture now includes the simple mount structure based on an external network file share.
  Instead of the file system resources needed for each SAP instance, a resource type *SAPStartSrv* controls the matching `sapstartsrv` framework process.
  The cluster configuration is straightforward.

.From classic resources to simple mount resources
image::sles4sap_s4hana2021_resources_classic_compared_to_simple_mount.svg[SVG]
////
- The described new concept differs from the old stack that used a multi-state architecture.
  With the new certification there is a switch to a simpler model with primitives.
  This means: on one machine there is the ASCS instance with its own resources and on
  the other machine there is the ERS instance with its own resources.
////
- For {s4Hana} the new concept implies that, after a resource failure, the ASCS does not need to be started at the ERS
  side. The new enqueue architecture is also named ENSA2.

- Use of native *systemd* integration for {sap} hostagent and instanceÂ´s `sapstartsrv`.
  Refer to the {sap} documentation for the necessary product version. See also SAP note 3139184.
  SUSE *systemd* version 234 at least is needed. For details refer to the {sles4sap} product documentation.
  SUSE resource agents are needed, at least _sapstartsrv-resource-agents 0.9.1_ and _resource-agents 4.x_ from November 2021.

=== Typical systems for ASCS, ERS, database and additional SAP instances

The document on hand describes the installation of a distributed {sap} system on three and more
systems. In this setup, only two or three systems reside inside the cluster. The database and
{sap} dialog instances can be controlled by an other cluster.
We recommend to install the database on a separate cluster. The cluster configuration
for three and more nodes is described at the end of this document.
The number of nodes within one cluster should be either two or an odd number.
////
TODO PRIO2: check text content
////
NOTE: Because the setup at hand focuses on the {sapCert} certification, the cluster detailed
in this guide only manages the {sap} instances ASCS and ERS.

If your database is {sapHana}, we recommend setting up the performance-optimized
system replication scenario using the automation solution {sapHanaSR}. The
{sapHanaSR} automation should be set up in an own two-node cluster. The setup is
described in a separate best practices document available from the SUSE Best
Practices documentation Web page at https://documentation.suse.com/sbp/sap-15/.

.Typical systems for the certification setup
image::sles4sap_s4hana_simple_fs02.svg[SVG]

.Clustered machines Two-Node scenario

*    One machine ({my2nd1}) for ASCS
**    Virtual host name:    {myVipNAscs}

*    One machine ({my2nd2}) for ERS
**    Virtual host name:   {myVipNErs}

.Optionally clustered machines

*    One machine ({myNode1}) for DB; Virtual host name:   {myVipNDb}

*    One machine ({my3nd3}) for the PAS; Virtual host name:   {myVipNPas}

*    One machine ({my3nd4}) for the AAS; Virtual host name:   {myVipNDSec}


=== Increasing high availability for the database

Depending on your needs, you can increase the availability of the database if your
database is not already highly available by design.

==== Implementing {SapHana} system replication

A perfect enhancement of the three-node scenario described in this document is
to implement an {saphana} system replication (SR) automation.

.One cluster for central services, one for {saphana} SR
image::sles4sap_nw740_cs+hanasr.svg[SVG]

.The following OS/database combinations are examples for this scenario
[width="85%",options="header"]
|=========================================================
2+^|{sles4sap} {slesProdVersion}
^| *Intel X86_64*
|SAP HANA DATABASE 2.0
^| *IBM PowerLE*
|SAP HANA DATABASE 2.0
|=========================================================

NOTE: Version for {sapS4pl} on Linux on AMD64/Intel 64 and IBM PowerLE. More information about
the supported combinations of OS and databases for {sapS4insm} or newer can be found at the
SAP Product Availability Matrix at https://apps.support.sap.com/sap/support/pam[SAP PAM].

=== Integrating {sapS4} into the cluster using the Cluster Connector

The integration of the HA cluster through the SAP control framework using the
*{s4sClConnector}* is of special interest. The service `{SAPSTARTSRV}` controls {sap}
instances since {sap} Kernel versions 6.40. One of the classic problems running
{sap} instances in a highly available environment is the following: If an {sap}
administrator changes the status (start/stop) of an {sap} instance without using
the interfaces provided by the cluster software, the cluster framework will
detect that as an error status and will bring the {sap} instance into the old
status by either starting or stopping the {sap} instance. This can result in
very dangerous situations, if the cluster changes the status of an {sap} instance
during some {sap} maintenance tasks. The new updated solution enables the central component
`{SAPSTARTSRV}` to report state changes to the cluster software. This avoids dangerous situations
as previously described.
More details can be found in the blog article "Using sap_vendor_cluster_connector for interaction between cluster
framework and sapstartsrv" at https://blogs.sap.com/2014/05/08/using-sapvendorclusterconnector-for-interaction-between-cluster-framework-and-sapstartsrv/comment-page-1/.

NOTE: If you update from an {sapS4Pl} version less than 1809, read SAP Note 2641019 carefully to adapt your cluster.

.Cluster connector to integrate the cluster with the {sap} start framework
image::sles4sap_clusterconnector.svg[SVG]

NOTE: For this scenario, an updated version of the *{s4sClConnector3}* is used.
It implements the API version 3 for the communication between the cluster
framework and the `{sapstartsrv}` service.

The new version of the *{s4sClConnector3}* allows starting, stopping and migrating
an {sap} instance. The integration between the cluster software and the
`{sapstartsrv}` also implements the option to run checks of the HA setup using either the
command line tool `sapcontrol` or even the {sap} management consoles ({sap} MMC or
{sap} MC). Since version 3.1.0 and later the maintenance mode of cluster resources triggered with SAP
`sapcontrol` commands is supported. See also manual page sap_suse_cluster_connector(8).


=== Sharing disks and NFS

XFS is used for all local file systems. For _/sapmnt/<SID>_ and _/usr/sap/<SID>_, NFS is used.

==== Sharing disk for SBD and mounting NFS for cluster ASCS and ERS

The disk for the fencing mechanism SBD must be shared and assigned to the
cluster nodes {my2nd1} and {my2nd2} in the two-node cluster example. The NFS
file systems for the ASCS and ERS instances are mounted both on {my2nd1} and
{my2nd2}. They could also be mounted on the {sap} application servers (in this
example {my2nd3}/PAS and {my2nd4}/AAS) to simplify the storage layout of the
complete {SAP} system even more.

Make sure that your shared SBD disk
_{myDevA}_ is visible on {my2nd1} and {my2nd2}:

[subs="attributes"]
----
# lsblk | grep {myDevA}
----

During the {sap} software installation you need to mount via NFS and entries in _/etc/fstab_.
////
TODO PRIO3: non line break with in command or variables
////

////
- {my2nd1}:   {myDevPartAscs}   {myMpAscs}
- {my2nd2}:   {myDevPartErs}   {myMpErs}
////

==== Preparing the disk for database and dialog instances (HANA DB)

The disk _{myDevB}_ for the database (260 GB) is assigned to {myNode1} and formatted with XFS.

////
TODO PRIO2: we used a non shared disk. Forgot to mount during installation?!?!?!
////

You can either use YaST or available command line tools to create the partitions.
The following script can be used for non-interactive setups.

.Create Partitions and File Systems for DB and App Servers on {myNode1}
==============================================
[subs="attributes"]
----
# lsblk
# parted -s {myDevB} print
# # we are on the 'correct' drive, right?
# mkfs.xfs {myDevB}
# mkdir /hana
# echo "{myDevB} /hana xfs defaults 0 2" >> /etc/fstab
# mount {myDevB}
----
==============================================


NOTE: {myInstPas}: Since NetWeaver 7.5 the primary application server instance
directory has been renamed to 'D<Instance_Number>'.


.NFS server
- {myNFSSrv}:{myNFSSapmnt}/{mySid} /sapmnt/{mySid}

- {myNFSSrv}:{myNFSUsrSap}/{mySid} /usr/sap/{mySid}

.Media
- {myNFSSrv}:{myNFSSapmedia} /sapmedia

=== Adding IP addresses and virtual names

Check if the file _/etc/hosts_ contains at least the following address resolutions.
Add those entries if they are missing.

[subs="attributes"]
----
{myIPNode1}  {myNode1}
{myIP2nd1}  {my2nd1}
{myIP2nd2}  {my2nd2}
{myIP3nd3}  {my3nd3}

{myVipAAscs}  sap{mySidLc}as
{myVipAErs}  sap{mySidLc}er
{myVipADb}  sap{mySidLc}db
{myVipAPas}  sap{mySidLc}d1
{myVipAAas}  sap{mySidLc}d2
----

////
TODO PRIO2: multinode cluster extension
# 3node cluster and divided application server
----
{myIPNode2}  {myNode2}
{myIPNode3}  {myNode3}
{myIP3nd1}  {my3nd1}
{myIP3nd2}  {my3nd2}
{myIP3nd3}  {my3nd3}
----
////


=== Creating mount points and NFS shares

In the present setup, the directory _/usr/sap_ is part of the root file system. You can
also create a dedicated file system for that area and mount _/usr/sap_
during the system boot. As _/usr/sap_ contains the {sap} control file
_sapservices_ and the *{saphostagent}*, the directory should not be placed on a
shared file system between the cluster nodes.

You need to create the directory structure on all nodes that should run the {sap} resource.

- Create mount points and mounting NFS shares on all non-HANA nodes, for example on
  {my2nd1}, {my2nd2}. If the application servers should also profit from the
  simplified storage setup, they can use the share as well.

.Mount NFS Shares on all non-HANA nodes
==============================================
[subs="attributes"]
----
# mkdir -p /sapmnt/{mySid} /usr/sap/{mySid} /sapmedia
# echo "{myNfsSrv}:{myNFSSapmnt}/{mySid} /sapmnt/{mySid} nfs defaults 0 0" >> /etc/fstab
# echo "{myNfsSrv}:{myNFSUsrSap}/{mySid} /usr/sap/{mySid} nfs defaults 0 0" >> /etc/fstab
# mount /sapmnt/{mySid}
# mount /usr/sap/{mySid}
----
Mount options may depend on your particular environment.
See also manual pages SAPStartsrv_basic_cluster(8) and mount.nfs(8).
==============================================


== Installing the {sap} system

The overall procedure to install the distributed {sap} system is as follows:

.Tasks

. Plan Linux user and group number scheme.
. Install the ASCS instance for the central services.
. Install the ERS to get a replicated enqueue scenario.
. Prepare the ASCS and ERS installations for the cluster take-over.
. Install the database.
. Install the primary application server instance (PAS).
. Install additional application server instances (AAS).

The result will be a distributed {sap} installation as illustrated here:

.Distributed installation of the {sap} system
image::sles4sap_s4hana_distInstall.svg[SVG]

=== Linux user and group number scheme

Whenever asked by the {sap} software provisioning manager (SWPM) which Linux User
IDs or Group IDs to use, refer to the following table as an example:

[subs="attributes"]
----
Group sapinst      1001
Group sapsys       1002
Group {hanaSidDBLc}shm       1003

User  {mysapadm}       2001
User  sapadm       2002
User  {hanaDBadm}       2003
----

=== Installing ASCS on {my2nd1}

Temporarily, as the local IP address, set the service IP address which you will later use in the
cluster, because the installer needs to be able to resolve and use it.
Make sure to use the correct virtual host name for each installation step.
If applicable, make sure to mount file systems like _/sapmedia/_.

[subs="attributes"]
----
# ip a a {myVipAAscs}{myVipNM} dev eth0
# # if not mounted yet, mount these now
# mount /sapmnt/{mySid}
# mount /usr/sap/{mySid}
# cd {mySAPinst}
# ./sapinst SAPINST_USE_HOSTNAME={myVipNAscs}
----

* SWPM product installation path:
** Installing {sapS4insm}  -> SAP HANA DATABASE -> Installation -> Application Server ABAP
-> High-Availability System -> ASCS Instance
* Use SID {mySid}.
* Use instance number {myAscsIno}.
* Deselect using FQDN.
* All passwords: use {mySapPwd}.
* Double-check during the parameter review if virtual name *{myVipNAscs}* is used.
* If you get an error during the installation about permissions, change the
  ownership of the ASCS directory.

[subs="attributes"]
----
# chown -R {mysapadm}:sapsys /usr/sap/{mySid}/{myInstAscs}
----

=== Installing ERS on {my2nd2}

Temporarily, as the local IP address, set the service IP address which you will later use in the
cluster, because the installer needs to be able to resolve and use it.
Make sure to use the correct virtual host name for each installation step.

[subs="attributes"]
----
# ip a a {myVipAErs}{myVipNM} dev eth0
# # if not mounted yet, mount these now
# mount /sapmnt/{mySid}
# mount /usr/sap/{mySid}
# cd {mySAPinst}
# ./sapinst SAPINST_USE_HOSTNAME={myVipNErs}
----

* SWPM product installation path:
** Installing {sapS4insm}  -> SAP HANA DATABASE -> Installation -> Application Server ABAP ->
High-Availability System -> ERS Instance
* Use instance number {myErsIno}.
* Deselect using FQDN.
* Double-check during the parameter review that virtual name *{myVipNErs}* is used.
* If you get an error during the installation about permissions, change the
  ownership of the ERS directory.

[subs="attributes"]
----
# chown -R {mysapadm}:sapsys /usr/sap/{mySid}/{myInstErs}
----

* If you get a prompt to manually stop/start the ASCS instance, log in to
{my2nd1} as user {mysapadm} and call 'sapcontrol'.

[subs="attributes"]
----
# sapcontrol -nr {myAscsIno} -function Stop    # to stop the ASCS
# sapcontrol -nr {myAscsIno} -function Start   # to start the ASCS
----

=== Performing subsequent steps for ASCS and ERS

After installation, you can perform several subsequent steps on the ASCS and ERS instances.

==== Stopping ASCS and ERS

To stop the ASCS and ERS instances, use the commands below.
On _{my2nd1}_, do the following:

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myAscsIno} -function Stop
# sapcontrol -nr {myAscsIno} -function StopService
----

On _{my2nd2}_, do the following:

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myErsIno} -function Stop
# sapcontrol -nr {myErsIno} -function StopService
----

==== Disabling *systemd* services of the ASCS and the ERS {sap} instance

This is mandatory for giving control over the instance to the HA cluster.
See also manual pages ocf_suse_SAPStartSrv(7) and SAPStartSrv_basic_Cluster(7).

[subs="specialchars,attributes"]
----
# systemctl disable SAP{mySid}_{myAscsIno}.service
# systemctl stop SAP{mySid}_{myAscsIno}.service
# systemctl disable SAP{mySid}_{myErsIno}.service
# systemctl stop SAP{mySid}_{myErsIno}.service
----
NOTE: Stopping this instance services will stop the SAP instance as well.
Starting the instance services will not start the SAP instances.

- Check the {sap} *systemd* integration:

[subs="specialchars,attributes"]
----
# systemctl list-unit-files | grep SAP
SAP{mySid}_{myAscsIno}.service disabled
SAP{mySid}_{myErsIno}.service disabled
----
The instance services are indeed disabled, as required.

[subs="specialchars,attributes"]
----
# systemctl list-unit-files | grep sap
saphostagent.service enabled
sapinit.service generated
saprouter.service disabled
saptune.service enabled
----
The mandatory `saphostagent` service is enabled. This is the installation default.
Some more {sap} related services might be enabled, for example the recommended `saptune`.

[subs="specialchars,attributes"]
----
# cat /usr/sap/sapservices
systemctl --no-ask-password start SAP{mySid}_{myAscsIno} # sapstartsrv pf=/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs}
systemctl --no-ask-password start SAP{mySid}_{myErsIno} # sapstartsrv pf=/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstErs}_{myVipNErs}
----
The _sapservices_ file is still there for compatibility. It shows native *systemd* commands, one per
line for each registered instance.
You will find a SystemV style example in the appendix.
// TODO PRIO3: link to example

==== Integrating the cluster framework using _{s4sClConnector3}_

Install the package `{s4sClConnector3}` version 3.1.0 from the SUSE
repositories:

[subs="attributes"]
----
# zypper in {s4sClConnector3}
----

NOTE: The package `{s4sClConnector3}` contains the version 3.x.x (SAP API 3).
The package `{s4sClConnector3}` with version 3.0.x implements the SUSE SAP API
version 3. New features like SAP Rolling Kernel Switch (RKS) and migration of ASCS are
only supported with this new version.
The package`{s4sClConnector3}` with version 3.1.x supports in addition the maintenance mode of
cluster resources triggered from SAP tools.

For the ERS and ASCS instances, edit the instance profiles
{mySid}_{myInstAscs}_{myVipNAscs} and {mySid}_{myInstErs}_{myVipNErs} in the
profile directory _/usr/sap/{mySid}/SYS/profile/_.

Tell the `{sapStartSrv}` service to load the HA script connector library and to
use the connector `{s4sClConnector3}`. On the other hand, make sure the
feature _Autostart_ is *not* used.

[subs="attributes"]
----
service/halib = $(DIR_EXECUTABLE)/saphascriptco.so
service/halib_cluster_connector = /usr/bin/sap_suse_cluster_connector
----

Add the user _{mySapAdm}_ to the Unix user group _haclient_.

[subs="attributes"]
----
# usermod -a -G haclient {mySapAdm}
----
See also manual pages sap_suse_cluster_connector(8), usermod(8) and groupmod(8).

==== Adapting {sap} profiles to match the {sapCert} certification

For the ASCS instance, change the start command from _Restart_Program_xx_ to
_Start_Program_xx_ for the enqueue server (Enqueue Server 2). This change tells the
{sap} start framework *not* to self-restart the enqueue process. Such a restart
would result in a loss of the locks.

File */usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs}*:

[subs="attributes"]
----
Start_Program_01 = local $(_ENQ) pf=$(_PF)
----

Optionally, you can limit the number of restarts of services (in the case of
ASCS, this limits the restart of the message server).

For the ERS instance, change the start command from _Restart_Program_xx_ to
_Start_Program_xx_ for the enqueue replication server (Enqueue Replicator 2).

File */usr/sap/{mySid}/SYS/profile/{mySid}_{myInstErs}_{myVipNErs}*:

[subs="attributes"]
----
Start_Program_00 = local $(_ENQR) pf=$(_PF) NR=$(SCSID)
----
// TODO PRIO2: Check if the variable is _PF or _PFL

==== Starting ASCS and ERS

To start the ASCS and ERS instances, use the commands below.

On _{my2nd2}_, do the following

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myErsIno} -function StartService {mySid}
# sapcontrol -nr {myErsIno} -function Start
----

On _{my2nd1}_, do the following:

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myAscsIno} -function StartService {mySid}
# sapcontrol -nr {myAscsIno} -function Start
----

=== Installing database on {myNode1}

The HANA DB has very strict HW requirements. The storage sizing depends on many
indicators. Check the supported configurations at
https://support.sap.com/en/release-upgrade-maintenance.html#section_1969201630[SAP HANA Hardware Directory]
and https://www.sap.com/documents/2016/05/e8705aae-717c-0010-82c7-eda71af511fa.html[SAP HANA TDI].

[subs="attributes"]
----
# ip a a {myVipADb}{myVipNM} dev eth0
# mount {myDevB} {hanaMPDb}
# cd {mySAPinst}
# ./sapinst SAPINST_USE_HOSTNAME={myVipNDb}
----

* SWPM product installation path:
** Installing {sapS4insm}  -> SAP HANA DATABASE -> Installation -> Application Server ABAP ->
High-Availability System -> Database Instance
* Profile directory is _/sapmnt/{mySid}/profile_.
* Deselect using FQDN.
* Database parameters : Database ID (DBSID) is {hanaSidDB};
Database Host is {myVipNDb}; Instance Number is {hanaDBIno}.
* Database System ID enter Instance Number is {hanaDBIno}; SAP Mount Directory is
/sapmnt/{mySID}/profile.
* Account parameters: change them in case of custom values needed.
* Clean-up: select *Yes*, remove operating system users from the group `sapinst`.
* Double-check during the parameter review, if virtual name *{myVipNDb}* is
  used.

=== Installing the primary application server (PAS) on {my2nd3}


[subs="attributes"]
----
# ip a a {myVipAPas}{myVipNM} dev eth0
# mount {myDevPartPas} {myMPPas}
# cd {mySAPinst}
# ./sapinst SAPINST_USE_HOSTNAME={myVipNPas}
----

* SWPM product installation path:
** Installing {sapS4insm}  -> SAP HANA DATABASE -> Installation -> Application Server ABAP ->
High-Availability System -> Primary Application Server Instance
* Use instance number {myPasIno}.
* Deselect using FQDN.
* For this example setup, we have used a default secure store key.
* Do not install Diagnostic Agent.
* No SLD is used.
* Double-check during the parameter review, if virtual name *{myVipNPas}* is used.

=== Installing an additional application server (AAS) on {my2nd4}

[subs="attributes"]
----
# ip a a {myVipAAas}{myVipNM} dev eth0
# mount {myDevPartSec} {myMPSec}
# cd {mySAPinst}
# ./sapinst SAPINST_USE_HOSTNAME={myVipNDSec}
----

* SWPM product installation path:
** Installing {sapS4insm}  -> SAP HANA DATABASE -> Installation -> Application Server ABAP ->
High-Availability System -> Additional Application Server Instance
* Use instance number {myDSecIno}.
* Deselect using FQDN.
* Do not install Diagnostic Agent.
* Double-check during the parameter review, if virtual name *{myVipNDSec}* is used.

=== Optional: Preparing additional cluster nodes

If you install a cluster with three or more nodes to control the ASCS and ERS, you need to
prepare these nodes to have {sap} Linux users and system configuration in place to be ready
to run the SAP instances. SWPM 2.0 already includes an installation option to prepare
additional cluster nodes. In this case `sapinst` is called without SAPINST_USE_HOSTNAME.

Ensure that the {sap} NFS shares are also mounted on the additional cluster node.

[subs="attributes"]
----
# cd {mySAPinst}
# ./sapinst
----

* SWPM product installation path:
** Installing {sapS4insm}  -> SAP HANA DATABASE -> Installation -> Application Server ABAP ->
High-Availability System -> Prepare Additional Cluster Node
* Provide SAP profile path.
* Double-check the parameter review.


== Implementing the cluster

The main procedure to implement the cluster is as follows:

.Tasks

. Prepare the operating system and install the cluster software.
. Configure the cluster base including corosync and resource manager.
. Configure the cluster resources.
. Tune the cluster timing in special for the SBD.

///////////////////////////////
TODO PRIO3: Do we really need to stop, unconfigure and unmount?
Maybe we find a way to configure the resources that the cluster just
accepts the already started resource groups - lets see ;-)
///////////////////////////////

NOTE: Before you continue to set up the cluster, perform the following actions:
First stop all SAP instances. Then remove the (manually added) IP addresses on the cluster nodes.
Finally unmount the file systems which will be controlled by the cluster later.

NOTE: The SBD device/partition needs to be created beforehand. Double-check which
device/partition to use! In this setup guide, a disk _{myDevA}_
is already reserved for SBD usage.

=== Preparing the operating system and installing the cluster software

- Set up and enable `chrony` with `yast2`.

// TODO PRIO3: pattern sap-nw?
- Install the RPM pattern *ha_sles* and package `sapstartsrv-resource-agents` on both cluster nodes.
+
[subs="attributes"]
----
# zypper in -t pattern ha_sles
# zypper in sapstartsrv-resource-agents
----

=== Configuring the cluster base

.Tasks

- To configure the cluster base, you can use either YaST or the interactive
command line tool `ha-cluster-init`. The example below uses the command line wizard.

- Install and configure the watchdog device on the first machine.

Instead of deploying the software-based solution, rather use a hardware-based watchdog device.
The following example uses the software device but can be easily adapted to the hardware device.

[subs="attributes"]
----
# modprobe softdog
# echo softdog > /etc/modules-load.d/watchdog.conf
# systemctl restart systemd-modules-load
# lsmod | egrep "(wd|dog|i6|iT|ibm)"
----

- Install and configure the cluster stack on the first machine

[subs="attributes"]
----
# ha-cluster-init -u -s  {myDevA}
----

- Join the second node.

On the second node, perform some preparation steps.

[subs="attributes"]
----
# modprobe softdog
# echo softdog > /etc/modules-load.d/watchdog.conf
# systemctl restart systemd-modules-load
# lsmod | egrep "(wd|dog|i6|iT|ibm)"
----

To configure the cluster base, you can use either YaST or the interactive
command line tool `ha-cluster-join`. The example below uses the command line wizard.

[subs="attributes"]
----
# ha-cluster-join -c {my2nd1}
----

- The _crm_mon -1r_ output should look as follows:

[subs="attributes"]
----
Stack: corosync
Current DC: {my2nd1} (version 1.1.18+20180430.b12c320f5-1.14-b12c320f5) - partition with quorum
Last updated: Mon Jan 28 13:10:37 2019
Last change: Wed Jan 23 09:52:57 2019 by root via cibadmin on {my2nd1}

2 nodes configured
1 resource configured

Online: [ {my2nd1} {my2nd2} ]

stonith-sbd	(stonith:external/sbd):	Started {my2nd1}
----

=== Configuring cluster resources

The SAPInstance resource configuration is needed to
start and stop the ASCS and the ERS instances themselves.
See manual page ocf_heartbeat_SAPInstance(7) for details.

The SAPStartSrv resource starts and stops the `sapstartsrv` service and
guarantees that only one instance is running per cluster at the same time.
See manual page ocf_suse_SAPStartSrv(7) for details.

////////////////////////////////////
TODO PRIO1: We need to adap this section, if the new enqueue2 is published.
////////////////////////////////////

With the new version of ENSA2, the ASCS instance can be started on the same host. There is no longer a need
to follow the ERS instance. The ASCS instance receives the enqueue lock table over the network from the ERS instance. If no
other node is available, the ASCS instance will be started on the same host where the ERS instance is running.

.Resources and constraints
image::sles4sap_s4hana2021_resources_simple_mount.svg[SVG]

////
The implementation is done using a new flag "runs_ers_$SID" within
the resource agent (RA), enabled using the resource parameter "IS_ERS=TRUE".
////

Another benefit of this concept is that you can work with native (mountable)
file systems instead of a shared (NFS) file system for the {sap} instance
directories.

==== Preparing the cluster for adding the resources

To prevent the cluster from starting partially defined resources, set the cluster
to the maintenance mode. This deactivates all monitor actions.

As user _root_, type the following command:

[subs="attributes"]
----
# crm configure property maintenance-mode="true"
----
////
TODO PRIO2: for 2+ node cluster different SBD settings
==== Configure the SBD Resource
Verify the SBD cluster configuration and if needed, modify them as described.

.SBD stonith
================================================
[subs="attributes"]
----
# crm resource param stonith-sbd set pcmk_action_limit -1
----
================================================
////

==== Preparing the new SAPStartSrv resource agent implementation

As a prerequisite for having a single NFS mount for ASCS and ERS, the
`sapstartsrv` instance agents of ASCS and ERS must not be started by the
`sapinit` service during system start-up, as these services are started and
stopped by dedicated cluster resources.

With the `sapstartsrv-resource-agents` RPM package there come two *systemd*
services called `sapping` and `sappong`.  `sapping` runs before `sapinit` and
moves _/usr/sap/sapservices_ out of the way. Consequently, the `sapstartsrv`
instance agents are not started automatically by `sapinit`. `sappong` runs after `sapinit`
and moves _/usr/sap/sapservices_ back to its original location.

On {my2nd1} and {my2nd2}, check for the  `sapstartsrv-resource-agents` package and
enable the `sapping` and `sappong` services.

================================================
[subs="attributes"]
----
# zypper info sapstartsrv-resource-agents
# systemctl enable sapping
# systemctl enable sappong
----
================================================
See manual pages ocf_suse_SAPStartSrv(7), sapping(8) and SAPStartSrv_basic_cluster(7)
for details.

==== Configuring resources for the ASCS Instance

First, configure the resources for the IP address, the {sap} instance agent and
the {sap} instance. You need to adapt the parameters for your specific environment.

Make sure that in the {sap} instance definition the parameter *MINIMAL_PROBE* is
set to *true*.

.ASCS primitive
================================================
[subs="attributes"]
----
primitive rsc_ip_{mySID}_{myInstAscs} IPaddr2 \
  params ip={myVipAAscs} \
  op monitor interval=10 timeout=20
primitive rsc_SAPStartSrv_{mySID}_{myInstAscs} ocf:suse:SAPStartSrv \
  params InstanceName={mySID}_{myInstAscs}_{myVipNAscs}
primitive rsc_sap_{mySID}_{myInstAscs} SAPInstance \
  op monitor interval=11 timeout=60 on-fail=restart \
  params InstanceName={mySID}_{myInstAscs}_{myVipNAscs} \
     START_PROFILE="/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs}" \
     AUTOMATIC_RECOVER=false MINIMAL_PROBE=true \
  meta resource-stickiness=5000
----
================================================
The shown SAPInstance monitor timeout is a trade-off between fast recovery of
the ASCS vs. resilience against sporadic temporary NFS issues. You may slightly
increase it to fit your infrastructure. Consult your storage or NFS server
documentation for appropriate timeout values. Make sure the SAPStartSrv resource
has *NO* monitor operation configured.
See also manual pages ocf_heartbeat_SAPInstance(7), ocf_heartbeat_IPaddr2(7) ocf_suse_SAPStartSrv(7)
and nfs(5).

.ASCS group
================================================
[subs="attributes"]
----
group grp_{mySID}_{myInstAscs} \
  rsc_ip_{mySID}_{myInstAscs} rsc_SAPStartSrv_{mySID}_{myInstAscs} rsc_sap_{mySID}_{myInstAscs} \
  meta resource-stickiness=3000
----
================================================

Create a _txt_ file (like _crm_ascs.txt_) with your preferred text editor. Add
both examples (primitives and group) to that file and load the configuration to
the cluster manager configuration.

As user _root_, type the following command:

[subs="attributes"]
----
# crm configure load update crm_ascs.txt
----

==== Configuring resources for the ERS Instance

Next, configure the resources for the IP address, the {sap} instance agent
and the {sap} instance. You need to adapt the parameters for your specific
environment.

Make sure that in the {sap} instance definition the
parameter *MINIMAL_PROBE* is set to *true*.

The specific parameter *IS_ERS=true* must only be set for the ERS instance.

.ERS primitive
================================================
[subs="attributes"]
----
primitive rsc_ip_{mySID}_{myInstErs} IPaddr2 \
  params ip={myVipAErs} \
  op monitor interval=10 timeout=20
primitive rsc_SAPStartSrv_{mySID}_{myInstErs} ocf:suse:SAPStartSrv \
  params InstanceName={mySID}_{myInstErs}_{myVipNErs}
primitive rsc_sap_{mySID}_{myInstErs} SAPInstance \
  op monitor interval=11 timeout=60 on-fail=restart \
  params InstanceName={mySID}_{myInstErs}_{myVipNErs} \
     START_PROFILE="/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstErs}_{myVipNErs}" \
     AUTOMATIC_RECOVER=false IS_ERS=true MINIMAL_PROBE=true
----
================================================
The shown SAPInstance monitor timeout is a trade-off between fast recovery of
the ERS vs. resilience against sporadic temporary NFS issues. You may slightly
increase it to fit your infrastructure. Consult your storage or NFS server
documentation for appropriate timeout values. Make sure the SAPStartSrv resource
has *NO* monitor operation configured.
See also manual pages ocf_heartbeat_SAPInstance(7), ocf_heartbeat_IPaddr2(7) ocf_suse_SAPStartSrv(7)
and nfs(5).

.ERS group
================================================
[subs="attributes"]
----
group grp_{mySID}_{myInstErs} \
  rsc_ip_{mySID}_{myInstErs} rsc_SAPStartSrv_{mySID}_{myInstErs} rsc_sap_{mySID}_{myInstErs}
----
================================================

Create a _txt_ file (like _crm_ers.txt_) with your preferred text editor. Add
both examples (primitives and group) to that file and load the configuration to
the cluster manager configuration.

As user _root_, type the following command:

[subs="attributes"]
----
# crm configure load update crm_ers.txt
----

==== Configuring the colocation constraints between ASCS and ERS

Compared to the ENSA1 configuration, the constraints between the ASCS and ERS instances are changed.
An ASCS instance should avoid starting up on the cluster node running the ERS instance if any other
node is available. Today the ENSA2 setup can resynchronize the lock table over the network.

If the ASCS instance has been started by the cluster on the ERS node, the ERS instance should
be moved to another cluster node (col_sap_{mysid}_no_both). This constraint
is needed to ensure that the ERS instance will synchronize the locks again and the cluster is
ready for an additional take-over.

.Location constraint
================================================
[subs="attributes"]
----
colocation col_sap_{mysid}_separate -5000: grp_{mySID}_{myInstErs} grp_{mySID}_{myInstAscs}
order ord_sap_{mysid}_ascs_first Optional: rsc_sap_{mySID}_{myInstAscs}:start \
      rsc_sap_{mySID}_{myInstErs}:stop symmetrical=false
----
================================================

Create a _txt_ file (like _crm_col.txt_) with a text editor. Add
both constraints to that file and load the configuration to the
cluster manager configuration.

As user _root_, type the following command:

[subs="attributes"]
----
# crm configure load update crm_col.txt
----

==== Activating the cluster

The last step is to end the cluster maintenance mode and to allow the
cluster to detect already running resources.

As user _root_, type the following command:

[subs="attributes"]
----
# crm configure property maintenance-mode="false"
----

////
 TODO PRIO3: Subsection "tune cluster timeouts and sbd"
////


////
############################
#
# ADMINISTRATION
#TODO PRIO3: renaming Do and Don't to Operational Tasks
############################
////


== Administration

=== Dos and Don'ts

NOTE: Before each test, verify that the cluster is in idle state, no migration constraints are active,
and no resource failure messages are visible. Start each procedure with a clean setup.

A minimal example sequence for checking the cluster status might look like the following:

[subs="attributes"]
----
# crm_mon -1r
# crm configure show | grep cli-
# cs_clusterstate -i
----

See also manual pages cs_clusterstate(8), crm(8) and crm_mon(8).

==== Maintenance procedure for a Linux cluster or operating system with ASCS and ERS instances remain running

Check state of Linux cluster and ASCS/ERS:

[subs="attributes"]
----
# cs_clusterstate -i
----

This must return the following output:

----
Cluster state: S_IDLE
----

Obtain a cluster summary, list of nodes, and a full list of resources:

[subs="attributes"]
----
# crm_mon -1r
----

Check for any undesired location constraints:

[subs="attributes"]
----
# crm configure show|grep cli-
----

If the command returns no output, it means that there are no undesired constraints.

Get a list of system instances:

[subs="attributes"]
----
# su - en2adm -c "sapcontrol -nr 00 -function GetSystemInstanceList"
----

Get a list of processes running on the ASCS and ERS instances:

[subs="attributes"]
----
# su - en2adm -c "sapcontrol -nr 00 -function GetProcessList"
----

Check whether maintenance mode is set in the cluster configuration:

[subs="attributes"]
----
# su - en2adm -c "sapcontrol -nr 00 -function HACheckMaintenanceMode"
----

Get the information about cluster solution, available HA nodes, and the active node where the given instance is running:

[subs="attributes"]
----
# su - en2adm -c "sapcontrol -nr 00 -function HAGetFailoverConfig"
----

Before you set the Linux cluster into the maintenance mode, check its state by running the `cs_clusterstate -i` command. Then run the command below to set the cluster into the maintenance mode:

[subs="attributes"]
----
# crm maintenance on
----

Stop the Linux cluster on all nodes:

[subs="attributes"]
----
# crm cluster stop
----

You can now perform maintenance on the Linux cluster or system. Before doing the system maintenance, you must bring down SAP instances as necessary. If the SAP instances were brought down, then you also need to bring up the SAP instances before activating the cluster.

When the maintenance is complete, start the Linux cluster on all nodes:

[subs="attributes"]
----
# crm cluster start
----

Let the Linux cluster detect the status of the ASCS and ERS resources:

[subs="attributes"]
----
# crm resource refresh rsc_sap_EN2_ASCS00
# crm resource refresh rsc_sap_EN2_ERS10
----

Set the cluster ready for operations:

[subs="attributes"]
----
# cs_clusterstate -i
# crm maintenance off
----

Check the status of the Linux cluster and ASCS/ERS:

[subs="attributes"]
----
# cs_clusterstate -i
# crm_mon -1r
# crm configure show|grep cli-
# su - en2adm -c "sapcontrol -nr 00 -function GetSystemInstanceList"
# su - en2adm -c "sapcontrol -nr 00 -function GetProcessList"
# su - en2adm -c "sapcontrol -nr 10 -function GetProcessList"
# su - en2adm -c "sapcontrol -nr 00 -function HACheckMaintenanceMode"
# su - en2adm -c "sapcontrol -nr 00 -function HAGetFailoverConfig"
# cs_clusterstate -i
----

==== Migrating the ASCS instance

To *migrate* the ASCS {sap} instance, you should use {sap} tools such as
the {sap} management console. This will trigger *{sapStartSrv}* to use the
{s4sClConnector3} to migrate the ASCS instance. As user _{mysapadm}_ you can run
the command below to migrate the ASCS. This will always
migrate the ASCS to the ERS side which will keep the {sap} enqueue locks.

As user _{mysapadm}_, type the command:

[subs="attributes"]
----
# sapcontrol -nr {myAscsIno} -function HAFailoverToNode ""
----

==== Using unique instance numbers

All {sap} instance numbers controlled by the cluster must be unique. If you need
multiple dialog instances with the same instance number running on different
systems, they must not be controlled by the cluster.

==== Setting the cluster to maintenance mode

The procedure to set the cluster into maintenance mode can be executed as user _root_ or _sidadm_.

As user _root_, type the following command:

[subs="attributes"]
----
# crm configure property maintenance-mode="true"
----

As user _{mysapadm}_, type the following command (the full path is needed):

[subs="attributes"]
----
# /usr/sbin/crm configure property maintenance-mode="true"
----

==== Stopping the cluster maintenance

The procedure to end the maintenance mode for the cluster can be executed as user _root_.
Type the following command:

[subs="attributes"]
----
# crm configure property maintenance-mode="false"
----
See also manual page crm(8).
// TODO PRIO1: use resource maintenance instead of cluster maintenance

==== Starting the Resource Maintenance Mode

The procedure to start the resource maintenance mode can be executed as user _{mysapadm}_.
This sets the ASCS and ERS cluster resource to *unmanaged*.

As user _{mysapadm}_, type the command:

[subs="attributes"]
----
# sapcontrol -nr {myAscsIno} -function HASetMaintenanceMode 1
----

==== Stopping the resource maintenance mode

The procedure to start the resource maintenance mode can be executed as user _{mysapadm}_.
This sets the ASCS and ERS cluster resource to *managed*.

As user _{mysapadm}_, type the command:

[subs="attributes"]
----
# sapcontrol -nr {myAscsIno} -function HASetMaintenanceMode 0
----

==== Cleaning up resources

You can also *clean up resource failures*. Failures are automatically deleted to
allow a failback after a specified period of time. You can also clean up the
status, including the failures, by running the following command as root:

[subs="attributes"]
----
# crm resource cleanup RESOURCE-NAME
----

=== Testing the cluster

It is strongly recommended to perform at least the following tests before you
go into production with your cluster:

==== Checking product names with HAGetFailoverConfig

Check if the name of the SUSE cluster solution is shown in the output of
  `sapcontrol` or the {sap} management console. This test checks the status of the
  {sapS4} cluster integration.

As user _{mysapadm}_, type the following command:

[subs="attributes"]
----
# sapcontrol -nr {myAscsIno} -function HAGetFailoverConfig
----

==== Running SAP checks using HACheckConfig and HACheckFailoverConfig

Check if the HA configuration tests are passed successfully and do not produce error messages.

As user _{mysapadm}_, type the following commands:

[subs="attributes"]
----
# sapcontrol -nr {myAscsIno} -function HACheckConfig
# sapcontrol -nr {myAscsIno} -function HACheckFailoverConfig
----

==== Manually migrating ASCS

Check if manually migrating the ASCS instance using HA tools works properly.

As user _root_, run the following commands:

[subs="attributes"]
----
# crm resource migrate rsc_sap_{mySid}_{myInstAscs} force
## wait until the ASCS is been migrated to the ERS host
# crm resource unmigrate rsc_sap_{mySid}_{myInstAscs}
----

==== Migrating ASCS using HAFailoverToNode

Check if moving the ASCS instance using {sap} tools like {sapCtrl} works properly.

As user _{mysapadm}_, type the following command:

[subs="attributes"]
----
# sapcontrol -nr {myAscsIno} -function HAFailoverToNode ""
----

==== Test ASCS migration after operating system failure

Check if the ASCS instance moves correctly after a node failure.
This test will immediately trigger a hard reboot of the node.

As user _root_, type the following command:

[subs="attributes"]
----
## on the ASCS host
# echo b >/proc/sysrq-trigger
----

==== Restarting ASCS in-place using `Stop` and `Start`

Check if the in-place restart of the {sap} resources have been processed
  correctly. The {sap} instance should not failover to an other node, it
  must start on the same node where it has been stopped.

////
TODO PRIO1: Is the statement still valid? I guess no.
WARNING: This test will force the SAP system to *lose* the enqueue locks.
   *This test should not be processed during production.*
////
As user _{mysapadm}_, do the following:

[subs="attributes"]
----
## example for ASCS
# sapcontrol -nr {myAscsIno} -function Stop
# sapcontrol -nr {myAscsIno} -function WaitforStopped 60 20
## cs_clusterstate -i
# sapcontrol -nr {myAscsIno} -function Start
----

==== Performing an automated restart of the ASCS instance (simulating rolling kernel switch)

The next test should proof that the cluster solution did nor interact neither try to restart the ASCS instance
during a maintenance procedure. In addition, it should verify that no locks are lost during the restart of
an ASCS instance during a rolling kernel switch (RKS) procedure. The cluster solution should recognize that the restart of
the ASCS instance was expected. No failure or error should be reported or counted.

Optionally, you can set locks and verify that they still exist after the maintenance procedure. There are multiple
ways to do that. One example test can be performed as follows:

. Log in to your SAP system and open the transaction SU01.
. Create a new user. Do not finish the transaction to see the locks.
. With the SAP MC / MMC, check if there are locks available.
. Open the ASCS instance entry and go to _Enqueue Locks_.
. With the transaction SM12, you can also see the locks.

Do this test multiple times in a short time frame. The restart of the ASCS instance in the example below happens five times.

As user _{mysapadm}_, create and execute the following script:
[subs="attributes"]
----
$ cat ascs_restart.sh
#!/bin/bash
for lo in 1 2 3 4 5; do
  echo LOOP "$lo - Restart ASCS{myAscsIno}"
  sapcontrol -host sap{mySidLc}as -nr {myAscsIno} -function StopWait 120 1
  sleep 1
  sapcontrol -host sap{mySidLc}as -nr {myAscsIno} -function StartWait 120 1
  sleep 1
done
----
[subs="attributes"]
----
$ bash ascs_restart.sh
----

==== Performing an RKS

The RKS is an automated procedure that enables the kernel in an ABAP system
to be exchanged without any system downtime. During an RKS, all instances of the system, and
generally all SAP start services (`sapstartsrv`), are restarted.

. Check in SAP note 953653 whether the new kernel patch is RKS compatible to your currently running kernel.
. Check SAP note 2077934 - Rolling kernel switch in HA environments.
. Download the new kernel from the SAP service market place.
. Make a backup of your current central kernel directory.
. Extract the new kernel archive to the central kernel directory.
. Start the RKS via SAP MMC, system overview (transaction SM51) or via command line.
. Monitor and check the version of your SAP instances with the SAP MC / MMC or with *sapcontrol*.

As user _{mysapadm}_, type the following commands:

[subs="specialchars,attributes"]
----
## sapcontrol [-user <sidadm psw>] -host <host> -nr <INSTANCE_NR> -function UpdateSystem 120 300 1
# sapcontrol -user {mySapAdm} {mySapPwd} -host {myVipNAscs} -nr {myAscsIno} -function UpdateSystem 120 300 1
# sapcontrol -nr {myAscsIno} -function GetSystemUpdateList -host {myVipNAscs} \
  -user {mySapAdm} {mySapPwd}
# sapcontrol -nr {myAscsIno} -function GetVersionInfo -host {myVipNAscs} \
  -user {mySapAdm} {mySapPwd}
# sapcontrol -nr {myErsIno} -function GetVersionInfo -host {myVipNErs} \
  -user {mySapAdm} {mySapPwd}
# sapcontrol -nr {myPasIno} -function GetVersionInfo -host {myVipNPas} \
  -user {mySapAdm} {mySapPwd}
# sapcontrol -nr {myDSecIno} -function GetVersionInfo -host {myVipNDSec} \
  -user {mySapAdm} {mySapPwd}
----


==== Additional tests

In addition to the already performed tests, you should do the following:

* Check the recoverable and non-recoverable outage of the message server process.

* Check the non-recoverable outage of the {sap} enqueue server process.

* Check the outage of the {sapERS}.

* Check the outage and restart of {sapStartSrv}.

* Check the simulation of an upgrade.

* Check the simulation of cluster resource failures.

//:leveloffset: 0


[[multicluster]]
== Multi-node cluster setups for {sapS4}

*Multi-node cluster* setups mean cluster configurations with more than two nodes.
Depending on the starting point it is possible to extend a two-node cluster setup or directly start with
more than two nodes for an ASCS / ERS high availability setup.
The examples below will show the setting up of multi-node cluster and the extension of an existing two node cluster pair.
The major configuration changes will be shown and the basic preparation of the new cluster member node.

The task list to set up the three node cluster is similar to the task list for the two-node cluster.
However some details are described different here to get a diskless SBD setup.
Such a diskless SBD setup is an optional improvement for three nodes, but does not work for two nodes.
On the other hand, priority fencing is an optional improvement for two nodes, but does not work for three nodes.
An example priority fencing configuration for the two-node cluster is shown in the appendix.
See the {sleHA} product documentation for details
(https://documentation.suse.com/sle-ha/15-SP3/single-html/SLE-HA-administration/#pro-ha-storage-protect-fencing).

NOTE: When extending a cluster from two to three nodes, make sure to not use priority fencing.

////
.Tasks

. OS preparation and install the cluster software
. Preparing watchdog and SBD parameters for diskless SBD
. Configure the cluster base including corosync and resource manager with diskless SBD
. Configure the cluster resources
. Tune the cluster timing in special for the SBD.
////

////
// BEGIN OF BLOCKED CHAPTERS FOR MULTINODE 2019-06-18
=== Setting up multi-node cluster from the top with diskless SBD

A fencing mechanism is essential for cluster controlled infrastructures. Sometimes there is no possibility
to have a shared block device or an IPMI device. A solution for such environments could be the
diskless SBD option. SBD can be operated in a diskless mode. In this mode, a watchdog device will be
used to reset the node in the following cases: if it loses quorum, if any monitored daemon is lost and not
recovered, or if Pacemaker decides that the node requires fencing. Diskless SBD is based on âself-fencingâ
of a node, depending on the status of the cluster, the quorum and some reasonable assumptions.

IMPORTANT: Do not use diskless SBD as a fencing mechanism for two-node clusters. Use it only
in clusters with an un-even number of nodes (like three).
SBD in diskless mode cannot handle split brain scenarios for two-node clusters.

==== Preparing the operating system and installing the cluster software

In general the OS preparation is similar to the procedure described for the two node cluster.
This includes the setup of NTP as well as installing the software pattern *ha_sles* using *yast2* or *zypper*.

==== Preparing watchdog and SBD parameters for diskless SBD

Diskless SBD does not share a drive for exchanging SBD status. Thus we need to set up SBD parameters carefully
before we initialize the cluster.

.Checking watchdog and start values for diskless SBD
==========
All following commands are executed as user _root_ .
Check if the correct watchdog device is loaded and active.

[subs="specialchars,attributes,quotes"]
----
# lsmod | egrep "(wd|dog|i6|iT|ibm)"
----

Check and set the parameters in _/etc/sysconfig/sbd_

[subs="specialchars,attributes,quotes"]
----
SBD_PACEMAKER=yes
SBD_STARTMODE=always
SBD_DELAY_START=no
SBD_WATCHDOG_DEV=/dev/watchdog
SBD_WATCHDOG_TIMEOUT=5
----

// TODO PRIO3: diskless sbd does not always work, see #1192467

==========

==== Configuring the multi-node cluster base with diskless SBD

.Creating a multi-node cluster
==========
This example uses three node but can easily extend to more. A dedicate network interface is used for
the corosync communication (eth1). The option *-S* is special for the diskless mode.

On the first node, execute the following command:
[subs="specialchars,attributes,quotes"]
----
# ha-cluster-init -u -i eth1 -S
----

On all remaining nodes, execute the following command:
[subs="specialchars,attributes,quotes"]
----
# ha-cluster-join -i eth1 -c <first cluster node>
----

After all nodes have joined check the cluster status:

[subs="specialchars,attributes,quotes"]
----
#  crm status

	Stack: corosync
	Current DC: <node1> (version 1.1.18+20180430.b12c320f5-3.3.1-b12c320f5) - partition with quorum
	Last updated: Thu Apr 25 06:30:50 2019
	Last change: Thu Apr 25 06:30:45 2019 by root via cibadmin on <node1>

	3 nodes configured
	0 resources configured

	Online: [ <node1> <node2> <node3> ]

	No resources
----

This is an example for a three node cluster _corosync.conf_ file:

[subs="specialchars,attributes,quotes"]
----
	# Please read the corosync.conf.5 manual page
	totem {
		version: 2
		secauth: on
		crypto_hash: sha1
		crypto_cipher: aes256
		cluster_name: hacluster
		clear_node_high_bit: yes
		token: 5000
		token_retransmits_before_loss_const: 10
		join: 60
		consensus: 6000
		max_messages: 20
		interface {
			ringnumber: 0
			mcastport: 5405
			ttl: 1
		}

		transport: udpu
	}

	logging {
		fileline: off
		to_stderr: no
		to_logfile: no
		logfile: /var/log/cluster/corosync.log
		to_syslog: yes
		debug: off
		timestamp: on
		logger_subsys {
			subsys: QUORUM
			debug: off
		}

	}

	nodelist {
		node {
			ring0_addr: {myIP3nd1}
			nodeid: 1
		}

		node {
			ring0_addr: {myIP3nd2}
			nodeid: 2
		}

		node {
			ring0_addr: {myIP3nd3}
			nodeid: 3
		}

	}

	quorum {

		# Enable and configure quorum subsystem (default: off)
		# see also corosync.conf.5 and votequorum.5
		provider: corosync_votequorum
		expected_votes: 3
		two_node: 0
	}
----

Verify the system and enable the SBD service if not done by the setup:

[subs="specialchars,attributes,quotes"]
----
# systemctl enable sbd
----

Check if the parameter *have-watchdog=true* has been automatically set:

[subs="specialchars,attributes,quotes"]
----
# crm configure show | grep have-watchdog
----

Adapt the cluster *stonith-watchdog-timeout* for diskless SBD:

[subs="specialchars,attributes,quotes"]
----
# crm configure property stonith-watchdog-timeout=10
----

For diskless SBD, this parameter must not equal zero. It defines after how long it is assumed that the
fencing target has already self-fenced. Therefore, its value needs to be >= 1.2 times the value of SBD_WATCHDOG_TIMEOUT in _/etc/sysconfig/sbd_ .

==========

==== Configurng the cluster resources

The resources for the three node cluster are set up exactly in the same way as for the two node cluster.

// TODO PRIO3:
// ==== Tuning the cluster timing in special for the SBD.

// END OF BLOCKED CHAPTERS FOR MULTINODE 2019-06-17
////

=== Extending an existing two-node cluster configuration

.Tasks

// THIS LIST TEXTS MUST MATCH THE FOLLOWING SECTION TITLES !!!
. Backing up the current cluster
. Installing the operating system of the new node
. Patching the existing nodes
. Preparing the new node's operating system
. Installing the cluster software on the new node
. Preparing SAPStartSrv resource agent on the new node
. Preparing the SAP installation on the new node
. Adding the new node to the cluster
. Testing the new cluster configuration

==== Backing up the current cluster

To *back up the current cluster*, perform a
==========

* Backup of your system including
** cluster configuration
** corosync.conf
** all data and configuration which are important and customized and not default

The system is configured as described in the SUSE Best Practices document
__SAP S/4 HANA - Enqueue Replication 2 High Availability Cluster With Simple Mount - Setup Guide__.

To *back up the cluster configuration*, go to one of the cluster nodes and safe the cluster configuration with _crm_ and _crm report_ commands:

[subs="attributes"]
----
# crm configure save 2node.txt
----

*Back up the existing /etc/corosync/corosync.conf* and all other files which may be important for a restore.
This example is one method creating a backup. The important point is using an *external* destination.

[subs="specialchars,attributes,quotes"]
----
# tar cvf /<path to an *external* storage>/bck_2nodes_configuration.tar \
      /etc/corosync/corosync.conf \
      2node.txt \
      <add your additional files here>
----
==========


==== Installing the operating system of the new node

We recommend using automating the installation to ensure that the system setup across nodes is identical.
Make sure to document any additional steps you take beyond the automated setup. In our example, we
deploy our machines with an AutoYaST configuration file and run a post step script which does the basic
configuration.

==== Patching the existing nodes

If applicable, install the latest updates and patches on your existing nodes.
Alternatively, if you are using frozen repositories such as those provided by SUSE Manager,
add the new system to the same repositories, so they have the same patch level as your
existing nodes.

Use `zypper patch` or `zypper update` depending on your company's rules.

- We recommend installing the latest available patches to guarantee system stability and hardening.
Bug fixing and security patches help avoid unplanned outages and make the system less vulnerable.

There are multiple ways:

[subs="specialchars,attributes,quotes"]
----
# zypper patch
## or
# zypper update
----

==== Preparing the new node's operating system

.Check for valid DNS, time synchronization, network settings
==========
- Verify that DNS is working:
+
[subs="specialchars,attributes,quotes"]
----
# ping <hostname>
----

- Set up *chrony* (this is best done with *yast2*) and enable it:
+
[subs="specialchars,attributes,quotes"]
----
# yast ntp-client
----

- Check the network settings:
+
[subs="specialchars,attributes,quotes"]
----
# ip r
----

==========

You may run into trouble if there is no valid or no default route configured.

- Patch the new system like the existing nodes, verify that your systems have the same patch level and
 that all required reboots have been performed.

==== Installing cluster software on the new node

// TODO PRIO3: pattern sap-nw?

.Installing packages
============
- Install the pattern *ha_sles* on the new cluster node:
+
[subs="specialchars,attributes,quotes"]
----
# zypper in -t pattern ha_sles
----

- Install the package *{s4sClConnector3}* version 3.1.0 from the SUSE repositories:
+
[subs="specialchars,attributes,quotes"]
----
# zypper in {s4sClConnector3}
----
============

==== Preparing SAPStartSrv resource agent on the new node

- Install the sapstartsrv-resource-agents package and enable the
sapping and sappong services.
+
[subs="specialchars,attributes,quotes"]
----
# zypper in sapstartsrv-resource-agents
# systemctl enable sapping
# systemctl enable sappong
----

After installing all necessary packages, compare installed package versions.

.Check that all nodes have the same software packages and versions:
============
- On the existing cluster nodes, type:
+
[subs="specialchars,attributes,quotes"]
----
# rpm -qa | sort >rpm_{my3nd1}.log
----

- On the new node, type:
+
[subs="attributes"]
----
# rpm -qa | sort >rpm_{my3nd3}.log
----

- Copy the file from one node to the other and compare the two versions:
+
[subs="attributes"]
----
# vimdiff rpm_{my3nd3}.log rpm_{my3nd1}.log
----
============


NOTE: If there any differences fix them first before you proceed.

.Install and configure the watchdog device on the new machine.
============

Instead of deploying the software-based solution, preferably use a hardware-based watchdog device.
The following example uses the software device but can be easily adapted to the hardware device.

[subs="specialchars,attributes,quotes"]
----
# modprobe softdog
# echo softdog > /etc/modules-load.d/watchdog.conf
# systemctl restart systemd-modules-load
# lsmod | egrep "(wd|dog|i6|iT|ibm)"
----
============

NOTE: Ensure that the new node is connected to the same SBD disk connected to the existing two nodes. Ensure that the new node has the same exact SBD configurations already exist on the two existing nodes

[subs="attributes"]
----
# sbd -d {myDevA} dump
----

==== Preparing the SAP installation on the new node

With SWPM 2.0 (SP4 or later), which is part of the SL Toolset, SAP provides a new option which can perform all necessary steps to prepare a fresh
install server to be able to fit into an existing SAP system. This new option will help us to prepare a
new host which can later run either the ASCS or ERS in the cluster environment.

You need to create the directory structure that should run the SAP resource.
The instance directory is located on an NFS share for all nodes.

- Create mount points and mount NFS shares on the new added node ({my3nd3}):

.Mount NFS Shares on {my3nd3}
==============================================
[subs="attributes"]
----
# mkdir -p /sapmnt/{mySid} /usr/sap/{mySid} /sapmedia
# mount -t nfs {myNfsSrv}:{myNFSSapmnt}/{mySid}    /sapmnt/{mySid}
# mount -t nfs {myNfsSrv}:{myNFSUsrSap}/{mySid}    /usr/sap/{mySid}
# mount -t nfs {myNfsSrv}:{myNFSSapmedia} /sapmedia
----
==============================================

As the directories _/sapmnt/{mySid}_, _/usr/sap/{mySid}_ need to be available at all time, make sure they are mounted during boot. This can be
achieved by putting the information into the _/etc/fstab_.

The next step requires the following information:

- profile directory
- password for SAP System Administrator
- UID for sapadm

[subs="specialchars,attributes,quotes"]
----
# cd {mySAPinst}
# ./sapinst
----

* SWPM product installation path:
** Installing {sapS4insm}  -> SAP HANA DATABASE -> Installation -> Application Server ABAP
-> High-Availability System -> Prepare Additional Cluster Node
* Use /sapmnt/{mySid}/profile for the profile directory
* All passwords: {mySapPwd}
*  UID for sapadm: 2002

.Post Step Procedure after SAP Preparation Step
==========
- Add the user _{mySapAdm}_ to the Unix user group _haclient_.
+
[subs="specialchars,attributes,quotes"]
----
# usermod -a -G haclient {mySapAdm}
----

- Register the ASCS and the ERS {sap} instance:
+
[subs="specialchars,attributes"]
----
# LD_LIBRARY_PATH=/usr/sap/hostctrl/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH
# /usr/sap/hostctrl/exe/sapstartsrv pf=/usr/sap/<SID>/SYS/profile/<SID>_ERS<instanceNumberErs>_<virtHostNameErs> -reg
# /usr/sap/hostctrl/exe/sapstartsrv pf=/usr/sap/<SID>/SYS/profile/<SID>_ASCS<instanceNumberAscs>_<virtHostNameAscs> -reg
----
+
NOTE: This must be done for each instance. Call sapstartsrv with parameters *pf=*<profile-of-the-sap-instance> and *-reg*.
+

- Disable *systemd* services of the ASCS and the ERS {sap} instance:
+
[subs="specialchars,attributes"]
----
# systemctl disable SAP{mySid}_{myAscsIno}.service
# systemctl disable SAP{mySid}_{myErsIno}.service
----
+
NOTE: This is mandatory for giving control over the instance to the HA cluster.
+

- Check the {sap} *systemd* integration:
+
[subs="specialchars,attributes"]
----
# systemctl list-unit-files | grep sap
saphostagent.service enabled
sapinit.service generated
saprouter.service disabled
saptune.service enabled
# systemctl list-unit-files | grep SAP
SAP{mySid}_{myErsIno}.service disabled
SAP{mySid}_{myErsIno}.service disabled
# cat /usr/sap/sapservices
systemctl --no-ask-password start SAP{mySid}_{myAscsIno} # sapstartsrv pf=/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs}
systemctl --no-ask-password start SAP{mySid}_{myErsIno} # sapstartsrv pf=/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstErs}_{myVipNErs}
----
==========

==== Adding the new node to the cluster

.Adding the New Node to the Cluster
==========
Check if the SBD device is available in case the SBD stonith method is in place for the two nodes.
If the existing cluster using a different, supported stonith mechanism check and verify them too for the
new cluster node.

[subs="specialchars,attributes,quotes"]
----
# sbd -d {myDevA} dump
----


- Joining the cluster can be done with `*ha-cluster-join*`:

[subs="specialchars,attributes,quotes"]
----
# ha-cluster-join -c {my2nd1}
----

After the new node has joined the cluster, the configuration must be adapted to the new situation.
Double-check if joining the cluster was successful and verify the file _/etc/corosync/corosync.conf_.

[subs="specialchars,attributes,quotes"]
----
# awk '/quorum/,/}/' /etc/corosync/corosync.conf
# corosync-quorumtool -s
----

The values *expected_votes* and *two_node* should now look like this on all nodes:

----
expected_votes: 3
two_node: 0
----

Modify the cluster configuration and set a new colocation rule with _crm_:

[subs="specialchars,attributes,quotes"]
----
# crm configure delete col_sap_{mySid}_no_both
# crm configure colocation ASCS{myAscsIno}_ERS{myErsIno}_separated_{mySid} -5000: grp_{mySid}_ERS{myErsIno} grp_{mySid}_ASCS{myAscsIno}
----
==========

==== Testing the new cluster configuration

It is highly recommended to run certain test to verify that the new configuration is working as expected.
A list of test can be found in the basic setup for the two node cluster above.

=== Pros and Cons for odd and even numbers of cluster nodes

There are certain use cases and infrastructure requirements which end up in different installation setups.
We will cover some advantages and disadvantages of special configuration below:

* The two node cluster and two locations
    - Advantage: symmetric spread of all nodes over all locations
    - Disadvantage: no diskless SBD feature allowed for all two node clusters
* The two node cluster and more than two locations
    - Advantage: SBD device can be provided from there (must be HA himself)
    - Advantage: cluster could operate with three SBD devices from different locations
    - Disadvantage: no diskless SBD feature allowed for all two node clusters
* The three node cluster and two locations
    - Advantage: less complex infrastructure
    - Advantage: diskless SBD feature is allowed
    - Disadvantage: "pre selected" location (two node + one node)
* The three node cluster and three locations
    - Advantage: symmetric spread of all nodes over all locations
    - Advantage: diskless SBD feature is allowed
    - Disadvantage: higher planning effort and complexity for infrastructure planning

== References

For more information, see the documents listed below.

=== {SUSE} product documentation

// TODO PRIO3: unify with HANA setup guide

- {SUSE} product manuals and documentation can be downloaded at
https://documentation.suse.com/

- {SUSE} release notes can be found at https://www.suse.com/releasenotes/

- {sles} technical information can be found at
https://www.suse.com/products/server/technical-information/



=== Pacemaker
// TODO PRIO3: move to include file?
- Pacemaker 2.0 Configuration Explained:
https://clusterlabs.org/pacemaker/doc/deprecated/en-US/Pacemaker/2.0/html-single/Pacemaker_Explained/

:leveloffset: 2
include::SAPNotes_s4_2101.adoc[]

++++
<?pdfpagebreak?>
++++

////
############################
#
# APPENDIX
#
############################
////

:leveloffset: 0
== Appendix

=== CRM configuration of the two-node cluster

Find below the complete crm configuration for {sap} system {mySid}. This example
is for the two node cluster, but without priority fencing.
In a multi-node cluster you will find additional node entries like *node 3: {my3nd3}*.

[subs="attributes"]
----
node 1: {my2nd1}
node 2: {my2nd2}
primitive rsc_ip_{mySid}_{myInstAscs} IPaddr2 \
    params ip={myVipAAscs} \
    op monitor interval=10 timeout=20
primitive rsc_ip_{mySid}_{myInstErs} IPaddr2 \
    params ip={myVipAErs} \
    op monitor interval=10 timeout=20
primitive rsc_SAPStartSrv_{mySID}_{myInstAscs} ocf:suse:SAPStartSrv \
    params InstanceName={mySID}_{myInstAscs}_{myVipNAscs}
primitive rsc_SAPStartSrv_{mySID}_{myInstErs} ocf:suse:SAPStartSrv \
    params InstanceName={mySID}_{myInstErs}_{myVipNErs}
primitive rsc_sap_{mySID}_{myInstAscs} SAPInstance \
    op monitor interval=11 timeout=60 on-fail=restart \
    params InstanceName={mySID}_{myInstAscs}_{myVipNAscs} \
        START_PROFILE="/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs}" \
        AUTOMATIC_RECOVER=false MINIMAL_PROBE=true \
        meta resource-stickiness=5000
primitive rsc_sap_{mySID}_{myInstErs} SAPInstance \
    op monitor interval=11 timeout=60 on-fail=restart \
    params InstanceName={mySID}_{myInstErs}_{myVipNErs} \
         START_PROFILE="/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstErs}_{myVipNErs}" \
         AUTOMATIC_RECOVER=false IS_ERS=true MINIMAL_PROBE=true
primitive stonith-sbd stonith:external/sbd \
    params pcmk_delay_max=30
group grp_{mySid}_{myInstAscs} rsc_ip_{mySid}_{myInstAscs} \
    rsc_SAPStartSrv_{mySID}_{myInstAscs} rsc_sap_{mySid}_{myInstAscs} \
    meta resource-stickiness=3000
group grp_{mySid}_{myInstErs} rsc_ip_{mySid}_{myInstErs} \
    rsc_SAPStartSrv_{mySID}_{myInstErs} rsc_sap_{mySid}_{myInstErs}
colocation col_sap_{mySid}_separate -5000: \
    grp_{mySid}_{myInstErs} grp_{mySid}_{myInstAscs}
order ord_sap_{mySid}_ascs_first Optional: rsc_sap_{mySID}_{myInstAscs}:start \
    rsc_sap_{mySID}_{myInstErs}:stop symmetrical=false
property cib-bootstrap-options: \
    have-watchdog=true \
    cluster-infrastructure=corosync \
    cluster-name=hacluster \
    stonith-enabled=true \
    stonith-timeout=150 \
    placement-strategy=balanced
rsc_defaults rsc-options: \
    resource-stickiness=1 \
    migration-threshold=3 \
    failure-timeout=86400
op_defaults op-options: \
    timeout=600 \
    record-pending=true
----

=== CRM configuration fragments of the two-node cluster with priority fencing

Find below crm configuration fragments for {sap} system {mySid}. This example
shows the specific items for the two node cluster with priority fencing.
This configuration is basically the same as above, except the items shown below.

[subs="attributes"]
----
...
primitive rsc_sap_{mySID}_{myInstAscs} SAPInstance \
    op monitor interval=11 timeout=60 on-fail=restart \
    params InstanceName={mySID}_{myInstAscs}_{myVipNAscs} \
        START_PROFILE="/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs}" \
        AUTOMATIC_RECOVER=false MINIMAL_PROBE=true \
        meta resource-stickiness=5000 priority=100
...
primitive stonith-sbd stonith:external/sbd \
    params pcmk_delay_max=15
...
property cib-bootstrap-options: \
    have-watchdog=true \
    cluster-infrastructure=corosync \
    cluster-name=hacluster \
    stonith-enabled=true \
    stonith-timeout=150 \
    placement-strategy=balanced \
    priority-fencing-delay=30
...
----

=== Corosync configuration of the two-node cluster

Find below the Corosync configuration for one corosync ring. Ideally two rings would be used.

[subs="specialchars,attributes"]
----
{my2nd1}:~ # cat /etc/corosync/corosync.conf
# Please read the corosync.conf.5 manual page
totem {
    version: 2
    secauth: on
    crypto_hash: sha1
    crypto_cipher: aes256
    cluster_name: hacluster
    clear_node_high_bit: yes
    token: 5000
    token_retransmits_before_loss_const: 10
    join: 60
    consensus: 6000
    max_messages: 20
    interface {
        ringnumber: 0
        mcastport: 5405
        ttl: 1
    }

    transport: udpu
}

logging {
    fileline: off
    to_stderr: no
    to_logfile: no
    logfile: /var/log/cluster/corosync.log
    to_syslog: yes
    debug: off
    timestamp: on
    logger_subsys {
        subsys: QUORUM
        debug: off
    }

}

nodelist {
    node {
        ring0_addr: {myIP2nd1}
        nodeid: 1
    }

    node {
        ring0_addr: {myIP2nd2}
        nodeid: 2
    }

}

quorum {

    # Enable and configure quorum subsystem (default: off)
    # see also corosync.conf.5 and votequorum.5
    provider: corosync_votequorum
    expected_votes: 2
    two_node: 1
}
----

=== Corosync configuration of the multi-node cluster

Find below the Corosync configuration for one corosync ring. Ideally two rings would be used.

[subs="specialchars,attributes,quotes"]
----
	# Please read the corosync.conf.5 manual page
	totem {
		version: 2
		secauth: on
		crypto_hash: sha1
		crypto_cipher: aes256
		cluster_name: hacluster
		clear_node_high_bit: yes
		token: 5000
		token_retransmits_before_loss_const: 10
		join: 60
		consensus: 6000
		max_messages: 20
		interface {
			ringnumber: 0
			mcastport: 5405
			ttl: 1
		}

		transport: udpu
	}

	logging {
		fileline: off
		to_stderr: no
		to_logfile: no
		logfile: /var/log/cluster/corosync.log
		to_syslog: yes
		debug: off
		timestamp: on
		logger_subsys {
			subsys: QUORUM
			debug: off
		}

	}

	nodelist {
		node {
			ring0_addr: {myIP3nd1}
			nodeid: 1
		}

		node {
			ring0_addr: {myIP3nd2}
			nodeid: 2
		}

		node {
			ring0_addr: {myIP3nd3}
			nodeid: 3
		}

	}

	quorum {

		# Enable and configure quorum subsystem (default: off)
		# see also corosync.conf.5 and votequorum.5
		provider: corosync_votequorum
		expected_votes: 3
		two_node: 0
	}
----

=== /usr/sap/sapservices without native *systemd* integration

[subs="specialchars,attributes,quotes"]
----
#!/bin/sh
LD_LIBRARY_PATH=/usr/sap/{mySid}/{myInstAscs}/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/{mySid}/{myInstAscs}/exe/sapstartsrv pf=/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs} -D -u {mySapAdm}
LD_LIBRARY_PATH=/usr/sap/{mySid}/{myInstErs}/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/{mySid}/{myInstErs}/exe/sapstartsrv pf=/usr/sap/{mySid}/{myInstErs}/profile/{mySid}_{myInstErs}_{myVipNErs} -D -u {mySapAdm}
----

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
== Legal Notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

// REVISION 1.1 2022/02
//   - ENSA2 cluster setup, 2 nodes, simple NFS mount structure
//   - SAP native systemd support
// REVISION 1.1a 2023/02
//   - S/4-HA-CLU-1.0 PowerLE certified
