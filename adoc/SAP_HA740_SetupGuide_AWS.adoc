
:docinfo:

:localdate:

//
// Start of the document
//

= SAP NetWeaver High Availability Cluster 7.40 for the AWS Cloud: Setup Guide
// Fabian Herschel, Bernd Schubert, Stefan Schneider (AWS)
// 2019/6/28

// :Revision: 1.3

// Revision {Revision} from {docdate}

// Standard SUSE includes
// include::common_copyright_gfdl.adoc[]

// :toc:


////
TODO maybe the whole setup could be divided in two parts: Enqueue Replication
cluster which is the core part, and the auxiliary pieces which might come from somewhere?
DONE cover: SAP SAP HA 740 Setup Guide -> (better title)
DONE p.1 solution e.g. as provided -> solution, e.g. as provided
DONE p.2 This mean we -> This means we
DONE p.2 it's -> its (2x)
DONE p.2 added t the ->  added to the
REJC p.2 either adding the third node to the cluster -> (would change the basic setup)
DONE p.3 SAP HANA we recommend -> SAP HANA, we recommend
DONE p.2,3,17 to setup -> to set up
DONE p.3 be setup -> be set up
DONE p.4 sap -> SAP NetWeaver
DONE p.4 in a highly -> in an highly  => *this is wrong! it must be: in a highly*
DONE p.4 software than the -> software then the
TODO p.4 https://blogs.sap.com/2014/05/08/using-sapvendorclusterconnector-for-interaction-between-cluster-framework-and-sapstartsrv/comment-page-1/ -> (check link)
DONE p.5 need to shared -> need to be shared
DONE p.5 to the cluster node hacert01 -> to the cluster nodes hacert01
DONE p.5 as SBD. -> as SBD. For production a more elaborated disk setup might be used.
DONE p.6 /dev/sda -> (really sda ?)
DONE p.6 # partprobe -> # partprobe; fdisk -l /dev/sda
TODO p.7 (somewhere explain that the D02 and DVEBMGS01 on the DB host just to simplify this lab environment)
DONE p.7 MEDIEN -> MEDIA
TODO p.7,8 (only short hostnames in /etc/hosts?)
REJC p.8 on a NFS -> on an NFS
DONE p.9 Server 2 -> server 2
DONE p.9 Server 3 -> server 3
DONE p.9 # l -> # ls -l
DONE p.9 insgesamt 0 -> total 0
DONE p.10 takeover -> take-over
DONE p.10 installation illustrated here -> installation as illustrated here
DONE p.11 Temporary we -> Temporarily we (2x)
DONE p.11 or use it. -> or use it. Please make sure to use the right virtual hostname for each installation step. (2x)
DONE p.12  instance please -> instance, please
DONE p.12 As user ha1adm.  -> (font)
TODO p.13 TODO/TBD ->
DONE p.13 product versions and currently -> product versions. Currently
DONE p.14 usermod -a -> # usermod -a
DONE p.14 leed in loss  -> lead in loss
TODO p.14 File /usr/sap/HA1/SYS/profile/HA1_ASCS00_sapha1as. -> (font)
TODO p.14 File /usr/sap/HA1/SYS/profile/HA1_ERS10_sapha1er. -> (font)
DONE p.15 GB and we used -> GB. We use
TODO p.17 TODO/TBD ->
DONE p.17 in forehand -> beforehand
DONE p.17 install patterns ha_sles -> install pattern ha_sles
TODO p.17 <nul> -> refer to sle-ha quickstart guide on our webpage
TODO p.18 <nul> -> (run ha-cluter-join or do something to enable cluster?)
TODO p.18 <nul> -> some notes on adpating resulting config, depending on environment
DONE p.19,20,21,22 crm configure -> # crm configure
DONE p.22,23 cd -> # cd
DONE p.22,23 unzip -> # unzip
DONE p.23 cd -> # cd
DONE p.23 tar -> # tar
DONE p.23 ln -s -> # ln -s
DONE p.23 wget -> # wget
DONE p.23 mv -> # mv
DONE p.23 hatool This -> hatool. This
DONE p.23 Setup the -> Set up the
DONE p.24 java -cp -> # java -cp
TODO p.24 /usr/sap/HA1/SYS/exe/uc/linuxx86_64/sapcontrol -> # linuxx86_64/sapcontrol
DONE p.25 lead into loss of enqueue locks, because -> lead into loss of enqueue locks. Because
TODO p.25 As ha1adm. -> (font)
TODO p.25,26,27,28,29 sapcontrol -nr -> # sapcontrol -nr
DONE p.26 As user root or ha1adm. -> (font 3x)
DONE p.26 crm configure -> # crm configure   (2x)
DONE p.26,28 crm resource -> # crm resource
DONE p.26 proceess -> process
DONE p.26 plan to go to -> plan going into
DONE p.26,27 * As user  ->  As user  (no bullet)
DONE p.28 echo b -> # echo b
DONE p.29 irecoverable -> non-recoverable (2x)
DONE p.29 rollong -> roll-on/roll-off (or just rolling?)
TODO p.31 in HA-Umgebungen -> in HA environments (links without "D"?)
TODO p.32 ... -> TODO/TBD
TODO p.32 -> SLE-HA release notes https://www.suse.com/releasenotes/x86_64/SLE-HA/12-SP2/
TODO p.32 SLE-HA quick setup guide
TODO p.32 SLE-HA product docu
TODO p.32 -> TODO SLES-for-SAP release note
TODO p.32 -> TODO product docu
TODO -> NFS SAP layout for instance
TODO: non line break with in command or variables
TODO: table of required values
////

// Load document variables
include::VariablesAWS.adoc[]

== About this Guide

=== Introduction

{sles4sapReg} is the optimal platform to
run {sapReg} applications with high availability. Together with a redundant layout
of the technical infrastructure, single points of failure can be eliminated.

{sapBSReg} is a sophisticated application platform for large enterprises
and mid-size companies. Many critical business environments require the highest
possible {sapReg} application availability.

The described cluster solution can be used for {sapReg} S/4 HANA and for
{sapReg} {sapNW}.

{sapNw} is a common stack of middleware functionality used to support the SAP
business applications. The {sapERS} constitutes application
level redundancy for one of the most crucial components of the {sapNw} stack,
the enqueue service. An optimal effect of the enqueue replication mechanism can
be achieved when combining the application level redundancy with a high
availability cluster solution, for example, as provided by {sles4sap}. The described
concept has proven its maturity over several years of productive operations for
customers of different sizes and branches.


=== Additional Documentation and Resources

Chapters in this manual contain links to additional documentation resources that
are either available on the system or on the Internet.

For the latest documentation updates, see https://documentation.suse.com.

You can also find numerous whitepapers and other
resources at the SUSE Linux Enterprise Server for SAP Applications resource
library: https://www.suse.com/products/sles-for-sap/resource-library/.

This guide and other SAP specific best practices can be downloaded via
https://documentation.suse.com/sbp/sap/.
Here you can find guides for {SAPHANA} system replication
automation and HA scenarios for {SAPNw} and {s4hana}.

// Standard SUSE includes
=== Feedback
include::common_intro_feedback.adoc[]

//=== Documentation Conventions
//TODO work on SUSE doc standard conventions file
//include::common_intro_typografie.adoc[]

== Scope of This Document

This guide will detail how to:

- Plan a {sleHA} platform for {sapNw},
  including {sapERS}.
- Set up a {linux} high availability platform and perform a basic {sapNW}
  installation including {sapERS} on {sle}.
- Integrate the high availability cluster with the {sap} control framework via
  {s4sClConnector}, as certified by {sap}.
- Install HA cluster solutions for the SAP HANA database on AWS as being
  described in SAP note 2309342 (SUSE Linux Enterprise High Availability
  Extension on AWS).

This guide focuses on the high availability (HA) of the central services. HA cluster
solutions for the database and {sapNW} instances are described in the best
practice "Simple Stack" available on the SUSE Best Practices documentation Web page (see section "Additional
documentation and resources"). For {saphana} system replication follow
the guides for the performance- or cost-optimized scenario.

== Overview

This guide describes how to set up a pacemaker cluster using {sles4sap}
{slesProdVersion} for the Enqueue Replication scenario on the AWS platform.
This guide does not document how to install on premises pacemaker clusters.
The goal is to match the {sapCert} certification specifications and goals.

These goals include:

- Integration of the cluster with the {SAP} start framework _sapstartsrv_ to
  ensure that maintenance procedures do not break the cluster stability
- Rolling Kernel Switch (RKS) awareness
- Standard {sap} installation to improve support processes

The updated certification {sapcert} has redefined some of the test procedures
and described new expectations how the cluster should behave under special
conditions. These changes allowed to improve the cluster architecture and to
design it for easier usage and setup.

Shared SAP resources are managed in AWS Elastic File Systems (EFS). The
SAP instances themselves are installed on EFS file systems to allow switching over
the file systems for proper functionality.

=== Using AWS Architectures in SUSE Linux Enterprise Server Pacemaker Clusters

SUSE Linux Enterprise Server pacemaker clusters will be installed in an AWS region. An AWS region
consists of multiple availability zones. Availability zones are located in different
data centers which are 10 to 50 kilometers apart. Availability zones have independent
flood levels, electricity and network hookup. They are supposed to be
independent. AWS recommends architectural patterns where redundant cluster nodes
are being spread across availability zones (AZs) to allow a customer
to overcome individual AZ failures.

An AWS Virtual Private Network (VPC) is spanning all AZs. We assume that a
customer will: 

-	Have identified two availability zones to be used
-	Have created subnets in the two AZs which can host the two nodes of a SUSE Linux Enterprise High Availability Extension
   cluster
-	Use a routing table which is attached to the two subnets
-	Optionally: host a Route53 private hosted naming zone to manage names in the VPC
-	All components of the cluster should reside in the same Amazon Account. The use of networking components such as a route
    table in another account (shared VPC setup) is not supported by the cluster resource agent.
    If you do require a multi account landscape then we advise you to reach to your AWS representative
    to have a look at implementing a Transit GateWay for cross account/VPC access.

The AWS specific components can be installed in two configurations. Both
configurations use the AWS Overlay IP address. An Overlay IP address is an
AWS specific routing entry which can send network traffic to an instance,
no matter which AZ the instance is located in.

The SUSE Linux Enterprise High Availability Extension cluster will update this routing entry as it is required.
All SAP system components in the VPC can reach an AWS instance with
an SAP system component inside a VPC through this Overlay IP address.

Overlay IP addresses have one disadvantage: they need to come from a CIDR range
which is outside of the VPC. Otherwise they would be part of a subnet and a
given availability zone.

On premises users like SAP GUIs cannot reach this IP address
since the AWS Virtual Private Network (VPN) gateway will not route traffic
to such an IP address.
A customer has two options to overcome this limitation.

1.	Use an SAP Router in the VPC. On premises users can reach it. The SAP router
can relay traffic to the ASCS system.
2.	Configure the additional Route 53 agent. Route 53 is the AWS specific name
service. The cluster agent will change the IP address for a given name of the
ASCS service. The on premises name server will need to delegate requests to
the sub domain in the AWS VPC to this name service. On premises SAP GUI user
will contact the ASCS through a name. Section TBD in the appendix explains how
to integrate Route 53 with your local naming services.

=== Prerequisites for the AWS-Specific HA Installation
There are several prerequisites which need to be met before starting
the installation:

* Have an AWS account
* Have an AWS user with admin rights, or at least rights to
**	Create security groups
** Create EFS file systems
** Modify AWS routing tables
** Create policies and attach them to IAM roles
** Optional for Route53 agent installation
*** Create and modify A-records in a private hosted zone
*	Understand your landscape
**	Know your region and its AWS name
**	Know your VPC and its AWS id
** Know which availability zones you want to use in your VPC
** Have a subnet in each of the availability zones
*** Have a routing table which is implicitly or explicitly attached to the two
subnets
*** Have free IP addresses in the two subnets for your SAP installation and EFS
mount points
*** Allow network traffic in between the two subnets
 Allow outgoing Internet access from the subnets
** Optionally: Have a Route 53 private hosted zone which hosts a subdomain for
instances in the two subnets
** Have a resource record with a name and the IP address for the SAP central
instance

Use the checklist in the appendix to make a note of all information needed
before starting the installation.

NOTE: With regards to the EFS/NFS filesystems, with SLES for SAP 15, it is now possible to mount these via the Operating System and remove the NFS mount & unmount control to outside of cluster.  This may simplify the ASCS / ERS environment.  Refer to https://documentation.suse.com/sbp/sap/html/SAP-S4HA10-setupguide-simplemount-sle15/index.html for more detail.

==== Tagging the EC2 Instances

The EC2 instances will have host names which are automatically generated.
Select host names which comply with SAP requirements, as detailed in SAP note 611361.

The cluster agents need to be able to identify the EC2 instances in the
correct way. This happens through instance tags.

Tag the two EC2 instances
through the console or the AWS Command Line Interface (CLI) with arbitrarily
chosen tags like _cluster_ and the host name as it will be shown in the command
_uname_ . Use the same tag (like _cluster_) and the individual host names
for both instances. The AWS documentation
(http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html) explains
how to tag EC2 instances.

NOTE: Refrain from using non-ASCII characters in any tag assigned to cluster-managed resources.

==== Security Groups

IMPORTANT: This section does not cover a discussion of SAP related ports in security groups,
it only lists the ports which need to be available for the SUSE cluster only.

The following ports and protocols need to be configured to allow the two
cluster nodes to communicate with each other:

* Port 5405 for inbound UDP: Used to configure the corosync communication layer.
Port 5405 is being used in common examples. A different port may be used
depending on the corosync configuration.

* Port 7630 for inbound TCP: Used by the SUSE "hawk" Web GUI.

* Enable ICMP: Used through a ping command in the AWS IP-move agent of the
SUSE cluster.

We assume that there are no restrictions for outbound network communication.

==== Creating an AWS CLI Profile on Both EC2 Instances

The SUSE Linux Enterprise Server agents use the AWS Command Line Interface (CLI).
This AWS CLI profile needs to be created for the root account _root_ on both
instances. The SUSE resources require a profile which creates output in text format. 
The name of the profile is arbitrary. The name chosen in this
example is _cluster_. The region of the instance needs to be added as well.
Replace the string _region-name_ with your target region in the following example.

One way to create such a profile is to create a file _/root/.aws/config_
with the following content:

[subs="attributes"]
----
[default]
region = region-name
[profile cluster]
region = region-name
output = text
----

The other way is to use the _aws configure_ CLI command in the following way:

[subs="specialchars,attributes"]
----
# aws configure
AWS Access Key ID [None]:
AWS Secret Access Key [None]:
Default region name [None]: region-name
Default output format [None]:

# aws configure --profile cluster
AWS Access Key ID [None]:
AWS Secret Access Key [None]:
Default region name [None]: region-name
Default output format [None]: text
----

This command sequence generates a default profile and a cluster profile.

==== Configure HTTP Proxies

Configuring HTTP Proxies is not needed if the system has transparent access to the Internet.
The resource agents execute the AWS CLI (Command Line Interface) commands. These
commands send HTTP/HTTPS requests to an access point in the Internet. These
access point are usually directly reachable. 

However, systems which do not offer transparent Internet access will need to provide an HTTP/HTTPS proxy.
The configuration of the proxy access is described in full detail in the AWS documentation.

Add the following environment variables to the root user's _.bashrc_
file:

[subs="attributes"]
----
export HTTP_PROXY=http://a.b.c.d:n
export HTTPS_PROXY=http://a.b.c.d:m
----

If authentication is required, add the following environment variables instead of the ones above:

[subs="specialchars,attributes"]
----
export HTTP_PROXY=http://username:password@a.b.c.d:n
export HTTPS_PROXY=http://username:password@a.b.c.d:m
----

The AWS Data Provider for SAP needs to reach the instance meta data service
directly. Add the following environment variable to the root user's
_.bashrc_ file:

[subs="specialchars,attributes"]
----
export NO_PROXY=169.254.169.254
----

==== Add a Second IP for each Cluster Instance

The cluster configuration will require two IP addresses for each cluster instance.
Adding a second IP address on the instance will allow the SUSE cluster to implement a
two ring corosync configuration. The two ring corosync configuration will allow the
cluster nodes to communicate with each other using the secondary IP address in the event
that there is an issue communicating with each other over the primary IP address.

Please refer to AWS documentation to understand how to assign a secondary IP address:
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/MultipleIP.html#assignIP-existing

After the secondary IP address is associated to the cluster instance in AWS, you will need
to add the secondary IP address to the cluster instance. Update the file
_/etc/sysconfig/network/ifcfg-eth0_. Replace XX.XX.XX.XX with the new secondary IP address
and replace 'XX' with the two digit subnet mask.

[subs="attributes"]
----
IPADDR_1=‘XX.XX.XX.XX/XX'
LABEL_1="1"
----

The system will read the file and add the secondary IP address after the cluster instance
is rebooted. Additionally, executing the command below as root will add the IP address
to the cluster instance network stack with out rebooting.

[subs="attributes"]
----
ip address add XX.XX.XX.XX/XX dev eth0
----

Replace XX.XX.XX.XX with the new secondary IP address and replace 'XX' with the two digit
subnet mask.

==== Disable the Source/Destination Check for the Cluster Instances

The source/destination check can be disabled through scripts using
the AWS command line interface (AWS-CLI). The following command needs to be
executed one time for both EC2 instances, which are supposed to receive traffic
from the Overlay IP address:

[subs="specialchars,attributes"]
----
# aws ec2 modify-instance-attribute --profile cluster --instance-id EC2-instance --no-source-dest-check
----

The system on which this command gets executed needs temporarily a role with
the following policy:

[subs="specialchars,attributes"]
----
{
   "Version": "2012-10-17",
   "Statement": [
   {
      "Sid": "Stmt1424870324000",
      "Effect": "Allow",
      "Action": [ "ec2:ModifyInstanceAttribute"],
      "Resource": [
      "arn:aws:ec2:region-name:account-id:instance/instance-a",
      "arn:aws:ec2:region-name:account-id:instance/instance-b"
      ]
   }
   ]
}
----

Replace the individual parameter for the region, the account
identifier and the two identifiers for the EC2 instances with appropriate
values.

The source/destination check can be disabled as well from the AWS console.
It takes the execution of the following drop-down box in the console for
both EC2 instances (see below).

.Disable Source/Destination Check at Console
image::SourceDestinationCheck.png[PNG]

==== Avoid Deletion of Cluster-Managed IP Address on the eth0 Interface

SUSE Linux Enterprise Server 12 SP3 is the first version which ships the cloud-netconfig package.
This package will remove any secondary IP address which is managed by the cluster
agents from the eth0 interface. This can cause service interruptions for
users of the HA service. Perform the following task on all cluster nodes:

Check whether the package cloud-netconfig-ec2 is installed with the command
[subs="specialchars,attributes"]
----
# zypper info cloud-netconfig-ec2
----

Update the file _/etc/sysconfig/network/ifcfg-eth0_ if this package is installed.
Change the following line to a „no“ setting or add the line if the package
is not yet installed:
[subs="specialchars,attributes"]
----
CLOUD_NETCONFIG_MANAGE='no'
----

==== AWS Roles and Policies
The SAP ASCS and ESR will run the SUSE Linux Enterprise Server Pacemaker software and the agents.
This software needs several AWS IAM privileges to operate the cluster.
Create a new role for every ASCS/ESR cluster and associate this role to the
two instances. Attach the policies detailed below to this role.

===== AWS Data Provider Policy
Every cluster node will operate an SAP system. SAP systems on AWS require an
installation of the “AWS Data Provider for SAP”. The data provider needs a
policy to access AWS resources. Use the policy as described in the
“AWS Data Provider for SAP Installation and Operations Guide“, section “IAM
Roles” and attach it to the role of the instance. This policy can be used by
all SAP systems. It takes only one policy in an AWS account. The policy does not
contain any instance-specific privileges.

[subs="specialchars,attributes"]
----
{
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "EC2:DescribeInstances",
                "EC2:DescribeVolumes"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": "cloudwatch:GetMetricStatistics",
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::aws-data-provider/config.properties"
        }
    ]
}
----

===== STONITH Policy
The instances of the SUSE cluster need the privilege to start and stop
the other nodes in the cluster. Create a policy with a name like _stonith-policy_
with the following content and attach it to the cluster role:

[subs="specialchars,attributes"]
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "Stmt1424870324000",
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeInstances",
                "ec2:DescribeInstanceAttribute",
                "ec2:DescribeTags"
            ],
            "Resource": "*"
        },
        {
            "Sid": "Stmt1424870324001",
            "Effect": "Allow",
            "Action": [
                "ec2:ModifyInstanceAttribute",
                "ec2:RebootInstances",
                "ec2:StartInstances",
                "ec2:StopInstances"
            ],
            "Resource": [
                "arn:aws:ec2:region-name:aws-account:instance/i-node1",
                "arn:aws:ec2:region-name:aws-account:instance/i-node2"
            ]
        }
    ]
}
----
Replace the variable _aws-account_ with the appropriate AWS account identifier.
Replace the variables _i-node1_ and _i-node2_ with the AWS instance-ids of
your two  cluster nodes of ({myNode1}) and ({myNode2}).
Replace the variable _region-name_ with the name of your AWS region (Example:
us-east-1 for the N. Virginia region).
This policy is dependent of the instances of your cluster. You need to create a separate policy for
every cluster!

==== Overlay IP Agent Policy
The Overlay IP agent changes a routing entry in an AWS routing table.
Create a policy with a name like _Manage-Overlay-IP-Policy_ and attach it to
the role of the cluster instances:

[subs="specialchars,attributes"]
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": "ec2:ReplaceRoute",
            "Resource": "arn:aws:ec2:region-name:account-id:route-table/rtb-XYZ"
        },
        {
            "Sid": "VisualEditor1",
            "Effect": "Allow",
            "Action": "ec2:DescribeRouteTables",
            "Resource": "*"
        }
    ]
}
----
This policy allows the agent to update the routing tables which get used.
Replace the following variables with the appropriate names:

- region-name : the name of the AWS region

- account-id : The name of the AWS account in which the policy is getting used

- rtb-XYZ : The identifier of the routing table which needs to be updated

==== Route 53 Updates
Optionally, you can install the Route 53 agent in the cluster. The following policy is
needed only when the Route 53 agent is used. Create a policy with the
name _Route53-Update_ and attach it to the role of the two cluster nodes:

[subs="specialchars,attributes"]
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "Stmt1471878724000",
            "Effect": "Allow",
            "Action": "route53:GetChange",
            "Resource": "arn:aws:route53:::change/*"
        },
        {
            "Sid": "Stmt1471878724001",
            "Effect": "Allow",
            "Action": "route53:ChangeResourceRecordSets",
            "Resource": "arn:aws:route53:::hostedzone/hosted zone ID/full name"
        },
        {
            "Sid": "Stmt1471878724002",
            "Effect": "Allow",
            "Action": [
                "route53:ListResourceRecordSets",
                "route53:ChangeResourceRecordSets"
            ],
            "Resource": "arn:aws:route53:::hostedzone/hosted zone ID"
        }
    ]
}

----
This policy is specific to the hosted zone and the resource record set. Replace
the variables _hosted zone ID_ with the AWS ID of the hosted zone. Replace
 _full name_ with the name of the entry.
 An individual Route 53 policy needs to be created for every cluster.

=== Add Overlay IP Addresses to Routing Table
Manually add two routing entries to the routing table which is assigned to the
two subnets. The IP addresses need to be outside of the CIDR range of the VPC.
Use the AWS console and search for “VPC”.

*	Select VPC
*	Click “Route Tables” in the left column
*	Select route table used for SAP ASCS subnets
*	Click tabulator “Routes”
*	Click “Edit”
* Scroll to the end of the list and click “Add another route”

==== Add the Service IP Address for Your ASCS Service
Add the service IP address of the ASCS service (node {myNode1}). Use as
filter /32 (example: 192.168.10.1/32). Add the Elastic Network Interface (ENI)
name of your instance
which is initially serving as ASCS. Save your changes by clicking “Save”.

This is the service IP address with the name {myNodeServiceASCS}.

==== Add the Service IP Address for Your ERS Service
Add the service IP address of the ERS service (node {myNode2}).
Use as filter /32 (example: 192.168.10.2/32). Add the Elastic Network Interface
(ENI) name of your instance which is initially serving as ERS. Save your changes
by clicking “Save”.

This is the IP address with the name {myNodeServiceERS}.

=== EFS File System
The cluster requires an NFS file system provided by AWS Elastic File
System (EFS). The file system will manage:

* /usr/sap/HA1 data for ASCS00, ERS10, D02, DVEBMGS01 and the other
application servers and the SYS directory
* /sapmnt

You need the identifier of your VPC and the subnet identifiers of the
subnets in which you plan to operate the two cluster nodes. It is also feasible to pick
other subnets. These subnets need to be reachable by the two cluster nodes
and they need to be in the same availability zone (AZ) for high availability reasons.
The option “General Purpose” will be sufficient.

Note down the DNS name of your specific EFS server. AWS name services will
resolve it internally to your VPC to an IP address which is in your availability
zone. We refer to this name as "efs-name" when we need to mount the
file systems.

We use one file system for two future mount points (/usr/sap/HA1/ASCS00,
/usr/SAP/HA1/ESR10). This keeps the administration level low and provides
more throughput. AWS throughput in EFS is based on the total size of the
file system.

Log in to one of the two cluster nodes and create several directories in
the EFS file system through a temporary mount. As user _root_, execute the following commands:

[subs="specialchars,attributes"]
----
# mount efs-name: /mnt
# mkdir -p /mnt/ASCS00 /mnt/ERS10 /mnt/D01 /mnt/DVEBMGS01 /mnt/D02 /mnt/SYS /mnt/sapmnt /mnt/sapcd
# umount /mnt
----
Create additional directories for other application servers. This NFS
file system will be used for all of them.

Mount the two mount points in the cluster nodes. Execute the following command
on both cluster nodes as root:

[subs="specialchars,attributes"]
----
# mkdir -p /sapmnt /usr/sap/HA1/SYS
----
Add the following two lines to the file /etc/fstab on the two instances which
will run the SAP ASCS and the ERS service.

[subs="specialchars,attributes"]
----
efs-name:SYS     /usr/sap/HA1/SYS    nfs4	rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2	0 0
efs-name:sapmnt  /sapmnt             nfs4	rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2	0 0
----

Replace _efs-name_ with the appropriate DNS name.

Mount the file system as root with the command

[subs="specialchars,attributes"]
----
# mount /usr/sap/HA1/SYS
# mount /sapmnt
----

==== Enable Cluster Instances to Use the Overlay IP Address

The two cluster instances need the Overlay IP address to be configured as
secondary IP address on their standard interface _eth0_. This can be achieved
by the command:

[subs="specialchars,attributes"]
----
# ip address add OVERLAY-IP dev eth0
----

Execute this command with root privileges on both instances.
Add the ASCS IP address on the ASCS node {myNode1}. Add the Enqueue Replication
address on the ERS node {myNode2}.

=== Differences to Previous Cluster Architecture
The concept is different to the old stack with the master-slave architecture.
With the new certification we switch to a more simple model with primitives.
This means we have on one machine the ASCS with its own resources and on
the other machine the ERS with its own resources.


=== Five Systems for ASCS, ERS, Database and Additional SAP Instances

This guide describes the installation of a distributed {sap} system on the five
systems. In this setup only two systems are in the cluster. The database and
{sap} dialog instances could also be added to the cluster by either adding the
three nodes to the cluster or by installing the database on either of the
nodes. However we recommend to install the database on a separate
cluster.

NOTE: The cluster in this guide only manages the {sap} instances ASCS and ERS,
because of the focus of the {sapCert} certification.

If your database is {sapHana}, we recommend to set up the performance optimized
system replication scenario using our automation solution {sapHanaSR}. The
{sapHanaSR} automation should be set up in an own two node cluster. The setup is
described in a separate best practice available at our best practice page.
https://www.suse.com/products/sles-for-sap/resource-library/sap-best-practices/

.Five systems for the certification setup
image::sles4sap_nw740_5nodes.svg[SVG]

.Clustered machines

*    One machine ({myNode1}) for ASCS; Hostname: {myVipNAscs}
*    One machine ({myNode2}) for ERS; Hostname:   {myVipNErs}

.Non-Clustered machine

*    One machine for DB; Hostname:   {myVipNDb}
*    One machine for PAS; Hostname:   {myVipNPas}
*    One machine for AAS; Hostname:   {myVipNDSec}

=== High Availability for the Database

Depending on your needs you could also increase the availability of the database,
if your database is not already high available by design.

==== {SapHana} System Replication

A perfect enhancement of the five node scenario described in this document is
to implement an {saphana} system replication (SR) automation.

.One cluster for central services, one for {saphana} SR
image::sles4sap_awsnw740_cs+hanasr.svg[SVG]

The following Databases are supported in combination with this scenario:

- SAP HANA DATABASE 1.0
- SAP HANA DATABASE 2.0

=== Integration of {SapNW} Into the Cluster Using the Cluster Connector

The integration of the HA cluster through the SAP control framework using the
{s4sClConnector} is of special interest. The {SAPSTARTSRV} controls {sap} instances since
{sap} Kernel versions 6.40. One classic problem of running
{sap} instances in a highly available environment is as follows: If an {sap}
administrator changes the status (start/stop) of an {sap} instance without using
the interfaces provided by the cluster software, the cluster framework will
detect that as an error status and will bring the {sap} instance into the old
status by either starting or stopping the {sap} instance. This can result in
very dangerous situations, if the cluster changes the status of an {sap} instance
during some {sap} maintenance tasks. This new updated solution enables the central component
{SAPSTARTSRV} to report state changes to the cluster software, and therefore avoids the
previously  described dangerous situations.
(See also the article "Using sap_vendor_cluster_connector for interaction between cluster
framework and sapstartsrv" at https://blogs.sap.com/2014/05/08/using-sapvendorclusterconnector-for-interaction-between-cluster-framework-and-sapstartsrv/comment-page-1/).

.Cluster connector to integrate the cluster with the {sap} start framework
image::sles4sap_clusterconnector.svg[SVG]

NOTE: For this scenario we are using an updated version of the {s4sClConnector}
which implements the API version 3 for the communication between the cluster
framework and the {sapstartsrv}.

The new version of the {s4sClConnector} now allows to start, stop and 'migrate'
an {sap} instance. The integration between the cluster software and the
{sapstartsrv} also implements to run checks of the HA setup using either the
command line tool sapcontrol or even the {SAP} management consoles ({SAP} MMC or
{sap} MC).

=== Disks and Partitions

For all {sap} file systems beside the EFS file systems
we are using XFS.

==== EFS File Systems for Cluster ASCS and ERS

Create the following sub directories on both cluster nodes as root:

[subs="attributes"]
----
# mkdir -p /usr/sap/HA1/ASCS00 /usr/sap/HA1/ERS10
----

The file systems for the ASCS and ERS instances need to be shared and assigned
to the cluster nodes {myNode1} and {myNode2}. Create an EFS file system.

During the SAP installation we need the filesystems
_/usr/sap/HA1/ASCS00_ to be mounted on
{myNode1} and _/usr/sap/HA1/ERS10_ to be mounted on {myNode2}.

[subs="attributes"]
----
{myNode1}: efs-name:/ASCS00 /usr/sap/HA1/ASCS00
{myNode2}: efs-name:/ERS10 /usr/sap/HA1/ERS10
----

Replace the variable _efs-name_ with the appropriate DNS name of the EFS
file system.

NOTE: {myNode1} and {myNode2} operate in different availability zones. They need
mount points named "efs-name" which are individual to the availability
zone. Use the DNS name provided by AWS. The DNS name will point to the
files system mount point local to a given Availability Zone.
During the SAP installation we need {myMpAscs} to be mounted on {myNode1}
and {myMpErs} to be mounted on {myNode2}.

////

==== Disk for DB and Dialog Instances (MaxDB Example)

The disk for the database and primary application server is assigned to
{myNode3}. In an advanced setup this disk should be shared between {myNode3}
and an optional additional node building an own cluster.

* partition one ({myDevPartSbd}) for SBD (7M) - not used here but a reservation
for an optional second cluster
* partition two ({myDevPartDb}) for the Database (60GB) formatted with XFS
* partition three ({myDevPartPas}) for the second file system (10GB) formatted
with XFS
* partition four ({myDevPartSec}) for the third file system (10GB)
formatted with XFS

You could either use YaST to create partitions or using available command line
tools. The following script could be used for non-interactive setups.

[subs="attributes"]
----
# parted -s {myDev} print
# # we are on the 'correct' drive, right?
# parted -s {myDev} mklabel gpt
# parted -s {myDev} mkpart primary 1049k 8388k
# parted -s {myDev} mkpart primary 8389k 60G
# parted -s {myDev} mkpart primary 60G 70G
# parted -s {myDev} mkpart primary 70G 80G
# mkfs.xfs {myDevPartDb}
# mkfs.xfs {myDevPartPas}
# mkfs.xfs {myDevPartSec}
----

.To be mounted either by OS or an optional cluster
- {myNode3}:   {myDevPartDb}   {myMpDb}

- {myNode3}:   {myDevPartPas}   {myMpPas}

- {myNode3}:   {myDevPartSec}   {myMpSec}

NOTE:  {myInstPas} => Since NetWeaver 7.5 the primary application server instance
directory has been renamed. (D<Instance_Number>)


.NFS server
- {myNfsSrv}:{myNFSExpPath}/{mySid}/sapmnt   /sapmnt

- {myNfsSrv}:{myNFSExpPath}/{mySid}/usrsapsys /usr/sap/{mySid}/SYS

.Media
- {myNfsSrv}:{myNFSExpPathSapMedia} /sapcd

or

- {myNfsSrv}:{bsNFSExpPathSapMedia} /sapcd

////

=== IP Addresses and Virtual Names

Check, if the file _/etc/hosts_ contains at least the following address
resolutions. Add those entries, if they are missing.
The 10.0.0.0 addresses in the example below are primary IP addresses within the
VPC CIDR block. The 192.168.201.0 addresses are the Overlay IP addresses for the
virtual services. The listing below lists a virtual IP address for the database
server. An SAP system installation against a virtual database server address
will allow to upgrade the database server to be a protected cluster service in
a later step.

[subs="attributes"]
----
{myIPNode1}  {myNode1}
{myIPNode2}  {myNode2}
{myIPNode3}  {myNode3}
{myVipAPas}  sap{mySidLc}ci
{myVipADSec}  sap{mySidLc}d2
{myVipAAscs}  sap{mySidLc}as
{myVipAErs}  sap{mySidLc}er
{myVipADb}  sap{mySidLc}db

----

=== Mount Points and NFS Shares

In our setup the directory _/usr/sap_ is part of the root file system. You could
of course also create a dedicated file system for that area and mount _/usr/sap_
during the system boot. As _/usr/sap_ also contains the {sap} control file
_sapservices_ and the {saphostagent}, the directory should not be placed on a
shared file system between the cluster nodes.

We need to create the directory structure on all nodes which might be able to
run the SAP resource. The SYS directory will be on a NFS share for all nodes.

- Creating mount points and mounting NFS share at all nodes
- Replace _efs-name_ with the appropriate DNS name.

.{sapNW} 7.4
==============================================
[subs="attributes"]
----
# mkdir -p /sapmnt
# mkdir -p /usr/sap/{mySid}/{{myInstAscs},{myInstDSec},{myInstPas},{myInstErs},SYS}
# mount -t nfs efs-name:/sapmnt /sapmnt
# mount -t nfs efs-name:/SYS    /usr/sap/{mySid}/SYS
# mount -t nfs efs-name:/sapcd /sapcd
----
==============================================

.{sapNW} 7.5
==============================================
[subs="attributes"]
----
# mkdir -p /sapmnt
# mkdir -p /usr/sap/{mySid}/{{myInstAscs},{bsInstPas},{myInstDSec},{myInstErs},SYS}
# mount -t nfs efs-name:/sapmnt    /sapmnt
# mount -t nfs efs-name:/SYS /usr/sap/{mySid}/SYS
# mount -t nfs efs-name:/sapcd /sapcd
----
==============================================

////

- Only MaxDB:  creating mount points for the database at {myNode3}:

[subs="attributes"]
----
# mkdir -p /sapdb
----

////

- Only HANA: creating mount points for database at {myNode3}:

[subs="attributes"]
----
# mkdir -p /hana/{shared,data,log}
----

////

- Other databases: creating mount points based on there installation guide.

////

////
review Lee means this looks like a 4 node cluster, adding additional title???
////

.File system layout including NFS shares
image::sles4sap_nw740_5nodes.svg[SVG]

We prepare the three servers for the distributed {sap} installation.

** Server 1 ({myNode1}) will be used to install the ASCS {sap} instance.
** Server 2 ({myNode2}) will be used to install the ERS {sap} instance
** Server 3 ({myNode3}) will be used to install the database.
** Server 4 ({myVipNPas}) will be used to install the PAS {sap} instance.
** Server 5 ({myVipNDSec}) will be used to install the AAS {sap} instance.
** Mounting the instance and database file systems at one specific node

////

.{sapNW} 7.40 on x86_64 architecture with MaxDB
=============================================================

[subs="attributes"]
----
(ASCS   {myNode1}) # mount {myDevPartAscs} {myMpAscs}
(ERS    {myNode2}) # mount {myDevPartErs} /usr/sap/{mySid}/{myInstErs}
(DB     {myNode3}) # mount {myDevPartDb} /sapdb
(Dialog {myNode3}) # mount {myDevPartPas} /usr/sap/{mySid}/{myInstPas}
(Dialog {myNode3}) # mount {myDevPartSec} /usr/sap/{mySid}/{myInstDSec}
----
=============================================================


.{sapNW} 7.50 on PowerLE architecture with HANA
=============================================================

[subs="attributes"]
----
(ASCS   {myNode1}) # mount {myDevPartAscs} {myMpAscs}
(ERS    {myNode2}) # mount {myDevPartErs} /usr/sap/{mySid}/{myInstErs}
(DB     {myNode3}) # mount {bsDevPartDbS} /hana/shared
(DB     {myNode3}) # mount {bsDevPartDbL} /hana/log
(DB     {myNode3}) # mount {bsDevPartDbD} /hana/data
(Dialog {myVipNPas}) # mount {myDevPartPas} /usr/sap/{mySid}/{bsInstPas}
(Dialog {myVipNDSec}) # mount {myDevPartSec} /usr/sap/{mySid}/{myInstDSec}
----
=============================================================

////

- As a result the directory _/usr/sap/{mySid}/_ should now look like:

[subs="attributes"]
----
# ls -l /usr/sap/{mySid}/
total 0
drwxr-xr-x 1 {mySidLc}adm sapsys 70 28. Mär 17:26 ./
drwxr-xr-x 1 root   sapsys 58 28. Mär 16:49 ../
drwxr-xr-x 7 {mySidLc}adm sapsys 58 28. Mär 16:49 {myInstAscs}/
drwxr-xr-x 1 {mySidLc}adm sapsys  0 28. Mär 15:59 {myInstDSec}/
drwxr-xr-x 1 {mySidLc}adm sapsys  0 28. Mär 15:59 {myInstPas}/
drwxr-xr-x 1 {mySidLc}adm sapsys  0 28. Mär 15:59 {myInstErs}/
drwxr-xr-x 5 {mySidLc}adm sapsys 87 28. Mär 17:21 SYS/
----

NOTE: The owner of the directory and files are changed during the {sap} installation. By default all
of them are owned by root.

== Testing the AWS Agents
Test the AWS agents before you start up the cluster. The tests will show
whether the AWS role and the policies are being configured correctly. The
tests should execute without AWS CLI errors

=== Testing the Overlay IP Agents

Replace the following variables in the commands

- _ip_address_: The service IP addresses of the ASCS and the ERS system
- _rtb-table_ : The name of AWS routing table of the Overlay IP address
- _cluster_ : replace the AWS CLI profile name if needed

The variables will need to match the variables in the OCF primitives
later on!

Run the following commands as root on both systems:

[subs="attributes"]
----
OCF_RESKEY_address=ip_address \
OCF_RESKEY_routing_table=rtb-table \
OCF_RESKEY_interface=eth0 OCF_ROOT=/usr/lib/ocf OCF_RESKEY_profile=cluster \
/usr/lib/ocf/resource.d/suse/aws-vpc-move-ip start

OCF_RESKEY_address=ip_address \
OCF_RESKEY_routing_table=rtb-table \
OCF_RESKEY_interface=eth0 OCF_ROOT=/usr/lib/ocf OCF_RESKEY_profile=cluster \
/usr/lib/ocf/resource.d/suse/aws-vpc-move-ip monitor

OCF_RESKEY_address=ip_address \
OCF_RESKEY_routing_table=rtb-table \
OCF_RESKEY_interface=eth0 OCF_ROOT=/usr/lib/ocf OCF_RESKEY_profile=cluster \
/usr/lib/ocf/resource.d/suse/aws-vpc-move-ip stop
----

Check for AWS CLI access issues and fix the AWS Policy.
Use the AWS console to check whether the IP address got added after _start_.
Use the AWS console to check whether the IP got removed after _stop_.
Use the other cluster node to execute some access commands (ping, SSH etc.)
Recheck and fix all network related settings if this does not work.

=== Testing Agents Mounting EFS File Systems

Test the monitoring function first.
Replace the following variable in the commands:

- _efs-name_: The name of the EFS file system

Run the following commands as root on both cluster nodes:

[subs="attributes"]
----
OCF_RESKEY_device="efs-name:ASCS00" \
OCF_RESKEY_directory="/usr/sap/HA1/ASCS00" OCF_RESKEY_fstype=nfs4 \
OCF_RESKEY_options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
OCF_ROOT=/usr/lib/ocf /usr/lib/ocf/resource.d/heartbeat/Filesystem start

OCF_RESKEY_device="efs-name:ERS10" \
OCF_RESKEY_directory="/usr/sap/HA1/ERS10" OCF_RESKEY_fstype=nfs4 \
OCF_RESKEY_options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
OCF_ROOT=/usr/lib/ocf /usr/lib/ocf/resource.d/heartbeat/Filesystem start

df -k

OCF_RESKEY_device="efs-name:ASCS00" \
OCF_RESKEY_directory="/usr/sap/HA1/ASCS00" OCF_RESKEY_fstype=nfs4 \
OCF_RESKEY_options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
OCF_ROOT=/usr/lib/ocf /usr/lib/ocf/resource.d/heartbeat/Filesystem stop

OCF_RESKEY_device="efs-name:ERS10" \
OCF_RESKEY_directory="/usr/sap/HA1/ERS10" OCF_RESKEY_fstype=nfs4 \
OCF_RESKEY_options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
OCF_ROOT=/usr/lib/ocf /usr/lib/ocf/resource.d/heartbeat/Filesystem stop

df -k
----

Check with the command _df -k_ whether the file systems got mounted and unmounted.
Potential problems arise with an incorrect _efs-name_ or with missing
subdirectories.

=== Optional: Testing Route 53 Agents

This test should be conducted if the Route 53 agent will be used.
Test the monitoring function first.
Replace the following variables in the commands:

- _hosted zone id_: The ID of the hosted private zones
- _fullname_ : The full name of the service name including sub domain and
a trailing dot.
- _cluster_ : replace the AWS CLI profile name if needed


The variables will need to match the variables in the OCF primitives
later on!

Run the following commands as root on both systems:

[subs="attributes"]
----
OCF_RESKEY_hostedzoneid=hosted zone id OCF_RESKEY_ttl=10 \
    OCF_RESKEY_fullname=fullname OCF_ROOT=/usr/lib/ocf \
    OCF_RESKEY_profile=cluster \
    /usr/lib/ocf/resource.d/heartbeat/aws-vpc-route53 monitor

OCF_RESKEY_hostedzoneid=hosted zone id OCF_RESKEY_ttl=10 \
    OCF_RESKEY_fullname=fullname OCF_ROOT=/usr/lib/ocf \
    OCF_RESKEY_profile=cluster \
    /usr/lib/ocf/resource.d/heartbeat/aws-vpc-route53 start

OCF_RESKEY_hostedzoneid=hosted zone id OCF_RESKEY_ttl=10 \
  OCF_RESKEY_fullname=fullname OCF_ROOT=/usr/lib/ocf \
  OCF_RESKEY_profile=cluster \
  /usr/lib/ocf/resource.d/heartbeat/aws-vpc-route53 stop
----

Fix any problems in monitoring first. Try a start as second test and a stop
as last test.

== SAP Installation

The overall procedure to install the distributed SAP is:

- Installing the ASCS instance for the central services
- Installing the ERS to get a replicated enqueue scenario
- Preparing the ASCS and ERS installations for the cluster take-over
- Installing the Database
- Installing the primary application server instance (PAS)
- Installing additional application server instances (AAS)

The result will be a distributed {sap} installation as illustrated here:

.Distributed installation of the {sap} system
image::sles4sap_awsnw740_distInstall.svg[SVG]

=== Linux User and Group Number Scheme

Whenever asked by the SAP software provisioning manager (SWPM) which Linux User
IDs or Group IDs to use, refer to the following table as an example.

[subs="attributes"]
----
Group sapinst      1000
Group sapsys       1001
Group sapadm       3000
Group sdba         3002

User  {mysapadm}       3000
User  sdb          3002
User  sqd{mySidLc}       3003
User  sapadm       3004
User  {bsDBadm}       4001
----

=== Installing ASCS on {myNode1}

Temporarily we need to set the service IP address which is used later in the
cluster as local IP, because the installer wants to resolve or use it.
Make sure to use the correct virtual host name for each installation step.
Take care of the file systems like {myDevPartAscs} and /sapcd/ which might also need
to be mounted.

[subs="attributes"]
----
# ip address add {myVipAAscs}{myVipNM} dev eth0
# mount {myDevPartAscs} {myMpAscs}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNAscs}
----

* SWPM option depends on {sapNW} version and architecture
** Installing {sapNW} 7.40 SR2 -> MaxDB -> SAP-Systems ->
Application Server ABAP -> High-Availability System -> ASCS Instance
** Installing {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> ASCS Instance
* SID id {mySid}
* Use instance number {myAscsIno}
* Deselect using FQDN
* All passwords: use {mySapPwd}
* Double-check during the parameter review, if virtual name *{myVipNAscs}* is
used

=== Installing ERS on {myNode2}

Temporarily we need to set the service IP address which is used later in the
cluster as local IP, because the installer wants to resolve or use it.
Make sure to use the correct virtual host name for each installation step.

[subs="attributes"]
----
# ip address add {myVipAErs}{myVipNM} dev eth0
# mount {myDevPartErs} {myMpErs}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNErs}
----

* SWPM option depends on {sapNW} version and architecture

** Installing {sapNW} 7.40 SR2 -> MaxDB -> SAP-Systems ->
Application Server ABAP -> High-Availability System -> Enqueue Replication
Server Instance

** Installing {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> Enqueue Replication
Server Instance
* Use instance number {myErsIno}
* Deselect using FQDN
* Double-check during the parameter review, if virtual name *{myVipNErs}* is
used
* If you get an error during the installation about permissions, change the
ownership of the ERS directory

[subs="attributes"]
----
# chown -R {mysapadm}:sapsys /usr/sap/{mySid}/{myInstErs}
----

* If you get a prompt to manually stop/start the ASCS instance, log in to
{mynode1} as user {mysapadm} and call sapcontrol.

[subs="attributes"]
----
# sapcontrol -nr {myAscsIno} -function Stop    # to stop the ASCS
# sapcontrol -nr {myAscsIno} -function Start   # to start the ASCS
----

=== Poststeps for ASCS and ERS

==== Stopping ASCS and ERS

_On {myNode1}_

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myAscsIno} -function Stop
# sapcontrol -nr {myAscsIno} -function StopService
----

_On {myNode2}_

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myErsIno} -function Stop
# sapcontrol -nr {myErsIno} -function StopService
----

==== Maintaining _sapservices_

Ensure that the file _/usr/sap/sapservices_ holds both entries (ASCS+ERS) on
both cluster nodes. Modify the file by copying the missing entry from both hosts across. Alternatively add the missing command string with _sapstartsrv_.

Example steps for ASCS profile on the ERS host:

[subs="attributes"]
----
# cd /usr/sap/hostctrl/exe
# export LD_LIBRARY_PATH=.
# ./sapstartsrv pf=/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs} -reg
----

This allows the {sapstartsrv} clients to start the service
like:

_As user {mySapAdm}_

[subs="attributes"]
----
# sapcontrol -nr {myErsIno} -function StartService {mySid}
----

The _/usr/sap/sapservices_ looks like (typically one line per instance):

[subs="attributes"]
----
#!/bin/sh
LD_LIBRARY_PATH=/usr/sap/{mySid}/{myInstAscs}/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/{mySid}/{myInstAscs}/exe/sapstartsrv pf=/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs} -D -u {mySapAdm}
LD_LIBRARY_PATH=/usr/sap/{mySid}/{myInstErs}/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/{mySid}/{myInstErs}/exe/sapstartsrv pf=/usr/sap/{mySid}/{myInstErs}/profile/{mySid}_{myInstErs}_{myVipNErs} -D -u {mySapAdm}
----

==== Integrating the Cluster Framework Using the {s4sClConnector} Package

Install the package *sap-suse-cluster-connector* version >3.0.0 from our
repositories:

[subs="attributes"]
----
# zypper install sap-suse-cluster-connector
----

////
NOTE: In future there might be two packages. The package {s4sClConnector}
might contain the old version 1.1.0 (SAP API 1). The package
sap-suse-cluster-connector might contain the new version 3.0.x (SAP API 3).
////

NOTE: The package {s4sClConnector} with version 3.0.x implements the SUSE SAP
API version 3. New features like SAP Rolling Kernel Switch (RKS) and migration of
ASCS are only supported with this new version.

For the ERS and ASCS instance edit the instance profile files
{mySid}_{myInstAscs}_{myVipNAscs} and {mySid}_{myInstErs}_{myVipNErs} in the
profile directory _/usr/sap/{mySid}/SYS/profile/_.

You need to tell the {sapStartSrv} service to load the HA script connector
library and to use the {s4sClConnector}.

[subs="attributes"]
----
service/halib = $(DIR_EXECUTABLE)/saphascriptco.so
service/halib_cluster_connector = /usr/bin/sap_suse_cluster_connector
----

Add the user {mySapAdm} to the unix user group haclient.

[subs="attributes"]
----
# usermod -a -G haclient {mySapAdm}
----

==== Adapting {sap} Profiles to Match the {sapCert} Certification

For the ASCS, change the start command from __Restart_Programm_xx__ to
__Start_Programm_xx__ for the enqueue server (enserver). This change tells the
{sap} start framework *not* to self-restart the enqueue process. Such a restart
would lead in loss of the locks.

.File /usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs}

[subs="attributes"]
----
Start_Program_01 = local $(_EN) pf=$(_PF)
----

Optionally you could limit the number of restarts of services (in the case of
ASCS this limits the restart of the message server).

For the ERS change instance the start command from __Restart_Programm_xx__ to
__Start_Programm_xx__ for the enqueue replication server (enrepserver).

.File /usr/sap/{mySid}/SYS/profile/{mySid}_{myInstErs}_{myVipNErs}

[subs="attributes"]
----
Start_Program_00 = local $(_ER) pf=$(_PFL) NR=$(SCSID)
----

==== Starting ASCS and ERS

_On {myNode1}_

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myAscsIno} -function StartService {mySid}
# sapcontrol -nr {myAscsIno} -function Start
----

_On {myNode2}_

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myErsIno} -function StartService {mySid}
# sapcontrol -nr {myErsIno} -function Start
----

////
=== Installing DB on {myNode3} (Example MaxDB)

The MaxDB needs min.40 GB. We use {myDevPartDb} and mount the partition to
_/sapdb_.

[subs="attributes"]
----
# ip address add {myVipADb}{myVipNM} dev eth0
# mount {myDevPartDb} {myMPDb}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNDb}
----

* We are installing SAP NetWeaver 7.40 SR2 -> MaxDB -> SAP-Systems ->
Application Server ABAP -> High Availability System -> DB
* Profile directory /sapmnt/{mySid}/profile
* DB ID is {mySid}
* Volume Media Type *keep* File (not raw)
* Deselect using FQDN
* Double-check during the parameter review, if virtual name *{myVipNDb}* is
used

////

=== Installing DB on {myNode3} (Example SAP HANA)

The HANA DB has very strict HW requirements. The storage sizing depends on many
indicators. Check the supported configurations at
https://support.sap.com/en/release-upgrade-maintenance.html#section_1969201630[SAP HANA Hardware Directory]
and https://www.sap.com/documents/2016/05/e8705aae-717c-0010-82c7-eda71af511fa.html[SAP HANA TDI].

Install the HANA file systems as being described in the section
http://docs.aws.amazon.com/quickstart/latest/sap-hana/planning.html["Planning the Deployment" of the AWS
SAP HANA on the AWS Cloud: Quick Start Reference Deployment]

Consider to install the database against an Overlay IP address which acts like
a service IP address. This will allow to upgrade the database to run in a SUSE Linux Enterprise High Availability Extension
for SAP cluster:


[subs="attributes"]
----
# ip address add {myVipADb}{myVipNM} dev eth0
# mount {bsDevPartDbS} {bsMPDb}/shared
# mount {bsDevPartDbL} {bsMPDb}/log
# mount {bsDevPartDbD} {bsMPDb}/data
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNDb}
----

* We are installing {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> Database Instance
* Profile directory /sapmnt/{mySid}/profile
* Deselect using FQDN
* Database parameters enter DBSID is {bsSidDB}; Database Host is {myVipNDb};
Instance Number is {bsDBIno}
* Database System ID enter Instance Number is {bsDBIno}; SAP Mount Directory is
{bsMPDb}/shared
* Account parameters change them in case of custom values needed
* Cleanup select *Yes*, remove operating system users from group'sapinst'....
* Double-check during the parameter review, if virtual name *{myVipNDb}* is
used

=== Installing the Primary Application Server (PAS) on {myVipNPas}

Add the following mount points to the _/etc/fstab_ file on host {myVipNPas}. Replace
the string "efs_fs_local_az" with the IP address of your EFS service in your
availability zone.

[subs="attributes"]
----
efs-name:sapcd     /sapcd    nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
efs-name:sapmnt    /sapmnt   nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
efs-name:DVEBMGS01 /usr/sap/HA1/DVEBMGS01  nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
efs-name:SYS       /usr/sap/HA1/SYS        nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
----

Replace the variable _efs-name_ with the appropriate DNS name.

Create mount directories and mount the file systems

[subs="attributes"]
----
# mkdir -p /sapcd /sapmnt /usr/sap/HA1/SYS /usr/sap/HA1/DVEBMGS01
# mount -a
----
Install the PAS server with the _sapinst_ tool.

* SWPM option depends on {sapNW} version and architecture
** Installing SAP NetWeaver 7.40 SR2 -> MaxDB -> SAP-Systems ->
Application Server ABAP -> High-Availability System -> Primary Application Server Instance
(PAS)
** Installing {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> Primary Application Server Instance
(PAS)
* Use instance number {myPasIno}
* Deselect using FQDN
* For our hands-on setup use a default secure store key
* Do not install Diagnostic Agent
* No SLD
* Double-check during the parameter review, if virtual name *{myVipNPas}* is
used

=== Installing an Additional Application Server (AAS) on {myVipNDSec}

Add the following mount points to the _/etc/fstab_ file on host {myVipNDSec}. Replace
the string "efs_fs_local_az" with the IP address of your EFS service in your
availability zone.

[subs="attributes"]
----
efs-name:sapcd     /sapcd            nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
efs-name:sapmnt    /sapmnt           nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
efs-name:D02       /usr/sap/HA1/D02  nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
efs-name:SYS       /usr/sap/HA1/SYS  nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
----

Replace the variable _efs-name_ with the appropriate DNS name.
Create mount directories and mount the file systems

[subs="attributes"]
----
# mkdir -p /sapcd /sapmnt /usr/sap/HA1/SYS /usr/sap/HA1/D02
# mount -a
----

Install the AAS server with the _sapinst_ tool.

* SWPM option depends on {sapNW} version and architecture
** Installing SAP NetWeaver 7.40 SR2 -> MaxDB -> SAP-Systems ->
Application Server ABAP -> High-Availability System -> Additional Application Server
Instance (AAS)
** Installing {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> Additional Application Server
Instance (AAS)
* Use instance number {myDSecIno}
* Deselect using FQDN
* Do not install Diagnostic Agent
* Double-check during the parameter review, if name *{myVipNDSec}* is
used

== Implementing the Cluster

The main procedure to implement the cluster is:

* Install the cluster software, if not already done during the installation of
the operating system
* Configure the cluster communication framework corosync
* Configure the cluster resource manager
* Configure the cluster resources

// * Tune the cluster timing in special for the SBD.

///////////////////////////////
TODO: Do we really need to stop, unconfigure and unmount?
Maybe we find a way to configure the resources that the cluster just
accepts the already started resource groups - lets see ;-)
///////////////////////////////

NOTE: Before we continue to set up the cluster we first stop all SAP instances, remove
the (manually added) IP addresses on the cluster nodes and unmount the file systems
which will be controlled by the cluster later.

////
NOTE: The SBD device/partition need to be created in beforehand. In this setup
guide we already have reserved partition {myDevPartSbd} for SBD usage.
////

.Tasks

. Set up NTP (best with YaST). Use AWS time service at 169.254.169.123 which
is accessible from all EC2 instances. Enable ongoing synchronization.

. Install pattern _ha_sles_ on both cluster nodes

[subs="attributes"]
----
# zypper install -t pattern ha_sles
----

Activate the public cloud module to get updates for the AWS CLI
(Command Line Interface):

[subs="attributes"]
----
# SUSEConnect --list-extensions
# SUSEConnect -p sle-module-public-cloud/12/x86_64
----

Update your packages with the command:

[subs="attributes"]
----
# zypper update
----

=== Configuring the Cluster Base

.Tasks

- Install and configure the cluster stack at the first machine

////

You could use either YaST to configure the cluster base or use the interactive
command line tool ha-cluster-init. The following script could be used for
automated setups.

[subs="attributes"]
----
# modprobe softdog
# echo "softdog" > /etc/modules-load.d/softdog.conf
# systemctl enable sbd
# ha-cluster-init -y csync2
# ha-cluster-init -y -i {myHaNetIf} corosync
# ha-cluster-init -y -s {myDevPartSbd} sbd
# ha-cluster-init -y cluster
----

////

==== Configuration of System Logging

SUSE recommends to use rsyslogd for logging with the SUSE cluster.
This is a default configuration. Some AWS AMIs however use syslogd logging.
Perform the following commands as root on all cluster nodes:

[subs="attributes"]
----
# zypper install rsyslog
----
Use option 1 (deinstallation of competing software, syslogd).
Reboot both nodes.

==== Corosync Configuration

===== Configuration of the _corosync.conf_ File

The configuration will have an IP address
for node node-1 which is supposed to be ip-node-1 and ip2-node-1. Node node-2 has an ip address to
which we refer as ip-node-2 and ip2-node-2.

All cluster nodes are required to have a local configuration
file _/etc/corosync/corosync.conf_ which will be structured as follows.

The relevant information is being located in the two sections describing
interface and nodelist. The other entries can be configured as needed for a
specific implementation.

NOTE: AWS requires a specific manual corosync configuration.

Use the following configuration in the _/etc/corosync/corosync.conf_ file
on both cluster nodes:

[subs="attributes"]
----
# Read the corosync.conf.5 manual page
totem {
  version: 2
  rrp_mode: passive
  token: 30000
  consensus: 32000
  token_retransmits_before_loss_const: 10
  max_messages: 20
  crypto_cipher: none
  crypto_hash: none
  clear_node_high_bit: yes
  interface {
    ringnumber: 0
    bindnetaddr: &lt;ip-local-node&gt;
    mcastport: 5405
    ttl: 1
  }
  transport: udpu
}
 logging {
      fileline: off
      to_logfile: yes
      to_syslog: yes
      logfile: /var/log/cluster/corosync.log
      debug: off
      timestamp: on
      logger_subsys {
         subsys: QUORUM
         debug: off
     }
}
nodelist {
  node {
  ring0_addr: &lt;ip-node-1&gt;
  ring1_addr: &lt;ip2-node-1&gt;
  nodeid: 1
  }
  node {
  ring0_addr: &lt;ip-node-2&gt;
  ring1_addr: &lt;ip2-node-2&gt;
  nodeid: 2
  }
}

quorum {
  # Enable and configure quorum subsystem (default: off)
  # see also corosync.conf.5 and votequorum.5
  provider: corosync_votequorum
  expected_votes: 2
  two_node: 1
}

----

Replace the variables _ip-node-1_ / _ip2-node-1_ and _ip-node-2_ / _ip2-node-2_ with the IP addresses of your
two cluster instances. Replace _ip-local-node_ with the IP address of the server
the file is being created.

The chosen settings for __crypto_cipher__ and __crypto_hash__ are suitable for
clusters in AWS. They may be modified according to SUSE's documentation
if strong encryption of cluster communication is desired.

==== Starting the Cluster

The next step is to start the cluster with the command on both nodes:

[subs="attributes"]
----
# systemctl start pacemaker
----

==== Checking the Configuration

The configuration can be checked with the command:

[subs="attributes"]
----
# corosync-cfgtool -s
----

It will create a result like the following one for a cluster node with the
IP address 10.0.0.111:

[subs="attributes"]
----
Printing ring status.
Local node ID 1
RING ID 0
id = 10.0.0.111
status = ring 0 active with no faults
----

The cluster in question has been using ring 0, the node had the ID 1.

- The _crm_mon -1_ output should look like this:

[subs="attributes"]
----
Stack: corosync
Current DC: hacert01 (version 1.1.15-19.15-e174ec8) - partition with quorum
Last updated: Wed Dec  6 16:02:42 2017
Last change: Wed Dec  6 15:44:45 2017 by hacluster via crmd on hacert01

2 nodes configured
0 resources configured

Online: [ hacert01 hacert02 ]

Full list of resources:
----

=== Configuring Cluster Resources

We need a changed SAPInstance resource agent for {sapNw} to *not* use
the master-slave construct anymore and to move to a more cluster-like construct. This allows to
start and stop the ASCS and the ERS itself and *not* only the complete
master-slave.

////////////////////////////////////
TODO: We need to adapt this section, if the new enqueue2 is published.
////////////////////////////////////

For this there is a new functionality for the ASCS needed to follow the ERS.
The ASCS needs to mount the shared memory table of the ERS to avoid the loss of
locks.

.Resources and constraints
image::sles4sap_nw740_resources.svg[SVG]

The implementation is done using a new flag "runs_ers_$SID" within
the RA, enabled with the help of the resource parameter "IS_ERS=TRUE".

There is the option to add a Route 53 agent. The architecture will then look
as follows:

.Resources and constraints
image::sles4sap_nw740_awsr53_resources.svg[SVG]

==== Preparing the Cluster for Adding the Resources

To avoid that the cluster starts partially defined resources we set the cluster
to the maintenance mode. This deactivates all monitor actions.

_As user root_

[subs="attributes"]
----
# crm configure property maintenance-mode="true"
----

==== Configuring AWS-Specific Settings

Execute the following commands on one of the two cluster nodes:

[subs="attributes"]
----
# vi crm-bs.txt
----

Enter the following information to the file _crm-bs.txt_:

[subs="attributes"]
----
property cib-bootstrap-options: \
    stonith-enabled="true" \
    stonith-action="off" \
    stonith-timeout="600s"
rsc_defaults rsc-options: \
	resource-stickiness=1 \
	migration-threshold=3
op_defaults op-options: \
	timeout=600 \
	record-pending=true
----

The setting _off_ forces the agents to shut down the instance.
This is desirable to avoid split brain scenarios on AWS.

Add the configuration to the cluster:

[subs="attributes"]
----
# crm configure load update crm-bs.txt
----

==== Configuration of AWS specific Stonith Resource

Create a file with the following content:

[subs="attributes"]
----
primitive res_AWS_STONITH stonith:external/ec2 \
op start interval=0 timeout=180 \
op stop interval=0 timeout=180 \
op monitor interval=300 timeout=60 \
params tag=pacemaker profile=cluster
----

The EC2 tag _pacemaker_ entry needs to match the tag chosen for
the EC2 instances. The value for this tag will contain the host name.
The name of the profile (_cluster_ in this example) will need to match the
previously configured AWS profile.

Name this file for example _aws-stonith.txt_ and add this file to the
configuration. The following command needs to be issued as root.
It uses the file name _aws-stonith.txt_:

[subs="attributes"]
----
# crm configure load update aws-stonith.txt
----

==== Configuring the Resources for the ASCS

First we configure the resources for the file system, IP address and the {sap}
instance. Of course you need to adapt the parameters to your environment.

Create a file with your editor of choice with a name _aws-ascs.txt_. Add the
ASCS primitive and the ASCS group to it. Do not forget to save your changes.

.ASCS primitive
================================================
[subs="attributes"]
----
primitive rsc_fs_{mySID}_{myInstAscs} Filesystem \
    params  device="{myDevPartAscs}" directory="/usr/sap/{mySID}/{myInstAscs}" \
            fstype="nfs4" \
            options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
    op start timeout=60s interval=0 \
    op stop timeout=60s interval=0 \
    op monitor interval=200s timeout=40s
primitive rsc_ip_{mySID}_{myInstAscs} ocf:suse:aws-vpc-move-ip \
    params  address={myVipAAscs} routing_table=rtb-table \
            interface=eth0 profile=cluster \
    op start interval=0 timeout=180 \
    op stop interval=0 timeout=180 \
    op monitor interval=60 timeout=60
primitive rsc_sap_{mySID}_{myInstAscs} SAPInstance \
    operations $id=rsc_sap_{mySID}_{myInstAscs}-operations \
    op monitor interval=120 timeout=60 on-fail=restart \
    params InstanceName={mySID}_{myInstAscs}_{myVipNAscs} \
        START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstAscs}_{myVipNAscs}" \
        AUTOMATIC_RECOVER=false \
    meta resource-stickiness=5000 failure-timeout=60 \
        migration-threshold=1 priority=10
----
================================================

Replace the variable _efs-name_ with the name of your EFS server.

Replace the variable _rtb-table_ with the identifier of the appropriate AWS
routing table for the subnets. The name of the AWS CLI profile (_cluster_ in this
example) will need to match the previously configured  AWS profile.

.ASCS group
================================================
[subs="attributes"]
----
group grp_{mySID}_{myInstAscs} \
  rsc_ip_{mySID}_{myInstAscs} rsc_fs_{mySID}_{myInstAscs} rsc_sap_{mySID}_{myInstAscs} \
	meta resource-stickiness=3000
----
================================================

Create a txt file _aws_ascs.txt_ with your preferred text editor, enter
both examples (primitives and group) to that file and load the configuration to
the cluster manager configuration.

[subs="attributes"]
----
# crm configure load update aws_ascs.txt
----

==== Optional: Including Route  53

Name this file for example _aws-route53.txt_ and add this file to the
configuration. The following command needs to be issued as root.
It uses the file name _aws-route53.txt_:

Enter the following primitive before or after the existing primitives in the
editor:

.ROUTE53 primitive
================================================
[subs="attributes"]
----
primitive rsc_r53_HA1_ASCS00 ocf:heartbeat:aws-vpc-route53 \
   params hostedzoneid=route-53-name ttl=10 fullname=name-full. profile=cluster \
   op start interval=0 timeout=180 \
   op stop interval=0 timeout=180 \
   op monitor interval=300 timeout=180
----
================================================

Replace the variable _route-53-name_ with the name of the associated private
hosted Route 53 zone.

Replace the variable _name-full._ will the fully qualified host name
with matches the private hosted Route 53 zone.

The agent uses a time-to-live (ttl) of 10 seconds in this example.
Change this parameter if needed.

Insert the __rsc_r53_{mySID}_{myInstAscs}__ after the
__rsc_ip_{mySID}_{myInstAscs}__. This will force the group to update
then Route 53 as second item after the Overlay IP address.

.ROUTE53 group
================================================
[subs="attributes"]
----
group grp_{mySID}_{myInstAscs} \
  rsc_ip_{mySID}_{myInstAscs} rsc_r53_{mySID}_{myInstAscs} \
  rsc_fs_{mySID}_{myInstAscs} rsc_sap_{mySID}_{myInstAscs} \
	meta resource-stickiness=3000
----

Create a txt file _aws-route53.txt_ with your preferred text editor, enter
both examples (primitives and group) to that file and load the configuration to
the cluster manager configuration.
Use the following command as root and modify ASCS group in the editor.

[subs="attributes"]
----
# crm configure load update aws-route53.txt
----

================================================

NOTE: Version 1.0.2 of the Route 53 agent
will not work if the EC2 metadata contains a
string like "local-ipv4" in the userdata section!

==== Configuring the Resources for the ERS

Second we configure the resources for the file system, IP address and the {sap}
instance. Of course you need to adapt the parameters to your environment.

Replace _efs-name_ with the name of your EFS server.

The specific parameter __IS_ERS=true__ should only be set for the ERS instance.

.ERS primitive
================================================
[subs="attributes"]
----
primitive rsc_fs_{mySID}_{myInstErs} Filesystem \
  params device="{myDevPartErs}" directory="/usr/sap/{mySID}/{myInstErs}" fstype=nfs4 \
  options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
  op start timeout=60s interval=0 \
  op stop timeout=60s interval=0 \
  op monitor interval=200s timeout=40s
primitive rsc_ip_{mySID}_{myInstErs} ocf:suse:aws-vpc-move-ip \
  params address={myVipAErs} routing_table=rtb-table \
  interface=eth0 profile=cluster \
  op start interval=0 timeout=180 \
  op stop interval=0 timeout=180 \
  op monitor interval=60 timeout=60
primitive rsc_sap_{mySID}_{myInstErs} SAPInstance \
  operations $id=rsc_sap_{mySID}_{myInstErs}-operations \
  op monitor interval=120 timeout=60 on-fail=restart \
  params InstanceName={mySID}_{myInstErs}_{myVipNErs} \
         START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstErs}_{myVipNErs}" \
         AUTOMATIC_RECOVER=false IS_ERS=true \
  meta priority=1000
----
================================================
Replace the variable _rtb-table_ with the identifier of the appropriate AWS
routing table for the subnets. The name of the AWS CLI profile (_cluster_
in this example) will need to match the previously configured  AWS profile.

.ERS group
================================================
[subs="attributes"]
----
group grp_{mySID}_{myInstErs} \
  rsc_ip_{mySID}_{myInstErs} rsc_fs_{mySID}_{myInstErs} rsc_sap_{mySID}_{myInstErs}
----
================================================

Create a txt file (like __aws_crm_ers.txt__) with your preferred text editor, enter
both examples (primitives and group) to that file and load the configuration to
the cluster manager configuration.

_As user root_

[subs="attributes"]
----
# crm configure load update aws_crm_ers.txt
----

==== Configuring the Colocation Constraints Between ASCS and ERS

The constraints between the ASCS and ERS instance are needed to define that the
ASCS instance should start up exactly on the cluster node running the ERS
instance after a failure (loc_sap_{mysid}_failover_to_ers). This constraint is
needed to ensure that the locks are not lost after an ASCS instance (or node)
failure.

If the ASCS instance has been started by the cluster the ERS instance should
be moved to an "other" cluster node (col_sap_{mysid}_no_both). This constraint
is needed to ensure that the ERS will synchronize the locks again and the cluster is
ready for an additional take-over.

.Location constraint
================================================
[subs="attributes"]
----
colocation col_sap_{mysid}_no_both -5000: grp_{mySID}_{myInstErs} grp_{mySID}_{myInstAscs}
location loc_sap_{mysid}_failover_to_ers rsc_sap_{mySID}_{myInstAscs} \
         rule 2000: runs_ers_{mySID} eq 1
order ord_sap_{mysid}_first_start_ascs Optional: rsc_sap_{mySID}_{myInstAscs}:start \
      rsc_sap_{mySID}_{myInstErs}:stop symmetrical=false
----
================================================

Create a text file (like _crm_col.txt_) with your preferred text editor, enter
all three constraints to that file and load the configuration to the
cluster manager configuration.

Issue the following command as root:

[subs="attributes"]
----
# crm configure load update crm_col.txt
----

==== Activating the Cluster

Now the last step is to end the cluster maintenance mode and to allow the
cluster to detect already running resources.

Issue the following command as root:

[subs="attributes"]
----
# crm configure property maintenance-mode="false"
----

The cluster will now start the ASCS and the ERS system. This can take a few
minutes. Check progress with the command:

[subs="attributes"]
----
# crm status
----

////
############## REMOVING THIS FROM THE OUTPUT FOR NOW ###########

=== Installing SAP Licenses

Most likely you have your own established method to maintain your {sap} licenses.
This section is only a reminder. You should have installed two license keys as
we need to failover the ASCS {sap} instance.

- Get the HWKEY of both cluster nodes
- Get the license from the SAP launchpad
- Install the two licenses per HWKEY using transaction SLICENSE

###################
////

////
############## REMOVING THIS FROM THE OUTPUT FOR NOW ###########

=== Optional - Installing the HA Test Tool HATool

This is an optional task. If you like to check your cluster using {sap}s HATool
you might find these notes helpful. Always follow {SAP}s documentation
how to install and use the HATool.

We install the test Client at {myNode3}. The server part is imported
as an SAP Transport. In our environment the {sap} dialog instance is also running
at {myNode3}, so we also store the transport files there.

- Installing the HATool Client (as root)

[subs="attributes"]
----
# cd /root/herschel
# unzip HATool_v213.zip
(creates directories HATool/{bin,check,config,doc,event,server)
----

- Installing other sw parts of the HATool Client (as root).

See also sections 3.2 and 4.2 of the "High Availability Test Tool - Installation
& Operations Guide"

[subs="attributes"]
----
# cd /root/herschel
##
## SAP JVM8 - we skipped that part, because we have Oracle VM8
##
#######
##
## SAP JCO 3.0
##
# mkdir SAPJCO; cd SAPJCO
# Download from Marketplace
# unzip mkdir SAPJCOmkdir SAPJCO
# tar -xvzf sapjco3-linuxx86_64-3.0.16.tgz
# unzip sapjco30P16HF_1-10009485.zipsapjco30P16HF_1-10009485.zip
# ln -s $PWD/sapjco3.jar /root/herschel/HATool/bin
# ln -s $PWD/libsapjco3.so /root/herschel/HATool/bin
##
## d3.min.js - graphic library
##
# mkdir d3.min.js
# wget https://d3js.org/d3.v3.min.js
# mv d3.v3.min.js d3.min.js/d3.min.js
# ln -s /root/herschel/d3.min.js/d3.v3.min.js \
      /root/herschel/HATool/bin/d3.min.js
----

- Importing the HATool server part into the SAP system.
This is described in section 4.3 in the "High Availability Test Tool -
Installation & Operations Guide".

- Create and configure user hatool.
This is described in section 4.3 in the "High Availability Test Tool -
Installation & Operations Guide".

- Set up the first property file for a smoke test.
In directory /root/herschel/HATool/config change the file
haQuickStartTemplate.properties like the following patch does. So setting
systemclient, user, password, messagehost and SID.

[subs="attributes"]
----
 --- haQuickStartTemplate.properties     2015-10-14 14:50:48.000000000 +0200
 +++ FHhaQuickStartTemplate.properties   2017-04-24 20:43:41.117512210 +0200
 @@ -53,20 +53,20 @@ resettestdata = 1
  ###################################
  ########## login information for server user ###########
 -systemclient = nnn
 -user = xxxxxxxxxx
 -password = xxxxxxxxx
 +systemclient = 001
 +user = hatool
 +password = SuSE1234
  language = en
  ################# RFC ###################
  # login via message server
  -msgserverhost = xxxxxxxxxx
  +msgserverhost = {myVipNAscs}
  # the port information is necessary, if there is no entry for sapms systemid in /etc/services resp. C:\Windows\System32\drivers\etc\services
  #msgserverport = 3600
  # for the logon group, PUBLIC is used by default; if no explicit group was created, use SPACE
  logongroup = SPACE
  # if msgserverport is not set, systemid is mandatory
 -systemid = SID
 +systemid = {mySid}
 ####################
 #   One HA Event   #
----

- Testrun
[subs="attributes"]
 java -cp haTestTool.jar:sapjco3.jar com.sap.test.haload.ClientDriver \
     file=FHhaQuickStartTemplate.properties

- Adapting the test properties
This is described in section 3.2 of the "SAP Application Server HA Interface
Certification" guide.

** Login.properties - set systemclient, user, password, messagehost and SID
** TEC05Event.properties - set eventcall to TEC05Event.sh
** TEC14Event.properties - set eventcall to TEC14Event.sh
** TEC05Event.sh - edit the _sapcontrol_ command

[subs="attributes"]
----
 /usr/sap/{mySid}/SYS/exe/uc/linuxx86_64/sapcontrol -prot NI_HTTP \
   -user {mySapAdm} SuSE1234 \
   -host {myVipNAscs} -nr {myAscsIno} -function HAFailoverToNode ""
----
** TEC14Event.sh - edit the sapctrl command

[subs="attributes"]
----
 /usr/sap/{mySid}/SYS/exe/uc/linuxx86_64/sapcontrol -prot NI_HTTP \
   -user {mySapAdm} SuSE1234 \
   -host {myVipNAscs} -nr {myAscsIno} -function UpdateSystem 120 300 1
----

NOTE: Possibly you need to change the rks-host (here {myVipNAscs}) after you get
feedback from SAP which instance should be referenced.

##############
////


////
############################
#
# ADMINISTRATION
#
############################
////

== Administration

=== Dos and Don'ts

==== Never Stop the ASCS Instance

For normal operation *do not stop* the ASCS {sap} instance with any tool such
as cluster tools or {sap} tools. The stop of the ASCS instance might lead to a loss of enqueue
locks. Because following the new {sapCert} certification the cluster must allow local restarts
of the ASCS. This feature is needed to allow Rolling Kernel Switch (RKS) updates without
reconfiguring the cluster.

WARNING: Stopping the ASCS instance might lead into the loss of {sap} enqueue
  locks during the start of the ASCS on the same node.

==== How to Migrate ASCS

To *migrate* the ASCS {sap} instance you should use the {sap} tools such as
  the {sap} management console. This will trigger {sapStartSrv} to use the
  {s4sClConnector} to migrate the ASCS instance. As user _{mysapadm}_ you might call
  the following command to migrate-away the ASCS. The migrate-away will always
  migrate the ASCS to the ERS side which will keep the {sap} enqueue locks.

_As {mysapadm}_

[subs="attributes"]
----
# sapcontrol -nr 00 -function HAFailoverToNode ""
----

==== Never Block Resources

With {sapCert} it is *not longer allowed to block resources* from being
  controlled manually. This using the variable __BLOCK_RESOURCES__ in
  __/etc/sysconfig/sap_suse_cluster_connector__ is not allowed anymore.

==== Always Use Unique Instance Numbers

Currently all {sap} *instance numbers controlled by the cluster must be unique*.
  If you need to have multiple dialog instances such as D00 running on different
  systems they should be not controlled by the cluster.

==== How to Set the Cluster in Maintenance Mode

Procedure to set the cluster into maintenance mode can be done as _root_ or _sidadm_.

_As user root_

[subs="attributes"]
----
# crm configure properties maintenance-mode="true"
----

_As user {mysapadm} (the full path is needed)_

[subs="attributes"]
----
# /usr/sbin/crm configure properties maintenance-mode="true"
----

==== Procedure to End the Cluster Maintenance

_As user root_

[subs="attributes"]
----
# crm configure properties maintenance-mode="false"
----

==== Cleanup Resources

How to *cleanup resource failures*? Failures of the ASCS will be automatically
  deleted to allow a failback after the configured period of time. For all other
  resources you can cleanup the status including the failures:

_As user root_

[subs="attributes"]
----
# crm resource cleanup RESOURCE-NAME
----

WARNING: You should not cleanup the complete group of the ASCS resource as this
   might lead into an unwanted cluster action to take over the complete group onto
   the node where ERS instance is running.

=== Test the Cluster

We strongly recommend that you at least process the following tests before you
plan going into production with your cluster.

==== Check Product Names with HAGetFailoverConfig

Check if the name of the SUSE cluster solution is shown in the output of
  sapcontrol or {sap} management console. This test checks the status of the
  {sapNW} cluster integration.


_As user {mysapadm}_

[subs="attributes"]
----
# sapcontrol -nr 00 -function HAGetFailoverConfig
----

==== Start SAP Checks Using HACheckConfig and HACheckFailoverConfig

Check if the HA configuration tests are showing no errors.

_As user {mysapadm}_

[subs="attributes"]
----
# sapcontrol -nr 00 -function HACheckConfig
# sapcontrol -nr 00 -function HACheckFailoverConfig
----

==== Manually Migrate ASCS

Check if manually migrating the ASCS using HA tools works properly.

_As user root_

[subs="attributes"]
----
# crm resource migrate rsc_sap_{mySid}_{myInstAscs} force
## wait till the ASCS is been migrated to the ERS host
# crm resource unmigrate rsc_sap_{mySid}_{myInstAscs}
----

==== Migrate ASCS Using HAFailoverToNode

Check if moving the ASCS instance using {sap} tools like {sapCtrl} does work
properly.

_As user {mysapadm}_

[subs="attributes"]
----
# sapcontrol -nr 00 -function HAFailoverToNode ""
----

==== Test ASCS Migration After Failure

Check if the ASCS instance moves correctly after a node failure.

_As user root_

[subs="attributes"]
----
## on the ASCS host
# echo b >/proc/sysrq-trigger
----

==== Inplace Restart of ASCS Using Stop and Start

Check if the inplace re-start of the {sap} resources have been processed
  correctly. The {sap} instance should not failover to an other node, it
  must start on the same node where it has been stopped.

WARNING: This test will force the SAP system to *lose* the enqueue locks.
   *This test should not be processed during production.*

_As user {mysapadm}_

[subs="attributes"]
----
## example for ASCS
# sapcontrol -nr 00 -function Stop
## wait till the ASCS is completely down
# sapcontrol -nr 00 -function Start
----

==== Additional Tests to Perform

* Automated restart of the ASCS (simulating RKS)

* Check the recoverable and non-recoverable outage of the message server process

* Check the non-recoverable outage of the {sap} enqueue server process

* Check the outage of the {sapERS}

* Check the outage and restart of {sapStartSrv}

* Check the Rolling Kernel Switch procedure (RKS), if possible

* Check the simulation of an upgrade

* Check the simulation of cluster resource failures

:leveloffset: 0

== AWS Specific Post Installation Tasks

The optional installation of the Route 53 agent will update the DNS name of
the central instance as needed.
The Route 53 naming tables may need to be made visible for on premises users
like SAP GUI users. This happens through updating the on-premises name
servers to delegate name resolution to Route 53.
This forwarding of name resolution requests acquiresan extra configuration in
the AWS VPC.

Active directory users need to configure an Active Directory Connector as
described in [https://aws.amazon.com/de/blogs/security/how-to-set-up-dns-resolution-between-on-premises-networks-and-aws-using-aws-directory-service-and-amazon-route-53/] .

DNS server users need to implement bind forwarding EC2 instances as
described in [http://www.scalingbits.com/aws/dnsfailover/backpropagation].


== Additional Implementation Scenarios

=== Adaptive Server Enterprise Replication Failover Automation Integration

==== FM Integration with SUSE Linux Enterprise High Availability Extension Cluster

Standard SAP on AWS for an HA setup is Multi-AZ deployment with ASCS, Primary DB running in
one AZ and their counterpart ERS and Secondary DB running in the second AZ of the same region.
The Primary Application Server & Additional Application servers based on the load can be distributed
in both AZ’s as well to provide resiliency.
Considering a scenario where SAP NetWeaver or Business Suite system is running on SAP Sybase ASE.
The completely automated HA for the ABAP Stack (ASCS) is provided by the SUSE Linux Enterprise High Availability Extension Cluster. For
the Sybase ASE DB the HA feature is provided with the Always On configuration and the failover
orchestration is done by the Fault Manager (FM) utility which traditionally was installed on a third host
(other than the Primary & Secondary DB). In an SAP world the FM utility comes along with an SAP DB
dependent Kernel and gets installed in the ASCS Work directory _/usr/sap/<SID>/ASCS<instnr>/exe/_. The
failover of the ASCS instance along with the associated directories (provided they are installed on a
shared file system using either Amazon EFS or NFS) is taken care by the SUSE Linux Enterprise High Availability Extension Cluster.

==== Sybase ASE Always On

SAP Sybase ASE comes with an Always On feature which provides native HA & DR capability. The
always-on option is a high availability and disaster recovery (HADR) system that consists of two
SAP ASE servers: One is designated as the primary server, on which all transaction processing takes place. The
other acts as a warm standby (called "standby server" in DR mode, and as a "companion" in
HA mode) for the primary server, and contains copies of designated databases from the primary server.
The failover orchestration is carried out by ASE provided utility called Fault Manager. The Fault
Manager monitors the various components of the HADR environment – Replication Management Agent
(RMA), ASE, Replication Server, applications, databases, and the operating system. Its primary job is
to ensure the high-availability (zero data loss during failover) of the ASE cluster by initiating automatic
failover with minimal manual intervention. In an SAP Stack, the fault manager utility (sybdbfm) comes
as part of the DB (Sybase ASE) dependent SAP Kernel.
Refer to the SAP Standard ASE HA-DR guide (https://help.sap.com/viewer/efe56ad3cad0467d837c8ff1ac6ba75c/16.0.3.6/en-US/a6645e28bc2b1014b54b8815a64b87ba.html)
for setting up the Sybase ASE DB in HA mode.

IMPORTANT: In the following section we use sometimes examples and sometimes general examples. In the general are terms like <SID>; <instance nr>. They must be adapted to your environment. As an example, _su - <sid>adm_ means _su - ha7adm_ or in capital letters _cd /usr/sap/<SID>/ASCS<instance nr>/work_ means _cd /usr/sap/HA7/ASCS00/work_

==== Database Host Preparation

This guide does not duplicate the official HADR documentation. The following procedure describes the
key points which you need to take care of.

.Installation 32-bit Environment

[subs="specialchars,attributes"]
----
# zypper install glibc-32bit libgcc_s1-32bit
----

For the example this software stack is used:

* SL TOOLSET 1.0 -- SWPM -> 1.0 SP25 for NW higher than 7.0x
* saphostagent -> 7.21 patch 41
* SAP Kernel -> 7.53 PL421
* SAP Installation Export ->  (51051806_1)
* Sybase RDBMS->  ASE 16.0.03.06 RDBMS (51053561_1)

NOTE: Very useful is that short table of installation information which helps to be prepared for the next steps.
SAP Adaptive Server Enterprise - Installation Worksheet
https://help.sap.com/viewer/efe56ad3cad0467d837c8ff1ac6ba75c/16.0.3.6/en-US/3fe35550f3814b2bb411d5494976e25a.html

IMPORTANT: The Fault Manager is enhanced to work in this setup. The minimal version's which support this scenario are
* SAP Kernel 749 PL632
* SAP Kernel 753 PL421

==== Database Installation for Replication Scenario

The installation can be done with the SWPM which is provided by SAP.

.Installing the primary database with SWPM:
* SWPM option depends on {sapNW} version and architecture
** Software Provisioning Manager 1.0 SP 25 -> SAP NetWeaver AS for ABAP 7.52 -> SAP ASE -> Installation -> Application Server ABAP -> High-Availability System -> Database Instance

The following information is requested from the wizard:

* Master Password <secure>
* SAP System Code Page: Unicode (default)
* Uncheck: -> Set FQDN for SAP system
* Sybase database Administrator UID: 2003
* In our test setup we uncheck --> Use separate devices for sybmgmtdb database

After the basis installation is finished the primary database must be prepared for the replication. First
the user *sa* must be unlocked.

[subs="specialchars,attributes"]
----
# su - syb<sid>
# isql -Usapsso -P <secure password> -S<SID> -X -w1900
# 1> go
# 1> exec sp_locklogin sa, 'unlock'
# 2> go
# Account unlocked.
# (return status = 0)
# 1> quit
----

In the next step, install the SRS software with a response file and enter the following command as user _syb<sid>_:

[subs="specialchars,attributes"]
----
# /sapcd/ase-16.0.03.06/BD_SYBASE_ASE_16.0.03.06_RDBMS_for_BS_/SYBASE_LINUX_X86_64/setup.bin -f /sybase/HA7/srs-setup.txt -i silent
----

Activate HADR on primary node with a response file and enter the following command as user _syb<sid>_:

[subs="specialchars,attributes"]
----
# setuphadr /sybase/HA7/HA7_primary_lin.rs.txt
----

NOTE: If the installation stops with an error message as displayed here, perform the steps explained below:

[subs="specialchars,attributes"]
----
Clean up environment.
Environment cleaned up.
Error: Fail to connect to "PRIM" site SAP ASE at "<hostname>:4901".  
----

Check if the host name and port number are correct and the database server is up and running. If everything is correct and network connection should be available, it might help to modify the _interface_ file. Try to add a new line in the _/sybase/<SID>/interfaces_ file for the <SID> section with the IP address of the corresponding host name.

[subs="specialchars,attributes"]
----
# vi /sybase/<SID>/interfaces
...
	master tcp ether <hostname> 4901
	master tcp ether 172.17.1.21 4901
...
----

Create a secure store key entry for the database:

[subs="specialchars,attributes"]
----
# /usr/sap/hostctrl/exe/saphostctrl -user sapadm <secure password> -function LiveDatabaseUpdate -dbname <SID> -dbtype syb -dbuser DR_admin -dbpass <Secure password> -updatemethod Execute -updateoption TASK=SET_USER_PASSWORD -updateoption USER=DR_ADMIN
----


.Installing the companion database with SWPM:
* SWPM option depends on {sapNW} version and architecture
** Software Provisioning Manager 1.0 SP 25 -> SAP NetWeaver AS for ABAP 7.52 -> SAP ASE -> Database Replication -> Setup of Replication Environment

The following information is requested from the wizard:

* Replication System Parameters --> SID, Master Password, check Set up a secondary database instance
* Primary Database server --> host name or virt. name
* Primary Database server port --> default is 4901, depends on the setup of your primary server

After the basis installation is finished the companion database must be prepared for the replication. First
the user *sa* must be unlocked.

[subs="specialchars,attributes"]
----
# su - syb<sid>
# isql -Usapsso -P <secure password> -S<SID> -X -w1900
# 1> go
# 1> exec sp_locklogin sa, 'unlock'
# 2> go
# Account unlocked.
# (return status = 0)
# 1> quit
----

Next step installing the SRS software with a response file on the companion site and enter the following command as user syb<sid>:

[subs="specialchars,attributes"]
----
# /sapcd/ase-16.0.03.06/BD_SYBASE_ASE_16.0.03.06_RDBMS_for_BS_/SYBASE_LINUX_X86_64/setup.bin -f /sybase/HA7/srs-setup.txt -i silent
----

Activate HADR on companion node with a response file and enter the following command as user syb<sid>:

[subs="specialchars,attributes"]
----
# setuphadr /sybase/HA7/HA7_companion_lin.rs.txt
----

NOTE: In certain circumstances the installation is not successful. It could help to set up the primary system again and install the companion afterward.

NOTE: If the system is reinstalled and the companion system reports *Missing read/write permissions* for this directory _/tmp/.SQLAnywhere_, check the permission on both node. In case the ownership must be changed run the setup again on both nodes. Start with the *Primary*.

Creating a secure store key entry for the database:

[subs="specialchars,attributes"]
----
# /usr/sap/hostctrl/exe/saphostctrl -user sapadm <secure password> -function LiveDatabaseUpdate -dbname <SID> -dbtype syb -dbuser DR_admin -dbpass <Secure password> -updatemethod Execute -updateoption TASK=SET_USER_PASSWORD -updateoption USER=DR_ADMIN
----

==== Fault Manager Installation

The Fault Manager is configured on the ASCS host. The benefit from this setup is that the sybdbfm service
can be monitored and tracked with the existing pacemaker for the ASCS / ERS replication.

[subs="specialchars,attributes"]
----
# su - <sid>adm
# cd /usr/sap/<SID>/ASCS00/exe/
# sybdbfm install
----

.Fault Manager Installation:
====
[subs="specialchars,attributes,quotes,verbatim"]
----
replication manager agent user DR_admin and password set in Secure Store.
Keep existing values (yes/no)? (yes)
SAPHostAgent connect user sapadm and password set in Secure Store.
Keep existing values (yes/no)? (yes)
Enter value for primary database host: (suse7cl1)
suse7db1
Enter value for primary database name: (HA7)
Enter value for primary database port: (4901)
Enter value for primary site name: (Site1)
Enter value for primary database heart beat port: (13777)
Enter value for standby database host: (suse7cl1)
suse7db2
Enter value for standby database name: (HA7)
Enter value for standby database port: (4901)
Enter value for standby site name : (Site2)
Enter value for standby database heart beat port: (13787)
Enter value for fault manager host: (s7as-service)
Enter value for heart beat to heart beat port: (13797)
Enter value for support for floating database ip: (no)
Enter value for use SAP ASE Cockpit if it is installed and running: (no)
----
====

Update the values as per your environment for the Primary DB & companion DB host name, SID &
Site Name. Make sure to use the virtual host name for the ASCS host. When the Fault Manager is installed,
profile for it will be created in the _/sapmnt/<SID>/profile_ by the name _SYBHA.PFL_ and will have the
configuration details.
Restart the ASCS Instance which will also start the Fault Manager that has been added to the start profile as below:

NOTE: In case of a reinstallation it might be better to overwrite the existing user name and password in the secure store for the _sapadm_ and _DR_admin_ if the old values are not 100% known.

[subs="specialchars,attributes"]
----
# cat /sapmnt/<SID>/profile/<SID>_ASCS00_<virt. ASCS hostname>
....
#-----------------------------------------------------------------------
# copy sybdbfm and dependent
#-----------------------------------------------------------------------
_CP_SYBDBFM_ARG1 = list:$(DIR_CT_RUN)/instancedb.lst
Execute_06 = immediate $(DIR_CT_RUN)/sapcpe$(FT_EXE) pf=$(_PF) $(_CP_SYBDBFM_ARG1)
_CP_SYBDBFM_ARG2 = list:$(DIR_GLOBAL)/syb/linuxx86_64/cpe_sybodbc.lst
_CP_SYBDBFM_ARG3 = source:$(DIR_GLOBAL)/syb/linuxx86_64/sybodbc
Execute_07 = immediate $(DIR_CT_RUN)/sapcpe$(FT_EXE) pf=$(_PF) $(_CP_SYBDBFM_ARG2) $(_CP_SYBDBFM_ARG3)
#-----------------------------------------------------------------------
# Start sybha
#-----------------------------------------------------------------------
_SYBHAD = sybdbfm.sap$(SAPSYSTEMNAME)_$(INSTANCE_NAME)
_SYBHA_PF = $(DIR_PROFILE)/SYBHA.PFL
Execute_08 = local rm -f $(_SYBHAD)
Execute_09 = local ln -s -f $(DIR_EXECUTABLE)/sybdbfm$(FT_EXE) $(_SYBHAD)
Restart_Program_02 = local $(_SYBHAD) hadm pf=$(_SYBHA_PF)
#-----------------------------------------------------------------------
....
----

The status of the FM can be checked as below. Navigate to the ASCS work directory and then run
_sybdbfm.sap.<SID>_ASCS<instance number> status_ :

_As user {mysapadm}_

[subs="specialchars,attributes,quotes,verbatim"]
----
# cd /usr/sap/<SID>/ASCS<instance number>/work
# ./sybdbfm.sap<SID>_ASCS<instance number> status

fault manager running, pid = 23234, fault manager overall status = OK, currently executing in mode PAUSING
**sanity check report (208)**.
node 1: server <DB server1>, site <site name one>.
db host status: OK.
db status OK hadr status PRIMARY.
node 2: server <DB server2>, site <site name two>.
db host status: OK.
db status OK hadr status STANDBY.
replication status: SYNC_OK.
----

Checking the log file is also a suitable meassure to validate the status.

_As user {mysapadm}_

[subs="specialchars,attributes"]
----
# cd /usr/sap/<SID>/ASCS<instance number>/work
# tail -f dev_sybdbfm
# ...

2019 09/11 15:34:30.523 (23234) ----- Log messages ----

2019 09/11 15:34:30.523 (23234) Info: saphostcontrol: Executing LiveDatabaseUpdate

2019 09/11 15:34:30.523 (23234) Info: saphostcontrol: LiveDatabaseUpdate successfully executed

2019 09/11 15:34:30.524 (23234) call is running.
2019 09/11 15:34:30.534 (23234) call exited (exit code 0).
2019 09/11 15:34:30.534 (23234) db status is:
 DB_OK.
2019 09/11 15:34:42.561 (23234) *** sanity check report (136)***.
2019 09/11 15:34:42.562 (23234) node 1: server <DB server1>, site <site name one>.
2019 09/11 15:34:42.562 (23234) db host status: OK.
2019 09/11 15:34:42.562 (23234) db status OK hadr status PRIMARY.
2019 09/11 15:34:42.562 (23234) node 2: server <DB server2>, site <site name two>.
2019 09/11 15:34:42.562 (23234) db host status: OK.
2019 09/11 15:34:42.562 (23234) db status OK hadr status STANDBY.
2019 09/11 15:34:42.562 (23234) replication status: SYNC_OK.
2019 09/11 15:34:57.688 (23234) *** sanity check report (137)***.
2019 09/11 15:34:57.688 (23234) node 1: server <DB server1>, site <site name one>.
2019 09/11 15:34:57.688 (23234) db host status: OK.
2019 09/11 15:34:57.688 (23234) db status OK hadr status PRIMARY.
2019 09/11 15:34:57.688 (23234) node 2: server <DB server2>, site <site name two>.
2019 09/11 15:34:57.688 (23234) db host status: OK.
2019 09/11 15:34:57.688 (23234) db status OK hadr status STANDBY.
2019 09/11 15:34:57.688 (23234) replication status: SYNC_OK.
2019 09/11 15:35:12.827 (23234) *** sanity check report (138)***.
2019 09/11 15:35:12.827 (23234) node 1: server <DB server1>, site <site name one>.
2019 09/11 15:35:12.827 (23234) db host status: OK.
2019 09/11 15:35:12.827 (23234) db status OK hadr status PRIMARY.
2019 09/11 15:35:12.827 (23234) node 2: server <DB server2>, site <site name two>.
2019 09/11 15:35:12.827 (23234) db host status: OK.
2019 09/11 15:35:12.827 (23234) db status OK hadr status STANDBY.
2019 09/11 15:35:12.827 (23234) replication status: SYNC_OK.
# ...
----


Below are a few parameters that need to be updated in the _SYBHA.PFL_ to make the failover working.

[subs="specialchars,attributes,verbatim,quotes"]
----
ha/syb/support_cluster = 1
ha/syb/failover_if_unresponsive = 1
ha/syb/allow_restart_companion = 1
ha/syb/set_standby_available_after_failover = 1
ha/syb/chk_restart_repserver = 1
ha/syb/cluster_fmhost1 = **Hostname for Node 1 of the ASCS HA Setup**
ha/syb/cluster_fmhost2 = **Hostname for Node 2 of the ASCS HA Setup**
ha/syb/use_boot_file_always = 1
ha/syb/dbfmhost = **virtual hostname of ASCS instance**
----

Details of all the FM parameters can be found in the *SAP ASE HA DR User Guide*. Those highlighted
in bold are of interest for the setup. Since the FM is installed with the ASCS which can
failover from Node 1 to Node 2, the parameters _ha/syb/cluster_fmhost1_ and
_ha/syb/cluster_fmhost2_ provide the physical host names of both nodes where the FM can potentially run.

In a scenario where the complete Availability Zone (AZ1) goes down and the ASCS and Primary database are running there, the DB failover is not triggered
until the ASCS failover is complete and the FM is up and running in the 2nd Availability Zone (AZ2). The FM then needs to read the boot file
to get the prior state of the ASE DB. This is mandatory to ensure that FM can trigger the failover correctly. The parameter
_ha/syb/use_boot_file_always=1_ makes sure that the FM always reads from the boot file which is part of the work
directory (the same for ASCS and FM) and failover along with FM.

=== Cluster Integration of Fault Manager

We implement the FM in the pacemaker environment as part of the ASCS instance.

.FM is integrated as included service along with the ASCS.
==============================================
The cluster configuration for the _primitive rsc_sap____<SID>____ASCS<instance number>_ have to be modified.
In the example we use:

- <SID> => {mySid}
- <instance number> => {myAscsIno}
- virtual hostname => {myVipNAscs}

[subs="specialchars,attributes,quotes,verbatim"]
----
# crm configure edit rsc_sap_{mySid}_ASCS{myAscsIno}
----

[subs="specialchars,attributes,quotes,verbatim"]
----
primitive rsc_sap_{mySid}_ASCS{myAscsIno} SAPInstance \
        operations $id=rsc_sap_{mySid}_ASCS{myAscsIno}-operations \
        op monitor interval=11 timeout=60 on-fail=restart \
        params InstanceName={mySid}_ASCS{myAscsIno}_{myVipNAscs} START_PROFILE="/sapmnt/{mySid}/profile/{mySid}_ASCS{myAscsIno}_{myVipNAscs}" \
        AUTOMATIC_RECOVER=false MONITOR_SERVICES="sybdbfm|msg_server|enserver" \
        meta resource-stickiness=5000 failure-timeout=60 migration-threshold=1 priority=10
----
The FM service is not part of the default observed SAP instance services. If we specify the *MONITOR_SERVICES*
all default settings are overwritten by the named services. That means we have to count all services which are shown
as a result of the _sapcontrol -nr {myAscsIno} -function GetProcessList_ command. The example above is for an ENSA1 configuration.

NOTE: The cluster configuration is different for ENSA1 and ENS2 installation. The names for the MONITOR_SERVICES differ between this two versions.
==============================================

=== Operating a Pacemaker-Controlled and FM-Monitored ASE Replication Setup

To implement an FM controlled ASE replication setup with pacemaker integration for FM some special rules need to be followed. First of all it needs to be detailed how to check the status of the replication and FM itself.
This chapter will also give guidelines how to improve the takeover time and how to control such an environment.


.Checking the status of the database situation
==========

Check the status and locate the primary site.
_As user {mysapadm}_ on the ASCS host
[subs="specialchars,attributes"]
----
# cd /usr/sap/<SID>/ASCS<instance nr>/work
# ./sybdbfm.sap<SID>_ASCS<instance nr> status
----

And check the log file _dev_sybdbfm_
[subs="specialchars,attributes"]
----
2019 09/13 09:58:52.200 (3290) *** sanity check report (2)***.
2019 09/13 09:58:52.200 (3290) node 1: server sapdb1, site LeonRot.
2019 09/13 09:58:52.200 (3290) db host status: OK.
2019 09/13 09:58:52.200 (3290) db status OK hadr status STANDBY.
2019 09/13 09:58:52.200 (3290) node 2: server sapdb2, site Orlando.
2019 09/13 09:58:52.201 (3290) db host status: OK.
2019 09/13 09dbs_syb_server sapdb1:sapdb2:58:52.201 (3290) db status OK hadr status PRIMARY.
2019 09/13 09:58:52.201 (3290) replication status: SYNC_OK.
----

_As user root_ on the database host
[subs="specialchars,attributes"]
----
# /usr/sap/hostctrl/exe/saphostctrl -user sapadm <secure password> -dbname <SID> -dbtype syb -function GetDatabaseSystemStatus
# /usr/sap/hostctrl/exe/saphostctrl -user sapadm <secure password> -dbname <SID> -dbtype syb -function GetDatabaseStatus
# /usr/sap/hostctrl/exe/saphostctrl -user sapadm <secure password> -dbname <SID> -dbtype syb -function LiveDatabaseUpdate -updatemethod Check -updateoption TASK=REPLICATION_STATUS
----

_As user syb<sid>_ on the database host
[subs="specialchars,attributes"]
----
#  isql -UDR_admin -P <secure password> -S<db host>:4909 -X -w 1000
1> sap_status active_path
2> go
----

==========

The application server (PAS and AAS) environment must be adapted for the DB failover situation (takeover). On each host which is providing a dialog server (PAS; AAS) the _.dbenv.sh_ and or _.dbenv.csh_ file needs to be extended.

.Modify the DB Environment Settings on the Dialog Server
==========
Add the missing value and extend the settings as below shown on each host who runs a dialog application server.

_As user {mysapadm}_
[subs="specialchars,attributes"]
----
# vi .dbenv.csh
...
setenv dbs_syb_server <server1:server2>
setenv dbs_syb_ha 1
...
----

_As user {mysapadm}_
[subs="specialchars,attributes"]
----
# vi .dbenv.sh
...
dbs_syb_server=<server1:server2>
export dbs_syb_server
dbs_syb_ha=1
export dbs_syb_ha
...
----
==========

IMPORTANT: The instance must be restarted to activate the changes.

.OS Settings for Faster Reaction Time After Primary DB Host is Down
=================

The default tcp_retries value is to high and causes a very long takeover time. With ASE16 PL7 the behavior is modified.
Up to this patch the change below improve the takeover time.

_As user root_
[subs="specialchars,attributes"]
----
# echo 3 >/proc/sys/net/ipv4/tcp_retries2
## makes the changes online
# vi /etc/sysctl.conf
...
net.ipv4.tcp_retries2 = 3
...
## makes the changes reboot persistent
----

=================


.Starting and Stopping The SAP System and Databases in Replication Mode
================

If Fault Manager is monitoring the Primary and Companion database and Fault Manager is monitored by Pacemaker there is a special prcedure necessary to start and stop the system.

.In general these steps are important to *start* the system:

* Start companion database + replication server
* Start primary database + replication server
* Change cluster maintenance mode to false
** Start ASCS with FM (automatic)
** Start ERS (automatic)
* Start PAS and AAS instances
* Optional: release cluster maintenance mode, if the SAP system was started manually
** File system must be mounted and IP must be set manually
** As user _<sid>adm_ with _sapcontrol -nr <instance number> -function StartSystem_



_As user root_ on companion database host
[subs="specialchars,attributes"]
----
# /usr/sap/hostctrl/exe/saphostctrl -function StartDatabase -dbname <SID> -dbtype syb
# /usr/sap/hostctrl/exe/saphostctrl -function StartDatabase -dbname <SID>_REP -dbtype syb
----

_As user root_ on primary database host
[subs="specialchars,attributes"]
----
# /usr/sap/hostctrl/exe/saphostctrl -function StartDatabase -dbname <SID> -dbtype syb
# /usr/sap/hostctrl/exe/saphostctrl -function StartDatabase -dbname <SID>_REP -dbtype syb
----

_As user root_ on one of the Pacemaker host for ASCS and ERS
[subs="specialchars,attributes"]
----
# crm configure property maintenance-mode=false
----

_As user <sid>adm_ on the host for PAS or AAS
[subs="specialchars,attributes"]
----
# sapcontrol -nr <instance number> -function StartSystem
----

NOTE: If the system should start one by one, use the command _sapcontrol -nr <instance number> -function StartSystem_. The direction must be: ASCS; ERS; PAS; AAS.

.In general these steps are important to *stop* the system:

* Set cluster maintenance mode to _true_
* Stop PAS and AAS instances
* Stop ASCS with FM
* Stop ERS
* Stop primary database + replication server
* Stop companion database + replication server


_As user root_
[subs="specialchars,attributes"]
----
# crm configure property maintenance-mode=true
# crm status
----

_As user <sid>adm_ on one of the Pacemaker host for ASCS and ERS or PAS / AAS
[subs="specialchars,attributes"]
----
# sapcontrol -nr <instance number> -function StopSystem
----

NOTE: If the system should stop one by one, use the command _sapcontrol -nr <instance number> -function Stop_ on each instance host. The direction must be: AAS; PAS; ASCS; ERS.

_As user root_ on primary database host
[subs="specialchars,attributes"]
----
# /usr/sap/hostctrl/exe/saphostctrl -function StopDatabase -dbname <SID> -dbtype syb
# /usr/sap/hostctrl/exe/saphostctrl -function StopDatabase -dbname <SID>_REP -dbtype syb
----

_As user root_ on companion database host
[subs="specialchars,attributes"]
----
# /usr/sap/hostctrl/exe/saphostctrl -function StopDatabase -dbname <SID> -dbtype syb
# /usr/sap/hostctrl/exe/saphostctrl -function StopDatabase -dbname <SID>_REP -dbtype syb
----

IMPORTANT: The Pacemaker-controlled server must be stopped in a proper way, too. Depending on the stonith method which is implemented different procedures are available.

_As user root_ on one cluster node
[subs="specialchars,attributes"]
----
# crm cluster run "crm cluster stop"
----

_As user root_ on each node
[subs="specialchars,attributes"]
----
# reboot
## or
# poweroff
----

================

==== Testing the Replication and FM Cluster Integration

Important for each high availability solution is an extensive testing procedure. That ensures that the solution is working as expected in case of a failure.

.Triggering a Database Failover and Monitoring if FM Is Working
================

Check the status and locate the primary site.
_As user {mysapadm}_ on the ASCS host
[subs="specialchars,attributes"]
----
# cd /usr/sap/<SID>/ASCS<instance nr>/work
# ./sybdbfm.sap<SID>_ASCS<instance nr> status
----

And check the log file _dev_sybdbfm_
[subs="specialchars,attributes"]
----
2019 09/13 09:58:52.200 (3290) *** sanity check report (2)***.
2019 09/13 09:58:52.200 (3290) node 1: server sapdb1, site LeonRot.
2019 09/13 09:58:52.200 (3290) db host status: OK.
2019 09/13 09:58:52.200 (3290) db status OK hadr status STANDBY.
2019 09/13 09:58:52.200 (3290) node 2: server sapdb2, site Orlando.
2019 09/13 09:58:52.201 (3290) db host status: OK.
2019 09/13 09dbs_syb_server sapdb1:sapdb2:58:52.201 (3290) db status OK hadr status PRIMARY.
2019 09/13 09:58:52.201 (3290) replication status: SYNC_OK.
----

* Now destroy the primary database server.
* Monitor the takeover process with the FM on the ASCS host.

_As user {mysapadm}_ on the ASCS host
[subs="specialchars,attributes"]
----
# cd /usr/sap/<SID>/ASCS<instance nr>/work
# tail -f  dev_sybdbfm
----
================

.Selected Output From the Takeover Process.
=========

[subs="specialchars,attributes,quotes,verbatim"]
-----
...
    2019 09/13 11:08:38.301 (3290) ** *** sanity check report (270)*** **.
    2019 09/13 11:08:38.301 (3290) node 1: server sapdb1, site LeonRot.
    2019 09/13 11:08:38.301 (3290) db host status: OK.
    2019 09/13 11:08:38.301 (3290) db status OK hadr status STANDBY.
    2019 09/13 11:08:38.301 (3290) node 2: server sapdb2, site Orlando.
    2019 09/13 11:08:38.301 (3290) db host status: OK.
    2019 09/13 11:08:38.301 (3290) db status OK hadr status PRIMARY.
    2019 09/13 11:08:38.301 (3290) replication status: SYNC_OK.
    2019 09/13 11:08:50.416 (3290) ERROR in function SimpleFetch (1832) (SQLExecDirect failed): (30046) [08S01] [SAP][ASE ODBC Driver]Connection to the server has been lost. Unresponsive Connection was disconnected during command timeout. Check the server to determine the status of any open transactions.
    2019 09/13 11:08:50.416 (3290) ERROR in function SimpleFetch (1832) (SQLExecDirect failed): (30149) [HYT00] [SAP][ASE ODBC Driver]The command has timed out.
    2019 09/13 11:08:50.416 (3290) execution of statement master..sp_hadr_admin get_request, '1' failed.
    2019 09/13 11:08:50.416 (3290) ERROR in function SimpleFetch (1824) (SQLAllocStmt failed): (30102) [HY010] [SAP][ASE ODBC Driver]Function sequence error
    2019 09/13 11:08:50.416 (3290) execution of statement select top 1 convert( varchar(10), @@hadr_mode ) || ' ' || convert( varchar(10), @@hadr_state ) from sysobjects failed.
    2019 09/13 11:08:50.416 (3290) disconnect connection
    2019 09/13 11:09:22.505 (3290) ERROR in function SQLConnectWithRetry (1341) (SQLConnectWithRetry failed): (30293) [HY000] [SAP][ASE ODBC Driver]The socket failed to connect within the timeout specified.
    2019 09/13 11:09:22.505 (3290) ERROR in function SQLConnectWithRetry (1341) (SQLConnectWithRetry failed): (30012) [08001] [SAP][ASE ODBC Driver]Client unable to establish a connection
    2019 09/13 11:09:22.505 (3290) connected with warnings (555E69805100)
    2019 09/13 11:09:22.505 (3290) ERROR in function SimpleFetch (1824) (SQLAllocStmt failed): (30293) [HY000] [SAP][ASE ODBC Driver]The socket failed to connect within the timeout specified.
    2019 09/13 11:09:22.505 (3290) ERROR in function SimpleFetch (1824) (SQLAllocStmt failed): (30012) [08001] [SAP][ASE ODBC Driver]Client unable to establish a connection
    2019 09/13 11:09:22.505 (3290) execution of statement select top 1 convert( varchar(10), @@hadr_mode ) || ' ' || convert( varchar(10), @@hadr_state ) from sysobjects failed.
    2019 09/13 11:09:22.505 (3290) disconnect connection
    2019 09/13 11:09:22.505 (3290) primary site unusable.
...
    2019 09/13 11:09:22.984 (3290) primary site unusable.
    2019 09/13 11:09:22.984 (3290) ** *** sanity check report (271)*** **.
    2019 09/13 11:09:22.984 (3290) node 1: server sapdb1, site LeonRot.
    2019 09/13 11:09:22.984 (3290) db host status: OK.
    2019 09/13 11:09:22.984 (3290) db status OK hadr status STANDBY.
    2019 09/13 11:09:22.984 (3290) node 2: server sapdb2, site Orlando.
    2019 09/13 11:09:22.984 (3290) db host status: UNUSABLE.
    2019 09/13 11:09:22.984 (3290) db status DB INDOUBT hadr status UNREACHABLE.
    2019 09/13 11:09:22.984 (3290) replication status: SYNC_OK.
    2019 09/13 11:09:23.047 (3290) doAction: Primary database is declared dead or unusable.
    2019 09/13 11:09:23.047 (3290) disconnect connection
    2019 09/13 11:09:23.047 (3290) database host cannot be reached.
    **2019 09/13 11:09:23.047 (3290) doAction: failover.**
...
    2019 09/13 11:11:55.497 (3290) ** *** sanity check report (273)*** **.
    2019 09/13 11:11:55.497 (3290) node 1: server sapdb1, site LeonRot.
    2019 09/13 11:11:55.497 (3290) db host status: OK.
    **2019 09/13 11:11:55.497 (3290) db status OK hadr status PRIMARY.**
    2019 09/13 11:11:55.497 (3290) node 2: server sapdb2, site Orlando.
    2019 09/13 11:11:55.497 (3290) db host status: UNUSABLE.
    2019 09/13 11:11:55.498 (3290) db status DB INDOUBT hadr status UNREACHABLE.
    2019 09/13 11:11:55.498 (3290) replication status: UNKNOWN.
    2019 09/13 11:11:55.555 (3290) doAction: Standby database is declared dead or unusable.
    2019 09/13 11:11:55.555 (3290) disconnect connection
    **2019 09/13 11:11:55.555 (3290) doAction: Companion db host is declared unusable.**
    2019 09/13 11:11:55.555 (3290) doAction: no action defined.
    2019 09/13 11:11:58.568 (3290) Error: NIECONN_REFUSED (No route to host), NiRawConnect failed in plugin_fopen()
...
############ host is coming back online ################
    2019 09/13 11:18:45.579 (3290) call is running.
    2019 09/13 11:18:45.589 (3290) call exited (exit code 0).
    2019 09/13 11:18:45.589 (3290) db status is: DB_OK.
    2019 09/13 11:18:45.589 (3290) doAction: Standby database is declared dead or unusable.
    2019 09/13 11:18:45.589 (3290) disconnect connection
    **2019 09/13 11:18:45.589 (3290) doAction: Companion db host is declared ok.**
    **2019 09/13 11:18:45.589 (3290) doAction: restart database.**
    2019 09/13 11:18:45.805 (3290) Webmethod returned successfully
...
    2019 09/13 11:22:43.677 (3290) ** *** sanity check report (286)*** **.
    2019 09/13 11:22:43.677 (3290) node 1: server sapdb1, site LeonRot.
    2019 09/13 11:22:43.677 (3290) db host status: OK.
    2019 09/13 11:22:43.677 (3290) db status OK hadr status PRIMARY.
    2019 09/13 11:22:43.677 (3290) node 2: server sapdb2, site Orlando.
    2019 09/13 11:22:43.677 (3290) db host status: OK.
    **2019 09/13 11:22:43.677 (3290) db status OK hadr status STANDBY.**
    **2019 09/13 11:22:43.677 (3290) replication status: SYNC_OK.**
...
-----

_As user root_
[subs="specialchars,attributes"]
----
# /usr/sap/hostctrl/exe/saphostctrl -user sapadm <secure password> -dbname <SID> -dbtype syb -function LiveDatabaseUpdate -updatemethod Check -updateoption TASK=REPLICATION_STATUS

Webmethod returned successfully
Operation ID: 5254001F87CB1EE9B5C34755C99DDDFA

----- Response data ----
TASK_NAME=REPLICATION_STATUS
REPLICATION_STATUS=active
PRIMARY_SITE=<site1>
STANDBY_SITE=<site2>
REPLICATION_MODE=sync
ASE transaction log backlog (MB)=0
Replication queue backlog (MB)=0
TASK_STATUS=OK
----- Log messages ----
Info: saphostcontrol: Executing LiveDatabaseUpdate
Info: saphostcontrol: LiveDatabaseUpdate successfully executed
----

=========

.Triggering an FM Failure
================

Killing the FM process more than five times will bring pacemaker in action. Up to five times the saphostagent will take care of the SAP process. If this fail-count is reached in a specific time window, the service will not be restarted.

_As user {mysapadm}_
[subs="specialchars,attributes"]
----
# pkill -9 sybdbfm
## check that the PID has changed
# sapcontrol -nr 00 -function GetProcessList
# pkill -9 sybdbfm
...
# sapcontrol -nr 00 -function GetProcessList
...
sybdbfm, , GRAY, Stopped, , , 11154
...
----

Now pacemaker will restart the FM instance locally first.
_As user root_
[subs="specialchars,attributes"]
----
# crm_mon -1rfn
...
Migration Summary:
* Node <hostname>:
rsc_sap_WAS_ASCS00: migration-threshold=3 fail-count=1 last-failure='Fri Sep 13 13:46:39 2019
...
----

NOTE: If the *fail-count* reaches the defined threshold, the ASCS is moved away from that host.
////
_As user root_
[subs="specialchars,attributes,quotes"]
----
----

_As user root_
[subs="specialchars,attributes,quotes,verbatim"]
=====
=====
////

================

== Appendix

=== CRM Configuration

The complete crm configuration for {sap} system {mySid}

[subs="specialchars,attributes"]
----
#
# nodes
#
node 1084753931: {myNode1}
node 1084753932: {myNode2}
#
# primitives for ASCS and ERS
#
primitive res_AWS_STONITH stonith:external/ec2 \
        op start interval=0 timeout=180 \
        op stop interval=0 timeout=180 \
        op monitor interval=300 timeout=60 \
        params tag=pacemaker profile=cluster
primitive rsc_fs_{mySID}_{myInstAscs} Filesystem \
  params device="efs-name:/ASCS00" \
         directory="/usr/sap/HA1/ASCS00" fstype=nfs4 \
         options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=200s timeout=40s
primitive rsc_fs_{mySID}_{myInstErs} Filesystem \
  params device="efs-name:/ERS10" \
    directory="/usr/sap/HA1/ERS10" fstype=nfs4 \
    options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=40s
primitive rsc_ip_{mySID}_{myInstAscs} ocf:suse:aws-vpc-move-ip \
        params address={myVipAAscs} routing_table=rtb-table-name \
        interface=eth0 profile=cluster \
        op start interval=0 timeout=180 \
        op stop interval=0 timeout=180 \
        op monitor interval=120 timeout=60
primitive rsc_ip_{mySID}_{myInstErs} ocf:suse:aws-vpc-move-ip \
        params address={myVipAErs} routing_table=rtb-table-name \
        interface=eth0 profile=cluster \
        op start interval=0 timeout=180 \
        op stop interval=0 timeout=180 \
        op monitor interval=120 timeout=60
primitive rsc_r53_{mySID}_{myInstAscs} aws-vpc-route53 \
        params hostedzoneid=hosted-zone-id ttl=10 \
        fullname=full-name profile=cluster \
        op start interval=0 timeout=180 \
        op stop interval=0 timeout=180 \
        op monitor interval=300 timeout=180
primitive rsc_sap_{mySID}_{myInstAscs} SAPInstance \
	operations $id=rsc_sap_{mySID}_{myInstAscs}-operations \
	op monitor interval=120 timeout=60 on-fail=restart \
	params InstanceName={mySID}_{myInstAscs}_{myVipNAscs} \
     START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstAscs}_{myVipNAscs}" \
     AUTOMATIC_RECOVER=false \
	meta resource-stickiness=5000 failure-timeout=60 migration-threshold=1 \
       priority=10
primitive rsc_sap_{mySID}_{myInstErs} SAPInstance \
	operations $id=rsc_sap_{mySID}_{myInstErs}-operations \
	op monitor interval=120 timeout=60 on-fail=restart \
	params InstanceName={mySID}_{myInstErs}_{myVipNErs} \
    START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstErs}_{myVipNErs}" \
    AUTOMATIC_RECOVER=false IS_ERS=true \
	meta priority=1000
#
# group definitions for ASCS and ERS
#
group grp_{mySID}_{myInstAscs} rsc_ip_{mySID}_{myInstAscs} \
   rsc_r53_{mySID}_{myInstAscs} rsc_fs_{mySID}_{myInstAscs} \
   rsc_sap_{mySID}_{myInstAscs} \
	 meta resource-stickiness=3000
group grp_{mySID}_{myInstErs} rsc_ip_{mySID}_{myInstErs} \
   rsc_fs_{mySID}_{myInstErs} rsc_sap_{mySID}_{myInstErs}

#
# constraints between ASCS and ERS
#
colocation col_sap_{mySid}_not_both -5000: grp_{mySID}_{myInstErs} grp_{mySID}_{myInstAscs}
location loc_sap_{mySid}_failover_to_ers rsc_sap_{mySID}_{myInstAscs} \
	rule 2000: runs_ers_{mySID} eq 1
order ord_sap_{mySid}_first_ascs Optional: rsc_sap_{mySID}_{myInstAscs}:start rsc_sap_{mySID}_{myInstErs}:stop symmetrical=false
#
# crm properties and more
#
property cib-bootstrap-options: \
    have-watchdog=false \
    dc-version=1.1.15-21.1-e174ec8 \
    cluster-infrastructure=corosync \
    stonith-enabled=true \
    stonith-action=off \
    stonith-timeout=600s \
    last-lrm-refresh=1513844735
rsc_defaults rsc-options: \
    resource-stickiness=1 \
    migration-threshold=3
op_defaults op-options: \
    timeout=600 \
	record-pending=true

----

=== Checklist AWS Installation

Check your AWS configuration upfront and gather the following AWS items
before you start the installation:

[width="85%",options="header"]
|=========================================================
2+^|Checklist AWS installation
^| *Item* ^|*Status/Value*
2+^|**_SLES subscription and update status_**
|All systems have a SLES for SAP subscription |
|All systems have a public cloud channel |
|All system have been updated to use the latest patch level |
2+^|**_AWS User Privileges for the installing person_**
|Creation of EC2 instances and EBS volumes |
|Creation security groups |
|Creation EFS file systems |
|Modification of AWS routing tables |
|Creation policies and attach them to IAM roles |
|Optional for Route53 agent installation |
|Create and modify A-records in a private hosted zone |
|Potentially needed :Creation of subnets and routing tables |
2+^|**_VPC and Network_**
|VPC Id |
|CIDR range of VPC |
|Subnet id A for systems in first AZ |
﻿|Subnet id B for systems in second AZ |
|Routing table id for subnet A and B |
|Is this routing table associated with both subnets? |
| Alternative: Is it associated to VPC? Subnets do not have their own ones |
2+^|**_Optional: Route 53 configuration_**
|Name of private hosted Route 53 zone |
|Name of DHCP option set (Verify options!) |
|Is option set associated to VPC? |
2+^|**_AWS Policies Creation_**
|Name of data provider policy |
|Name of STONITH policy |
|Name of Move IP (Overlay IP) policy |
|Optionally: Name of Route53 policy |
2+^|**_First cluster node (ASCS and ERS)_**
|instance id |
|ENI id |
|IP address |
|host name |
|instance is associated to subnet A? |
|instance has all 3 or 4 policies attached? |
|EC2 tag _pacemaker_ set with host name? |
|AWS CLI profile _cluster_ created and set to _text_? |
|source/destination check disabled? |
2+^|**_Second cluster node (ASCS and ERS)_**
|instance id |
|ENI id |
|IP address |
|host name |
|instance is associated to subnet B? |
|instance has all 3 or 4 policies attached? |
|EC2 tag _pacemaker_ set with host name? |
|AWS CLI profile _cluster_ created and set to _text_?|
|source/destination check disabled? |
2+^|**_PAS system_**
|IP address |
|host name |
|instance is associated to subnet A or B? |
|instance has data provider policy attached? |
2+^|**_AAS system_**
|IP address |
|host name |
|instance is associated to subnet A or B |
|instance has data provider﻿ policy attached? |
2+^|**_DB system (is potentially node 1 of a database failover cluster)_**
|instance id |
|ENI id |
|IP address |
|host name |
|instance is associated to subnet A? |
|instance has data provider﻿ policy attached?
A cluster node has 2 to 3 more policies attached |
2+^|**_Overlay IP address: service ASCS_**
|IP address |
|Has it been added to routing table? |
|Does it point to the ENI of first node? |
2+^|**_Overlay IP address: service ERS_**
|IP address |
|Has it been added to routing table? |
|Does it point to the ENI of the second node? |
2+^|**_Optional: Overlay IP address DB server_**
|IP address |
|Has it been added to routing table? |
|Does it point to the ENI of the DB server? |
2+^|**_Optional: Route 53 configuration_**
|The Route 53 private hosted zone has an A record with
the name of the ASCS system
the IP address of the first cluster node |
2+^|**_Creation of EFS file system_**
| DNS name of EFS file system |
2+^|**_Internet access_**
|All instances have Internet access ? Check routing tables |
|Alternative: Add HTTP proxies for data providers and cluster software |
|=========================================================

////
//changelog since August 2018
- adding picture
- adding chapter regarding cloud-netconfig-ec2
-- May/June 2019
- adding ASE replication cluster integration
////

:leveloffset: 2
include::SAPNotes_AWS.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes

:leveloffset: 1
= Legal Notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

/////
ASCIIDOC BUILD NOTES:
You can enable it for any block by using the subs attribute to the block. The
subs attribute accepts any of the following (in a list):
/////

/////
    none - Disables substitutions
    normal - Performs all substitutions except for call-outs
    verbatim - Replaces special characters and processes call-outs
    specialchars / special characters - Replaces <, >, and & with their
    corresponding entities
=>  quotes - Applies text formatting
=>  attributes - Replaces attribute references
    replacements - Substitutes textual and character reference replacements
    macros - Processes macros
    post_replacements - Replaces the line break character (+)
/////

/////
 You must enable experimental attribute for keyboard shortcuts.
 experimental:
/////

/////
 Global Settings
:imagesdir: ./data/GITHUB/saphadoc/SAPHanaSR/doc-slesforsap/images
:iconsdir: ./icons
:stylesdir: ./styles
/////
