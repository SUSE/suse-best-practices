:docinfo:

:localdate:

// defining article ID
[#art-sap-hana-srguide-perfopt12-ali]

// Start of the document
//

= SAP HANA High Availability Cross-Zone Solution on Alibaba Cloud: With SUSE Linux Enterprise Server for SAP applications

// Jinhui Li Bernd Schubert

:toc:

////
Version Control:
|=========================================================
| **Version** | **Revision Date** | **Types Of Changes** | **Effective Date**
| 1.0 |  |  | 2018/3/7
| 1.1 | 2018/7/04 |Add corosync and cluster configuration example| 2018/7/04
| 1.2 | 2018/10/29 |Add more details regarding network, restructure stonith, correct table of network design | 2018/10/29
| 1.3 | 2019/7/24 |changes for SLES 12 SP3/4, security enhancements, fix screenshot issues  | 2019/7/24
|=========================================================
////

== Solution Overview


=== SAP HANA System Replication
SAP HANA provides a feature called *System Replication* which is available in every SAP HANA installation. It offers an inherent disaster recovery support.

For details, refer to the SAP Help Portal, HANA system replication: 

https://help.sap.com/viewer/6b94445c94ae495c83a19646e7c3fd56/2.0.03/en-US/b74e16a9e09541749a745f41246a065e.html.


=== High Availability Extension Included with SUSE Linux Enterprise Server for SAP applications
The SUSE High Availability Extension is a high availability solution based on Corosync and Pacemaker. With SUSE Linux Enterprise Server for SAP applications, SUSE provides SAP specific Resource Agents (SAPHana, SAPHanaTopology etc.) used by Pacemaker. This helps you to build your SAP HANA HA solution up more effectively.

For details, refer to the latest version of the document *SAP HANA SR Performance Optimized Scenario* at the SUSE documentation Web page:  

https://documentation.suse.com/sbp/sap-12/

=== Architecture Overview
This document guides you on how to deploy an SAP HANA High Availability (HA) solution cross different Zones. The following list contains a brief architecture overview:

. The High Availability Extension included with SUSE Linux Enterprise Server for SAP applications is used to set up the HA Cluster;

. SAP HANA System Replication is activated between the two HANA nodes;

. Two HANA nodes are located in different Zones of the same Region;

. The Alibaba Cloud Specific Virtual IP Resource Agent is used to allow Moving IP automatically switched to Active SAP HANA node; the Alibaba Cloud specific STONITH device is used for fencing.

Alibaba Cloud Architecture - Overview:

image::Alicloud_image001.png[arch,width="60%"]

Network Architecture - Overview:

image::Alicloud_network.png[network,width="60%"]

=== Network Design

The following table contains information for the Network design.

|=========================================================
|hostname|role|heartbeat IP|business IP|virtual IP
|hana0|HANA primary node|192.168.0.83|192.168.0.82 1.2+|192.168.4.1
|hana1|HANA secondary node|192.168.1.246|192.168.1.245
|HANAStudio|HANA Studio|no|192.168.0.79|no
|=========================================================

== Infrastructure Preparation

The next sections contain information about how to prepare the infrastructure.

=== Infrastructure List

To set up your infrastructure, the following components are required:

* 1 Virtual Private Cloud (VPC) network;
https://www.alibabacloud.com/product/vpc[Virtual Private Cloud]
* 2 Elastic Compute Service (ECS) instances in different zones of the same VPC;
https://www.alibabacloud.com/product/ecs[Elastic Compute Service]
* 2 Elastic Network Interfaces (ENI) - one for each ECS instance;
https://www.alibabacloud.com/help/doc-detail/58496.htm[Elastic network interfaces]
* Alibaba Cloud specific Virtual IP Resource Agent and STONITH device;
* NAT Gateway and SNAT entry;
https://www.alibabacloud.com/help/product/44413.htm[NAT Gateway]

=== Creating VPC
First, create a VPC via *Console->Virtual Private Cloud->VPCs->Create VPC*.
In this example, a VPC named *suse_hana_ha* in the Region *EU Central 1 (Frankfurt)* has been created:

image::Alicloud_VPC.png[vpc,width="46%"]

There should be at least two vSwitches (subnets) defined within the VPC network. Each vSwitch should be bound to a different Zone. In this example, the following two vSwitches (subnets) are defined:

* Switch1 192.168.0.0/24  Zone A, for SAP HANA Primary Node;
* Switch2 192.168.1.0/24  Zone B, for SAP HANA Secondary Node;

image::Alicloud_vswitch.png[vswitch,width="45%"]

=== Creating ECS Instances
Two ECS instances are created in different Zones of the same VPC via *Console->Elastic Compute Service ECS->Instances->Create Instance*. Choose the "SUSE Linux Enterprise Server for SAP applications" image from the Image Market place.

In this example, two ECS instances (hostname: _hana0_ and _hana1_) are created in the Region *EU Central 1*, Zone A and Zone B, within the VPC: *suse_hana_ha*, with SUSE Linux Enterprise Server for SAP applications 12 SP2 image from the Image Market Place. Host _hana0_ is the primary SAP HANA database node, and _hana1_ is the secondary SAP HANA database node.

image::Alicloud_instance.png[instance,width="45%"]

=== Creating ENIs and Binding to ECS Instances
Create two ENIs via *Console-> Elastic Compute Service ECS->Network and Security->ENI*, and attach one for each ECS instance, for HANA System Replication purposes. Configure the IP addresses of the ENIs to the subnet for HANA System Replication only.

In this example, the ENIs are attached to the ECS instances hana0 and hana1. In addition, the IP addresses are configured as 192.168.0.83 and 192.168.1.246 within the same vSwitches of _hana0_ and _hana1_, and put into the VPC: *suse_hana_ha*

image::Alicloud_ENI.png[ENI,width="50%"]

Meanwhile, within the Guest OS, _/etc/hosts_ should also be configured.

According to the example at hand, run the following two commands on both sites:

----
echo "192.168.0.82 hana0 hana0" >> /etc/hosts

echo "192.168.1.245 hana1 hana1" >> /etc/hosts
----

The output looks as follows:

image::Alicloud_hosts.png[hosts,width="80%"]

=== Creating NAT Gateway and configure SNAT entry
Now create an NAT Gateway attached to the given VPC. In the example at hand, an NAT Gateway named *suse_hana_ha_GW* has been created:

image::Alicloud_NAT_GW.png[NATGW,width="53%"]

After having creating the NAT Gateway, you need to create a corresponding SNAT entry to allow ECS instances within the VPC to access public addresses on the Internet.

NOTE: An Alibaba Cloud specific STONITH device and Virtual IP Resource Agent are mandatory for the cluster. They need to access Alibaba Cloud OpenAPI through a public domain.

In the example at hand, two SNAT entries have been created, for ECS instances located in a different network range:

image::Alicloud_SNAT.png[SNAT,width="50%"]

=== Creating STONITH Device and Virtual IP Resource Agent
Download the STONITH fencing software with the following command:

----
wget http://sap-automation-cn-beijing.oss-cn-beijing.aliyuncs.com/software/aliyun-pacemaker_new.zip
----

For an HA solution, a fencing device is an essential requirement. Alibaba Cloud provides its own STONITH device, which allows the servers in the HA cluster to shut down the node that is not responsive. The STONITH device leverages Alibaba Cloud OpenAPI underneath the ECS instance, which is similar to a physical reset / shutdown in an on-premise environment.

image::Alicloud_image017.png[aliyunpacemaker,width="75%"]

Extract the package and install the software.
----
tar –xvf aliyun-ecs-pacemaker.tar.gz
./install
----
image::Alicloud_image019.png[installpacemaker,width="99%"]

Install Alibaba Cloud OpenAPI SDK.
----
pip install aliyun-python-sdk-ecs aliyun-python-sdk-vpc aliyuncli
----
image::Alicloud_image021.png[SDKInstall,width="73%"]

Configure Alibaba Cloud OpenAPI SDK and Client.
----
aliyuncli configure
----
//image::Alicloud_image023.png[SDKConfigure,width="100%"]

Get your Access Key as shown below:

image::Alicloud_image025.png[AccessKey,width="45%"]

== Software Preparation
The next sections contain information about the required software.

=== Software List

The following software must be available:
- SUSE Linux Enterprise Server for SAP applications 12 SP2 or newer
- HANA Installation Media
- SAP Host Agent Installation Media

=== High Availability Extension Installation
Both ECS instances are created with the SUSE Linux Enterprise Server for SAP applications image. On both ECS instances, the High Availability Extension (with the major software components: Corosync and Pacemaker), and the package SAPHanaSR should be installed.
To do so, you can use zypper.

First, install the pattern High Availability Extension on both nodes:

----
zypper in -t pattern ha_sles
----

Now, install the Resource Agents for controlling the SAP HANA system replication on both cluster nodes:

----
zypper in SAPHanaSR SAPHanaSR-doc
----

=== SAP HANA Installation
Next, install the SAP HANA software on both ECS instances. Make sure the SAP HANA SID and Instance Number are the same (this is required by SAP HANA System Replication). It is recommended to use _hdblcm_ to do the installation. For details refer to the SAP HANA Server Installation and Update Guide at https://help.sap.com/viewer/2c1988d620e04368aa4103bf26f17727/2.0.03/en-US.

In the example at hand, both node are installed with SAP HANA (Rev. 2.00.030.00), and SID: **JL0**, Instance Number: **00**.

image::Alicloud_hanaVersion.png[HANAVersion,width="70%"]

=== SAP Host Agent Installation
When you have finished the HANA installation with _hdblcm_ as mentioned above, the SAP Host Agent should already be installed on your server. To install it manually, refer to the article Installing SAP Host Agent Manually: https://help.sap.com/saphelp_nw73ehp1/helpdata/en/8b/92b1cf6d5f4a7eac40700295ea687f/content.htm?no_cache=true.

Check the SAP Host Agent status after you have installed SAP HANA with _hdblcm_ on _hana0_ and _hana1_:

image::Alicloud_hostcontrol1.png[HostControl,width="95%"]

== Configuring SAP HANA System Replication
The following sections detail how to configure SAP HANA System Replication.

=== Backing up SAP HANA on Primary ECS Instance
To do a backup on SAP HANA, you can either use SAP HANA studio or _hdbsql_ as the client command tool.

The backup command is

- For HANA 1 single container mode:

----
BACKUP DATA USING FILE('COMPLETE_DATA_BACKUP');
----

- For HANA 2 with multitenant as default mode (You should back up systemDB and all tenantDB as shown below in the example):

----
BACKUP DATA for <DATABASE> using FILE('COMPLETE_DATA_BACKUP')
----

Command line example:

----
BACKUP DATA for SYSTEMDB using FILE('COMPLETE_DATA_BACKUP')
----

----
BACKUP DATA for JL0 using FILE('COMPLETE_DATA_BACKUP')
----

In this example for HDB Studio, the SAP HANA database backup is executed on both ECS instances as shown below:

image::Alicloud_backup.png[backup,width="55%"]

=== Configuring SAP HANA System Replication on Primary Node

Log on to the primary node with: `su - <sid>adm`.
**[sidadm]** should be replaced by your SAP HANA database SID. In our example it is `su - jl0adm`;

Stop HANA with: `HDB stop`.

Change the following file content:
----
vi /hana/shared/<SID>/global/hdb/custom/config/global.ini
----

Add the following content:
----
[system_replication_hostname_resolution]
<IP> = <HOSTNAME>
----

**[IP]** should be the address of the ENI (heartbeat IP address for SAP HANA system replication) attached to the Secondary node;

**[HOSTNAME]** should be the hostname of the Secondary node;


In the example at hand, the configuration is as follows:

----
[system_replication_hostname_resolution]

192.168.1.246 = hana1
----

=== Configuring SAP HANA System Replication on Secondary Node
Perform the same steps as outlined above for the Primary node on the Secondary node. However, do not forget to use here the IP and hostname of the Primary node instead of the Secondary node.

In the example at hand, the configuration is as follows:
----
[system_replication_hostname_resolution]

192.168.0.83 = hana0
----

=== Enable SAP HANA System Replication on Primary Node
Log on to the primary node with: `su - <sid>adm`;

Start HANA with: `HDB start`;

Enable System Replication with:

----
hdbnsutil -sr_enable --name= [primary location name]
----

**[primary location name]** should be replaced by the location of your primary SAP HANA node.

For the example at hand, the following command is used:

----
hdbnsutil -sr_enable --name=hana0
----

NOTE: All of the above operations are done on the Primary node.

=== Register the Secondary Node to the Primary SAP HANA Node
Log on to the secondary node with: `su - <sid>adm`;

Stop SAP HANA with:  `HDB stop`;

Register the Secondary SAP HANA node to the Primary SAP HANA node by running the following command:

----
hdbnsutil -sr_register --remoteHost=[location of primary Node] --remoteInstance=[instance number of primary node] --replicationMode=sync --name=[location of the secondary node] --operationMode=logreplay
----
For the example at hand, the following command is used:
----
hdbnsutil -sr_register --name=hana1 --remoteHost=hana0 --remoteInstance=00 --replicationMode=sync --operationMode=logreplay
----

Start SAP HANA with: `HDB start`;

Verify the System Replication Status with the following command:
----
hdbnsutil -sr_state
----
In the example at hand, the following status is displayed on the secondary SAP HANA node _hana1_:

image::Alicloud_replicationState.png[HSRStatus,width="120%"]

NOTE: All of the above operations are done on the Secondary node.

== Configuring High Availability Extension for SAP HANA

=== Configuration of Corosync
It is recommended that you add more redundancy for messaging (Heartbeat) by using separate ENIs attached to the ECS instances with a separate network range.

On Alibaba Cloud, it is strongly suggested to only use Unicast for the transport setting in Corosync.

Follow the steps below to configure Corosync:

* Create Keys

Run `corosync-keygen` on the Primary SAP HANA node. The generated key will be located in the file: _/etc/corosync/authkey_.

In the example at hand, the command is executed on _hana1_:

image::Alicloud_authKey1.png[authKey,width="80%"]

* Configure _/etc/corosync/corosync.conf_ as root on the Primary SAP HANA node with the following content: +
```shell
totem {
        version: 2
        token: 5000
        token_retransmits_before_loss_const: 6
        secauth: on
        crypto_hash: sha1
        crypto_cipher: aes256
        clear_node_high_bit: yes
        interface {
            ringnumber: 0
            bindnetaddr: **IP-address-for-heart-beating-for-the-current-server**
            mcastport: 5405
            ttl: 1
        }
        # On Alibaba Cloud, transport should be set to udpu, means: unicast
        transport: udpu
}
logging {
      fileline: off
      to_logfile: yes
      to_syslog: yes
      logfile: /var/log/cluster/corosync.log
      debug: off
      timestamp: on
      logger_subsys {
          subsys: QUORUM
          debug: off
      }
}
nodelist {
    node {
        ring0_addr: **ip-node-1**
        nodeid: 1
    }
    node {
        ring0_addr: **ip-node-2**
        nodeid: 2
    }
}
quorum {
    # Enable and configure quorum subsystem (default: off)
    # see also corosync.conf.5 and votequorum.5
    provider: corosync_votequorum
    expected_votes: 2
    two_node: 1
}
```

**IP-address-for-heart-beating-for-the-current-server** should be replaced by the IP address of the current server, used for messaging (heartbeat) or SAP HANA System Replication. In the example at hand, the IP address of ENI of the current node (192.168.0.83 for hana0 and 192.168.1.246 for hana1) is used.

NOTE: This value will be different on Primary and Secondary node.
The *nodelist* directive is used to list all nodes in the cluster.

**ip-node-1** and **ip-node-2** should be replaced by the IP addresses of the ENIs attached to ECS instances for Heartbeat purpose or HANA System Replication purpose. In the example at hand, it should be 192.168.0.83 for _hana0_ and 192.168.1.246 for _hana1_.

After completing the editing of _/etc/corosync/corosync.conf_ on the Primary HANA node, copy the files _/etc/corosync/authkey_ and _/etc/corosync/corosync.conf_ to _/etc/corosync_ on the Secondary SAP HANA node with the following command:
----
scp /etc/corosync/authkey root@**hostnameOfSecondaryNode**:/etc/corosync
----
In the example at hand, the following command is executed:

image::Alicloud_image069.png[scpkey1,width="80%"]

After you have copied the _corosync.conf_ to the Secondary node, configure the *bindnetaddr* as above to the local heart beating IP address.

=== Configuration of Pacemaker
For the SAP HANA High Availability solution, you need to configure seven resources and the corresponding constraints in Pacemaker.

NOTE: The following Pacemaker configuration only needs to be done on one node. It is usually done on the Primary node.

==== Cluster Bootstrap and More

Add the configuration of the bootstrap and default setting of the resource and operations to the cluster.  Save the following scripts in a file: _crm-bs.txt_.
```javascript
property $id='cib-bootstrap-options' \
              stonith-enabled="true" \
              stonith-action="off" \
              stonith-timeout="150s"
rsc_defaults $id="rsc-options" \
              resource-stickness="1000"    \
              migration-threshold="5000"
op_defaults $id="op-options" \
              timeout="600"
```
Execute the command below to add the setting to the cluster:

----
crm configure load update crm-bs.txt
----

==== STONITH Device

This part defines the Aliyun STONITH devices in the cluster.

Save the following scripts in a file for *SLES12 SP2 and older releases*: _crm-stonith.txt_.
```shell
primitive res_ALIYUN_STONITH_1 stonith:fence_aliyun \
			op monitor interval=120 timeout=60 \
        	params pcmk_host_list=<primary node hostname> port=<primary node instance id>  \
        	access_key=<access key> secret_key=<secret key> \
        	region=<region> \
        	meta target-role=Started
primitive res_ALIYUN_STONITH_2 stonith:fence_aliyun \
        	op monitor interval=120 timeout=60 \
			params pcmk_host_list=<secondary node hostname> port=<secondary node instance id> \
        	access_key=<access key> secret_key=<secret key> \
        	region=<region> \
        	meta target-role=Started
			location loc_<primary node hostname>_stonith_not_on_<primary node hostname> res_ALIYUN_STONITH_1 -inf: <primary node hostname>
			#Stonith 1 should not run on primary node because it is controlling primary node
			location loc_<secondary node hostname>_stonith_not_on_<secondary node hostname> res_ALIYUN_STONITH_2 -inf: <secondary node hostname>
			#Stonith 2 should not run on secondary node because it is controlling secondary node
```


Save the following scripts in a file for *SLES12 SP3 and newer releases*: _crm-stonith.txt_.
```shell
primitive res_ALIYUN_STONITH_1 stonith:fence_aliyun \
			op monitor interval=120 timeout=60 \
        	params plug=<primary node instance id>  \
        	access_key=<access key> secret_key=<secret key> \
        	region=<region> \
        	meta target-role=Started
primitive res_ALIYUN_STONITH_2 stonith:fence_aliyun \
        		op monitor interval=120 timeout=60 \
		params plug=<secondary node instance id> \
        	access_key=<access key> secret_key=<secret key> \
        	region=<region> \
        	meta target-role=Started
			location loc_<primary node hostname>_stonith_not_on_<primary node hostname> res_ALIYUN_STONITH_1 -inf: <primary node hostname>
			#Stonith 1 should not run on primary node because it is controlling primary node
			location loc_<secondary node hostname>_stonith_not_on_<secondary node hostname> res_ALIYUN_STONITH_2 -inf: <secondary node hostname>
			#Stonith 2 should not run on secondary node because it is controlling secondary node
```
Be sure to implement the following changes:

* **[primary node hostname]** / **[secondary node hostname]** should be replaced by the real hostname of your primary and secondary hostname, in our case is hana0 and hana1.

* **[primary node instance id]** / **[secondary node instance id]** should be replaced by the real instance-id of your ECS instance, you can get this from the Alibaba Cloud console. For SLES 12 SP3 or newer releases, please use _plug_ instead of _port_.

* **[access key]** should be replaced with real access key.

* **[secret key]** should be replaced with real secret key.

* **[region]** should be replaced with real region name where the node is located.

Execute the command below to add the resource to the cluster:

----
crm configure load update crm-stonith.txt
----

====	SAPHanaTopology

This part defines an SAPHanaTopology RA, and a clone of the SAPHanaTopology on both nodes in the cluster. Save the following scripts in a file: _crm-saphanatop.txt_.
```shell
primitive rsc_SAPHanaTopology_<SID>_HDB<instance number> ocf:suse:SAPHanaTopology \
        operations $id="rsc_SAPHanaTopology_<SID>_HDB<instance number>-operations" \
           op monitor interval="10" timeout="600"  \
           op start interval="0" timeout="600" \
           op stop interval="0" timeout="300" \
           params SID="<SID>" InstanceNumber="<instance number>"
clone cln_SAPHanaTopology_<SID>_HDB<instance number> rsc_SAPHanaTopology_<SID>_HDB<instance number> \
        meta clone-node-max="1" interleave="true"
```
Be sure to implement the following changes:

* **[SID]** should be replaced by the real SAP HANA SID.

* **[instance number]** should be replaced by the real SAP HANA Instance Number.

Execute the command below to add the resources to the cluster:

----
crm configure load update crm-saphanatop.txt
----

====	SAPHana

This part defines an SAPHana RA, and a multi-state resource of SAPHana on both nodes in the cluster. Save the following scripts in a file: _crm-saphana.txt_.

```shell
primitive rsc_SAPHana_<SID>_HDB<instance number> ocf:suse:SAPHana \
        operations $id="rsc_SAPHana_<SID>_HDB<instance number>-operations" \
        op start interval="0" timeout="3600" \
        op stop interval="0" timeout="3600" \
        op promote interval="0" timeout="3600" \
        op monitor interval="60" role="Master" timeout="700" \
        op monitor interval="61" role="Slave" timeout="700" \
        params SID="<SID>" InstanceNumber="<instance number>" PREFER_SITE_TAKEOVER="true" \
        DUPLICATE_PRIMARY_TIMEOUT="7200" AUTOMATED_REGISTER="false"
ms msl_SAPHana_<SID>_HDB<instance number> rsc_SAPHana_<SID>_HDB<instance number> \
        meta clone-max="2" clone-node-max="1" interleave="true"
```
Be sure to implement the following changes:

* **[SID]** should be replaced by the real SAP HANA SID.

* **[instance number]** should be replaced by the real SAP HANA Instance Number.

Execute the command below to add the resources to the cluster:

----
crm configure load update crm-saphana.txt
----

====	Virtual IP

This part defines a Virtual IP RA in the cluster. Save the following scripts in a file: _crm-vip.txt_.

```shell
primitive res_vip_<SID>_HDB<instance number> ocf:aliyun:vpc-move-ip \
			op monitor interval=60 \
			meta target-role=Started \
			params address=<virtual_IPv4_address> routing_table=<route_table_ID> interface=eth0
```
Be sure to implement the following changes:

* **[virtual_IP4_address]** should be replaced by the real IP address you prefer to provide the service.

* **[route_table_ID]** should be replaced by the route table ID of your VPC.

* **[SID]** should be replaced by the real SAP HANA SID.

* **[instance number]** should be replaced by the real SAP HANA Instance Number.

Execute the command below to add the resource to the cluster:

----
crm configure load update crm-vip.txt
----

====	Constraints

Two constraints are organizing the correct placement of the virtual IP address for the client database access and the start order between the two resource agents SAPHana and SAPHanaTopology. Save the following scripts in a file: _crm-constraint.txt_.

```shell
colocation col_SAPHana_vip_<SID>_HDB<instance number> 2000: rsc_vip_<SID>_HDB<instance number>:started \
        	msl_SAPHana_<SID>_HDB<instance number>:Master
order ord_SAPHana_<SID>_HDB<instance number> Optional: cln_SAPHanaTopology_<SID>_HDB<instance number> \
        	msl_SAPHana_<SID>_HDB<instance number>
```
Be sure to implement the following changes:

* **[SID]** should be replaced by the real SAP HANA SID;

* **[instance number]** should be replaced by the real SAP HANA Instance Number;

Execute the command below to add the resource to the cluster:

----
crm configure load update crm-constraint.txt
----

=== Check the Cluster Status
Start the SAP HANA High Availability Cluster on both nodes:
----
systemctl start Pacemaker
----

Monitor the SAP HANA High Availability Cluster with the following commands:
----
systemctl status pacemaker
crm_mon –r
----

In the example at hand, the result looks as follows:

image::Alicloud_clusterStatushana0.png[clusterStatushana0,width="70%"]

Meanwhile, check if a new entry **[virtual_IP4_address]** is added into the route table of the VPC.

In the example at hand, the following result is shown:

image::Alicloud_RouteTableHana0.png[RouteTableHANA0,width="45%"]

=== Verify the High Availability Takeover

Shut down the primary node.

Check the status of Pacemaker as shown below:

image::Alicloud_clusterStatushana2.png[clusterStatushana2,width="80%"]

Compare the entry of the route table in the VPC as shown below:

image::Alicloud_RouteTableHana1.png[RouteTableHANA1,width="45%"]

== Example
=== Example of Cluster Configuration

You can check the cluster configuration via the command `crm configure show`.
For the example at hand, the cluster configuration should display the following content: 

```shell
node 1: hana0 \
	attributes hana_jl0_vhost=hana0 hana_jl0_srmode=sync hana_jl0_remoteHost=hana1 hana_jl0_site=hana0 lpa_jl0_lpt=10 hana_jl0_op_mode=logreplay
node 2: hana1 \
	attributes lpa_jl0_lpt=1529509236 hana_jl0_op_mode=logreplay hana_jl0_vhost=hana1 hana_jl0_site=hana1 hana_jl0_srmode=sync hana_jl0_remoteHost=hana0
primitive res_ALIYUN_STONITH_0 stonith:fence_aliyun \
	op monitor interval=120 timeout=60 \
	params pcmk_host_list=hana0 port=i-gw8byf3m4f9a8os6rke8 access_key=<access key> secret_key=<secret key> region=eu-central-1 \
	meta target-role=Started
primitive res_ALIYUN_STONITH_1 stonith:fence_aliyun \
	op monitor interval=120 timeout=60 \
	params pcmk_host_list=hana1 port=i-gw8byf3m4f9a8os6rke9 access_key=<access key> secret_key=<secret key> region=eu-central-1 \
	meta target-role=Started
primitive rsc_SAPHanaTopology_JL0_HDB00 ocf:suse:SAPHanaTopology \
	operations $id=rsc_SAPHanaTopology_JL0_HDB00-operations \
	op monitor interval=10 timeout=600 \
	op start interval=0 timeout=600 \
	op stop interval=0 timeout=300 \
	params SID=JL0 InstanceNumber=00
primitive rsc_SAPHana_JL0_HDB00 ocf:suse:SAPHana \
	operations $id=rsc_SAPHana_JL0_HDB00-operations \
	op start interval=0 timeout=3600 \
	op stop interval=0 timeout=3600 \
	op promote interval=0 timeout=3600 \
	op monitor interval=60 role=Master timeout=700 \
	op monitor interval=61 role=Slave timeout=700 \
	params SID=JL0 InstanceNumber=00 PREFER_SITE_TAKEOVER=true DUPLICATE_PRIMARY_TIMEOUT=7200 AUTOMATED_REGISTER=false
primitive rsc_vip_JL0_HDB00 ocf:aliyun:vpc-move-ip \
	op monitor interval=60 \
	meta target-role=Started \
	params address=192.168.4.1 routing_table=vtb-gw8fii1g1d8cp14tzynub interface=eth0
ms msl_SAPHana_JL0_HDB00 rsc_SAPHana_JL0_HDB00 \
	meta clone-max=2 clone-node-max=1 interleave=true target-role=Started
clone cln_SAPHanaTopology_JL0_HDB00 rsc_SAPHanaTopology_JL0_HDB00 \
	meta clone-node-max=1 interleave=true
colocation col_SAPHana_vip_JL0_HDB00 2000: rsc_vip_JL0_HDB00:Started msl_SAPHana_JL0_HDB00:Master
location loc_hana0_stonith_not_on_hana0 res_ALIYUN_STONITH_0 -inf: hana0
location loc_hana1_stonith_not_on_hana1 res_ALIYUN_STONITH_1 -inf: hana1
order ord_SAPHana_JL0_HDB00 Optional: cln_SAPHanaTopology_JL0_HDB00 msl_SAPHana_JL0_HDB00
property cib-bootstrap-options: \
	have-watchdog=false \
	dc-version=1.1.15-21.1-e174ec8 \
	cluster-infrastructure=corosync \
	stonith-action=off \
	stonith-enabled=true \
	stonith-timeout=150s \
	last-lrm-refresh=1529503606 \
	maintenance-mode=false
rsc_defaults rsc-options: \
	resource-stickness=1000 \
	migration-threshold=5000
op_defaults op-options: \
	timeout=600
```

=== Example of /etc/corosync/corosync.conf

For the example at hand, the _corosync.conf_ on _hana1_ should display the following content:
```shell
totem{
version: 2
token: 5000
token_retransmits_before_loss_const: 6
secauth: on
crypto_hash: sha1
crypto_cipher: aes256
clear_node_high_bit: yes
interface {
ringnumber: 0
bindnetaddr: 192.168.0.83
mcastport: 5405
ttl: 1
}
# On Alibaba Cloud, transport should be set to udpu, means: unicast
transport: udpu
}
logging {
fileline: off
to_logfile: yes
to_syslog: yes
logfile: /var/log/cluster/corosync.log
debug: off
timestamp: on
logger_subsys {
subsys: QUORUM
debug: off
}
}
nodelist {
node {
ring0_addr: 192.168.0.83
nodeid: 1
}
node {
ring0_addr: 192.168.1.246
nodeid: 2
}
}
quorum {
# Enable and configure quorum subsystem (default: off)
# see also corosync.conf.5 and votequorum.5
provider: corosync_votequorum
expected_votes: 2
two_node: 1
}
```
== Reference
- Pacemaker 1.1 Configuration Explained
https://clusterlabs.org/pacemaker/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/.
- SAP HANA SR Performance Optimized Scenario
https://documentation.suse.com/sbp/sap-12/html/SLES4SAP-hana-sr-guide-PerfOpt-12/index.html
- SAP HANA system replication - SAP Help Portal
https://help.sap.com/viewer/6b94445c94ae495c83a19646e7c3fd56/2.0.03/en-US/b74e16a9e09541749a745f41246a065e.html.
- SAP applications on Alibaba Cloud: Supported Products and IaaS VM Types
https://launchpad.support.sap.com/#/notes/2552731


++++
<?pdfpagebreak?>
++++

:leveloffset: 0
// Standard SUSE Best Practices includes
== Legal notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
:leveloffset: 0
include::common_gfdl1.2_i.adoc[]
