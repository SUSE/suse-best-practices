:docinfo:
// Document Variables
:slesProdVersion: 15

= SAP NetWeaver Enqueue Replication 1 High Availability Cluster - Setup Guide for SAP NetWeaver 7.40 and 7.50
//Fabian Herschel, Bernd Schubert, Lars Pinne
//2022/06/28
//:Revision: 1.1b
//Revision {Revision} from {docdate}
// Standard SUSE includes
//include::common_copyright_gfdl.adoc[]
//:toc:

include::Variables_HA740.adoc[]

////
Feedback Ab:
TODO SAP NetWeaver 7.40 -> SAP NetWeaver 7.50 (7.50 used, both possible)
TODO Check groupNr sapinst. Ab said sapinst was already existing with Nr 1001
TODO different SWMP menue-klicks for SAP NW 7.50
TODO For SAP NW7.50 the parameter for Enqueue was already "Start_Program_xx"
TODO SLE15 has crony not ntpd
TODO New cluster-init-command with option "-u"
TODO check the cluster-init-command either with all steps or add the ssh-step to the list of steps
TODO check cluster-join-command either with all steps or add the ssh-step to the list of steps
Weitere TODOs:
TODO use the correct include files and include "places" for the common files to be compatible for the documentation team
TODO maybe the whole setup could be divided in two parts: Enqueue Replication
cluster which is the core part, and the auxiliary pieces which might come from somewhere?
DONE PRIO1: Document that autostart for SAP instances must be switched-off
REJC p.2 either adding the third node to the cluster -> (would change the basic setup)
TODO p.4 https://blogs.sap.com/2014/05/08/using-sapvendorclusterconnector-for-interaction-between-cluster-framework-and-sapstartsrv/comment-page-1/ -> (check link)
TODO p.7 (somewhere explain that the D02 and DVEBMGS01 on the DB host just to simplify this lab environment)
TODO p.7,8 (only short hostnames in /etc/hosts?)
TODO p.13 TODO/TBD ->
TODO p.14 File /usr/sap/HA1/SYS/profile/HA1_ASCS00_sapha1as. -> (font)
TODO p.14 File /usr/sap/HA1/SYS/profile/HA1_ERS10_sapha1er. -> (font)
TODO p.17 TODO/TBD ->
TODO p.17 <nul> -> refer to sle-ha quickstart guide on our webpage
TODO p.18 <nul> -> (run ha-cluter-join or do something to enable cluster?)
TODO p.18 <nul> -> some notes on adpating resulting config, depending on environment
TODO p.24 /usr/sap/HA1/SYS/exe/uc/linuxx86_64/sapcontrol -> # linuxx86_64/sapcontrol
TODO p.25 As ha1adm. -> (font)
TODO p.25,26,27,28,29 sapcontrol -nr -> # sapcontrol -nr
TODO p.31 in HA-Umgebungen -> in HA environments (links without "D"?)
TODO p.32 ... -> TODO/TBD
TODO p.32 -> SLE-HA release notes https://www.suse.com/releasenotes/x86_64/SLE-HA/12-SP2/
TODO p.32 SLE-HA quick setup guide
TODO p.32 SLE-HA product docu
TODO p.32 -> TODO SLES-for-SAP release note
TODO p.32 -> TODO product docu
TODO -> NFS SAP layout for instance
TODO: non line break with in command or variables
TODO: table of required values
////

== About this guide

The following sections focus on background information and the purpose of the document at hand.

=== Introduction

{sles4sapReg} is the optimal platform to
run {sapReg} applications with high availability (HA). Together with a redundant layout
of the technical infrastructure, single points of failure can be eliminated.

{sapBSReg} is a sophisticated application platform for large enterprises
and mid-size companies. Many critical business environments require the highest
possible {sapReg} application availability.

The described cluster solution can be used for {sapReg} S/4 HANA and for
{sapReg} {sapNW}.

{sapNw} is a common stack of middleware functionality used to support the SAP
business applications. The {sapERS} constitutes application
level redundancy for one of the most crucial components of the {sapNw} stack,
the enqueue service. An optimal effect of the enqueue replication mechanism can
be achieved when combining the application level redundancy with a high
availability cluster solution as provided by {sles4sap}. The described
concept has proven its maturity over several years of productive operations for
customers of different sizes and industries.

The here described HA setup is based on cluster-controlled file systems for the
SAP central services working directories.
This setup with cluster-controlled file system resources has been used in former
HA certifications. It still is supported.
For deploying new HA clusters, we recommend using the so-called *simple mount* setup
described in SUSE TID "Use of Filesystem resource for ASCS/ERS HA setup not possible"
(https://www.suse.com/support/kb/doc/?id=000019944).


=== Additional documentation and resources

Chapters in this manual contain links to additional documentation resources that
are either available on the system or on the Internet.

For the latest documentation updates, see https://documentation.suse.com .

Numerous whitepapers, a best practices guide, and other
resources are provided at the {sles4sap} resource
library: https://www.suse.com/products/sles-for-sap/#resource .

This guide and other SAP-specific best practices documents can be downloaded from
the documentation portal at https://documentation.suse.com/sbp/sap .

Here you can find guides for {SAPHANA} system replication
automation and HA scenarios for {SAPNw} and {s4hana}.

// Standard SUSE includes
=== Feedback
include::common_intro_feedback.adoc[]

//=== Documentation conventions
//TODO work on SUSE doc standard conventions file
//include::common_intro_typografie.adoc[]

== Scope of this document

This guide details how to:

- Plan a {sleHA} platform for {sapNw},
  including {sapERS}.
- Set up a {linux} high availability platform and perform a basic {sapNW}
  installation including {sapERS} on {sle}.
- Integrate the high availability cluster with the {sap} control framework via
  `{s4sClConnector3}`, as certified by {sap}.

This guide focuses on the high availability of the central services. 

////
HA cluster solutions for the database and {sapNW} instances are described in the best
practice "Simple Stack" available on our landing page (see section "Additional
documentation and resources"). 
////

For {saphana} system replication, follow the guides for the performance- or cost-optimized scenario.

== Overview

This guide describes how to set up a pacemaker cluster using {sles4sap}
{slesProdVersion} for the Enqueue Replication scenario. The goal is to match
the {sapCert} certification specifications and goals.

These goals include:

- Integration of the cluster with the native systemd based {SAP} start framework
  _sapstartsrv_ to ensure that maintenance procedures do not break the cluster stability
- Rolling Kernel Switch (RKS) awareness
- Standard {sap} installation to improve support processes

The updated certification {sapcert} has redefined some of the test procedures
and described new expectations how the cluster should behave in special
conditions. These changes allowed us to improve the cluster architecture and to
design it for easier usage and setup.

Shared SAP resources are on a central NFS server.

The {sap} instances themselves are installed on a shared disk to allow switching over the file
systems for proper functionality. The second need for a shared disk is that we are using the SBD
for the cluster fencing mechanism STONITH.
////
TODO PRIO2: with SAP clarify for NFS layout for instance profile
////

=== Differences to previous cluster architectures

- The concept is different to the old stack with the multi-state architecture.
With the new certification we switch to a more simple model with primitives.
This means we have on one machine the ASCS with its own resources and on
the other machine the ERS with its own resources.
- Use of native systemd integration for {sap} hostagent and instanceÂ´s sapstartsrv.
  Refer to {sap} documentation for the neccessary  product version, see also SAP note 3139184.
  SUSE systemd version 234 at least is needed. For details refer to the {sles4sap} product documentation.
  SUSE resource agents are needed at least sapstartsrv-resource-agents 0.9.1 and resource-agents 4.x from November 2021.

=== Three systems for ASCS, ERS, database and additional SAP Instances

This guide describes the installation of a distributed {sap} system on three
systems. In this setup, only two systems are in the cluster. The database and
{sap} dialog instances could also be added to the cluster by either adding the
third node to the cluster or by installing the database on either of the
nodes. However we recommend to install the database on a separate
cluster.

NOTE: The cluster in this guide only manages the {sap} instances ASCS and ERS,
because of the focus of the {sapCert} certification.

If your database is {sapHana}, we recommend to set up the performance optimized
system replication scenario using our automation solution {sapHanaSR}. The
{sapHanaSR} automation should be set up in an own two node cluster. The setup is
described in a separate best practice document available at http://documentation.suse.com/sbp/sap.

.Three systems for the certification setup
image::sles4sap_nw740_3nodes.svg[SVG]

.Clustered machines

*    One machine ({myNode1}) for ASCS
**    Host name:    {myVipNAscs}

*    One machine ({myNode2}) for ERS
**    Host name:   {myVipNErs}

.Non-Clustered machine

*    One machine ({myNode3}) for DB and DI
**    Host name:   {myVipNDb}
**    Host name:   {myVipNPas}
**    Host name:   {myVipNDSec}

=== High availability for the database

Depending on your needs you can also increase the availability of the database if your
database is not already highly available by design.

==== {SapHana} system replication

A perfect enhancement of the three node scenario described in this document is
to implement an {saphana} system replication (SR) automation.

.One cluster for central services, one for {saphana} SR
image::sles4sap_nw740_cs+hanasr.svg[SVG]

The following Databases are supported in combination with this scenario:

- SAP HANA DATABASE 1.0
- SAP HANA DATABASE 2.0

==== Simple stack

Another option is to implement a second cluster for a database without SR aka
"ANYDB". The cluster resource agent SAPDatabase uses the SAPHOSTAGENT to control
and monitor the database.

.One cluster for the central services and one cluster for the ANY database
image::sles4sap_nw740_cs+anydb.svg[SVG]

.The following OS / Databases combination are examples for this scenario
[width="85%",options="header"]
|=========================================================
2+^|{sles4sap} 15
^| *Intel X86_64* ^|*POWER LITTLE ENDIAN*
|SAP HANA DATABASE 1.0  |
|SAP HANA DATABASE 2.0  |SAP HANA DATABASE 2.0
|DB2 FOR LUW 10.5|
|MaxDB 7.9|
|ORACLE 12.1|
|SAP ASE 16.0 FOR BUS. SUITE |
|=========================================================

NOTE:The first version for {sapNW} on Power Little Endian is 7.50. More information about
supported combinations of OS and databases for {sapNW} can be found at the
SAP Product Availability Matrix. (https://apps.support.sap.com/sap/support/pam[SAP PAM])

=== Integration of {SapNW} into the cluster using the cluster connector

The integration of the HA cluster through the SAP control framework using the
`{s4sClConnector}` is of special interest. The `{SAPSTARTSRV}` controls {sap} instances since
{sap} Kernel versions 6.40. One of the classical problems running
{sap} instances in a highly available environment is the following: If an {sap}
administrator changes the status (start/stop) of an {sap} instance without using
the interfaces provided by the cluster software, the cluster framework will
detect that as an error status. It will then bring the {sap} instance into the old
status by either starting or stopping the {sap} instance. This can result in
very dangerous situations if the cluster changes the status of an {sap} instance
during some {sap} maintenance tasks. This new updated solution enables the central component
`{SAPSTARTSRV}` to report state changes to the cluster software, and therefore avoids the
previously described dangerous situations.
(See also blog article "Using sap_vendor_cluster_connector for interaction between cluster
framework and sapstartsrv")
(https://blogs.sap.com/2014/05/08/using-sapvendorclusterconnector-for-interaction-between-cluster-framework-and-sapstartsrv/comment-page-1/).

.Cluster connector to integrate the cluster with the {sap} start framework
image::sles4sap_clusterconnector.svg[SVG]

NOTE: For this scenario we are using an updated version of the `{s4sClConnector3}`.
This version implements the API version 3 for the communication between the cluster
framework and the `{sapstartsrv}`.

The new version of the `{s4sClConnector3}` now allows to start, stop and 'move'
an {sap} instance. The integration between the cluster software and the
{sapstartsrv} also implements the option to run checks of the HA setup using either the
command line tool sapcontrol or the {SAP} management consoles ({SAP} MMC or
{sap} MC).

=== Disks and partitions

For all {sap} file systems beside the file systems on NFS we are using XFS.

==== Shared disk for cluster ASCS and ERS

The disk for the ASCS and ERS instances need to be shared and assigned to the
cluster nodes {myNode1} and {myNode2}. Beside the partitions for the file systems
for the SAP instances, the disk also provides the partition to be used as SBD.

On {myNode1}, prepare the file systems for the shared disk. Create three partitions on
the shared drive {myDev}:

* partition one ({myDevPartSbd}) for SBD (7M)
* partition two ({myDevPartAscs}) for the first file system (10GB) formatted
with XFS
* partition three ({myDevPartErs}) for the second file system (10GB) formatted
with XFS

To create partitions, you can either use YaST or available command line
tools. The following script can be used for non-interactive setups.

[subs="attributes"]
----
# parted -s {myDev} print
# # we are on the 'correct' drive, right?
# parted -s {myDev} mklabel gpt
# parted -s {myDev} mkpart primary 1049k 8388k
# parted -s {myDev} mkpart primary 8389k 10.7G
# parted -s {myDev} mkpart primary 10.7G 21.5G
# mkfs.xfs {myDevPartAscs}
# mkfs.xfs {myDevPartErs}
----

For these file systems we recommend to use plain partitions to keep the cluster
configuration as simple as possible. However, you can also place these file
systems in separate volume groups. In that case, you need to add further cluster
resources to control the logical volume groups. This is out of the scope of this
setup guide.

After you have partitioned the shared disk on {myNode1}, you need to request a
partition table rescan on {myNode2}.

[subs="attributes"]
----
# partprobe; fdisk -l {myDev}
----

During the SAP installation, {myMpAscs} needs to be mounted on {myNode1} and
{myMpErs} needs to be mounted on {myNode2}.
////
TODO: non line break with in command or variables
////
- {myNode1}:   {myDevPartAscs}   {myMpAscs}
- {myNode2}:   {myDevPartErs}   {myMpErs}

==== Disk for DB and dialog instances (MaxDB example)

The disk for the database and primary application server is assigned to
{myNode3}. In an advanced setup, this disk should be shared between {myNode3}
and an optional additional node building an own cluster.

* partition one ({myDevPartSbd}) for SBD (7M) - not used here but a reservation
for an optional second cluster
* partition two ({myDevPartDb}) for the Database (60GB) formatted with XFS
* partition three ({myDevPartPas}) for the second file system (10GB) formatted
with XFS
* partition four ({myDevPartSec}) for the third file system (10GB)
formatted with XFS

To create partitions, you can either use YaST or available command line
tools. The following script can be used for non-interactive setups.

[subs="attributes"]
----
# parted -s {myDev} print
# # we are on the 'correct' drive, right?
# parted -s {myDev} mklabel gpt
# parted -s {myDev} mkpart primary 1049k 8388k
# parted -s {myDev} mkpart primary 8389k 60G
# parted -s {myDev} mkpart primary 60G 70G
# parted -s {myDev} mkpart primary 70G 80G
# mkfs.xfs {myDevPartDb}
# mkfs.xfs {myDevPartPas}
# mkfs.xfs {myDevPartSec}
----

.To be mounted either by OS or an optional cluster
- {myNode3}:   {myDevPartDb}   {myMpDb}

- {myNode3}:   {myDevPartPas}   {myMpPas74}

- {myNode3}:   {myDevPartSec}   {myMpSec}

NOTE:  {myInstPas750}: Since NetWeaver 7.5, the primary application server instance
directory has been renamed. (D<Instance_Number>)


.NFS server
- {myNfsSrv}:{myNFSExpPath}/{mySid}/sapmnt   /sapmnt

- {myNfsSrv}:{myNFSExpPath}/{mySid}/usrsapsys /usr/sap/{mySid}/SYS

.Media
- {myNfsSrv}:{myNFSExpPathSapMedia740} /sapcd

or

- {myNfsSrv}:{myNFSExpPathSapMedia750} /sapcd

=== IP addresses and virtual names

Check if _/etc/hosts_ contains at least the following address resolutions.
Add those entries, if they are missing.

[subs="attributes"]
----
{myIPNode1}  {myNode1}
{myIPNode2}  {myNode2}
{myIPNode3}  {myNode3}
{myVipAAscs}  {myVipNAscs}
{myVipAErs}  {myVipNErs}
{myVipADb}  {myVipNDb}
{myVipAPas}  {myVipNPas}
{myVipADSec}  {myVipNDSec}
----

=== Mount points and NFS shares

In our setup, the directory _/usr/sap_ is part of the root file system. You can
also create a dedicated file system for that area and mount _/usr/sap_
during the system boot. As _/usr/sap_ also contains the {sap} control file
_sapservices_ and the {saphostagent}, the directory should not be placed on a
shared file system between the cluster nodes.

Create the directory structure on all nodes which might be able to
run the SAP resource. The SYS directory will be on an NFS share for all nodes.

- Creating mount points and mounting NFS share at all nodes

.{sapNW} 7.4
==============================================
[subs="attributes"]
----
# mkdir -p /sapcd
# mkdir -p /sapmnt
# mkdir -p /usr/sap/{mySid}/{{myInstAscs},{myInstDSec},{myInstPas740},{myInstErs},SYS}
# mount -t nfs {myNfsSrv}:{myNFSExpPath}/{mySid}/sapmnt    /sapmnt
# mount -t nfs {myNfsSrv}:{myNFSExpPath}/{mySid}/usrsapsys /usr/sap/{mySid}/SYS
# mount -t nfs {myNfsSrv}:{myNFSExpPathSapMedia740} /sapcd
----
==============================================

.{sapNW} 7.5
==============================================
[subs="attributes"]
----
# mkdir -p /sapcd
# mkdir -p /sapmnt
# mkdir -p /usr/sap/{mySid}/{{myInstAscs},{myInstPas750},{myInstDSec},{myInstErs},SYS}
# mount -t nfs {myNfsSrv}:{myNFSExpPath}/{mySid}/sapmnt    /sapmnt
# mount -t nfs {myNfsSrv}:{myNFSExpPath}/{mySid}/usrsapsys /usr/sap/{mySid}/SYS
# mount -t nfs {myNfsSrv}:{myNFSExpPathSapMedia750} /sapcd
----
==============================================

- Only MaxDB:  creating mount points for the database at {myNode3}:

[subs="attributes"]
----
# mkdir -p /sapdb
----

- Only HANA: creating mount points for database at {myNode3}:

[subs="attributes"]
----
# mkdir -p /hana/{shared,data,log}
----

- Other databases: creating mount points based on there installation guide.

As you do not control the NFS shares via the cluster in this setup, you should
add these file systems to _/etc/fstab_ to get the file systems mounted during
the next system boot.

////
review Lee means this looks like a 4 node cluster, adding additional title???
////
.File system layout including NFS shares
image::sles4sap_nw750_fs.svg[SVG]

Prepare the three servers for the distributed {sap} installation. Server 1
({myNode1}) will be used to install the ASCS {sap} instance. Server 2
({myNode2}) will be used to install the ERS {sap} instance and server 3
({myNode3}) will be used to install the dialog {sap} instances and the database.

- Mounting the instance and database file systems at one specific node:

.{sapNW} 7.40 on x86_64 architecture with MaxDB
=============================================================

[subs="attributes"]
----
(ASCS   {myNode1}) # mount {myDevPartAscs} {myMpAscs}
(ERS    {myNode2}) # mount {myDevPartErs} /usr/sap/{mySid}/{myInstErs}
(DB     {myNode3}) # mount {myDevPartDb} /sapdb
(Dialog {myNode3}) # mount {myDevPartPas} /usr/sap/{mySid}/{myInstPas740}
(Dialog {myNode3}) # mount {myDevPartSec} /usr/sap/{mySid}/{myInstDSec}
----
=============================================================

.{sapNW} 7.50 on PowerLE architecture with HANA
=============================================================

[subs="attributes"]
----
(ASCS   {myNode1}) # mount {myDevPartAscs} {myMpAscs}
(ERS    {myNode2}) # mount {myDevPartErs} /usr/sap/{mySid}/{myInstErs}
(DB     {myNode3}) # mount {bsDevPartDbS} /hana/shared
(DB     {myNode3}) # mount {bsDevPartDbL} /hana/log
(DB     {myNode3}) # mount {bsDevPartDbD} /hana/data
(Dialog {myNode3}) # mount {myDevPartPas} /usr/sap/{mySid}/{myInstPas750}
(Dialog {myNode3}) # mount {myDevPartSec} /usr/sap/{mySid}/{myInstDSec}
----
=============================================================

- As a result, the directory _/usr/sap/{mySid}/_ should now look like:

[subs="attributes"]
----
# ls -la /usr/sap/{mySid}/
total 0
drwxr-xr-x 1 {mySidLc}adm sapsys 70 28. Mar 17:26 ./
drwxr-xr-x 1 root   sapsys 58 28. Mar 16:49 ../
drwxr-xr-x 7 {mySidLc}adm sapsys 58 28. Mar 16:49 {myInstAscs}/
drwxr-xr-x 1 {mySidLc}adm sapsys  0 28. Mar 15:59 {myInstDSec}/
drwxr-xr-x 1 {mySidLc}adm sapsys  0 28. Mar 15:59 {myInstPas750}/
drwxr-xr-x 1 {mySidLc}adm sapsys  0 28. Mar 15:59 {myInstErs}/
drwxr-xr-x 5 {mySidLc}adm sapsys 87 28. Mar 17:21 SYS/
----

NOTE: The owner of the directory and files is changed during the {sap} installation. By default all
of them are owned by root.

== SAP installation

The overall procedure to install the distributed SAP is:

- Installing the ASCS instance for the central services
- Installing the ERS to get a replicated enqueue scenario
- preparing the ASCS and ERS installations for the cluster take-over
- Installing the Database
- Installing the primary application server instance (PAS)
- Installing additional application server instances (AAS)

The result will be a distributed {sap} installation as illustrated here:

.Distributed installation of the {sap} system
image::sles4sap_nw740_distInstall.svg[SVG]

=== Linux user and group number scheme

Whenever asked by the SAP software provisioning manager (SWPM) which Linux User
IDs or Group IDs to use, refer to the following table which is only an example.

[subs="attributes"]
----
Group sapinst      1000
Group sapsys       1001
Group sapadm       3000
Group sdba         3002

User  {mysapadm}       3000
User  sdb          3002
User  sqd{mySidLc}       3003
User  sapadm       3004
User  {bsDBadm}       4001
----


=== Installing ASCS on {myNode1}

Temporarily, you need to set the service IP address used later in the
cluster as local IP, because the installer wants to resolve or use it.
Make sure to use the right virtual host name for each installation step.
Take care for file systems like _{myDevPartAscs}_ and _/sapcd/_ which might also need
to be mounted.

[subs="attributes"]
----
# ip a a {myVipAAscs}{myVipNM} dev eth0
# mount {myDevPartAscs} {myMpAscs}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNAscs}
----

* SWPM option depends on {sapNW} version and architecture
** Installing {sapNW} 7.40 SR2 -> MaxDB -> SAP-Systems ->
Application Server ABAP -> High-Availability System -> ASCS Instance
** Installing {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> ASCS Instance
* SID id {mySid}
* Use instance number {myAscsIno}
* Deselect using FQDN
* All passwords: use {mySapPwd}
* Double-check during the parameter review if virtual name *{myVipNAscs}* is used

=== Installing ERS on {myNode2}

Temporarily, you need to set the service IP address used later in the
cluster as local IP because the installer wants to resolve or use it.
Make sure to use the right virtual host name for each installation step.

[subs="attributes"]
----
# ip a a {myVipAErs}{myVipNM} dev eth0
# mount {myDevPartErs} {myMpErs}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNErs}
----

* SWPM option depends on {sapNW} version and architecture
** Installing {sapNW} 7.40 SR2 -> MaxDB -> SAP-Systems ->
Application Server ABAP -> High-Availability System -> Enqueue Replication
Server Instance
** Installing {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> Enqueue Replication
Server Instance
* Use instance number {myErsIno}
* Deselect using FQDN
* Double-check during the parameter review if virtual name *{myVipNErs}* is used
* If you get an error during the installation about permissions, change the
  ownership of the ERS directory

[subs="attributes"]
----
# chown -R {mysapadm}:sapsys /usr/sap/{mySid}/{myInstErs}
----

* If you get a prompt to manually stop/start the ASCS instance, log in at
{mynode1} as user {mysapadm} and call sapcontrol.

[subs="attributes"]
----
# sapcontrol -nr {myAscsIno} -function Stop    # to stop the ASCS
# sapcontrol -nr {myAscsIno} -function Start   # to start the ASCS
----

=== Poststeps for ASCS and ERS

==== Stopping ASCS and ERS

_On {myNode1}_

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myAscsIno} -function Stop
# sapcontrol -nr {myAscsIno} -function StopService
----

_On {myNode2}_

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myErsIno} -function Stop
# sapcontrol -nr {myErsIno} -function StopService
----


==== Disabling `systemd` services of the ASCS and the ERS {sap} instance

This is mandatory for giving control over the instance to the HA cluster.
See also manual pages ocf_suse_SAPStartSrv(7) and SAPStartSrv_basic_Cluster(7).

[subs="specialchars,attributes"]
----
# systemctl disable SAP{mySid}_{myAscsIno}.service
# systemctl stop SAP{mySid}_{myAscsIno}.service
# systemctl disable SAP{mySid}_{myErsIno}.service
# systemctl stop SAP{mySid}_{myErsIno}.service
----
NOTE: Stopping these instance services will stop the SAP instance as well.
Starting the instance services will not start the SAP instances.

- Check the {sap} systemd integration:

[subs="specialchars,attributes"]
----
# systemctl list-unit-files | grep SAP
SAP{mySid}_{myErsIno}.service disabled
SAP{mySid}_{myErsIno}.service disabled
----
The instance services are now disabled as required.

[subs="specialchars,attributes"]
----
# systemctl list-unit-files | grep sap
saphostagent.service enabled
sapinit.service generated
saprouter.service disabled
saptune.service enabled
----
The mandatory `saphostagent` service is enabled. This is the installation default.
Some more {sap} related services might be enabled, for example the recommended `saptune`.

[subs="specialchars,attributes"]
----
# cat /usr/sap/sapservices
systemctl --no-ask-password start SAP{mySid}_{myAscsIno} # sapstartsrv pf=/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs}
systemctl --no-ask-password start SAP{mySid}_{myErsIno} # sapstartsrv pf=/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstErs}_{myVipNErs}
----
The _sapservices_ file is still there for compatibility. It shows native `systemd` commands, one per
line for each registered instance.
You will find a SystemV style example in the appendix.
// TODO PRIO3: link to example



////
TODO PRIO1: replace this with native systemd integration

==== Maintaining _sapservices_

Ensure _/usr/sap/sapservices_ hold both entries (ASCS+ERS) on both cluster
nodes. This allows the `{sapstartsrv}` clients to start the service alike
(do not execute this at this point in time).

_As user {mySapAdm}_

[subs="attributes"]
----
# sapcontrol -nr {myErsIno} -function StartService {mySid}
----
The _/usr/sap/sapservices_ looks like (typically one line per instance):

[subs="attributes"]
----
#!/bin/sh
LD_LIBRARY_PATH=/usr/sap/{mySid}/{myInstAscs}/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/{mySid}/{myInstAscs}/exe/sapstartsrv pf=/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs} -D -u {mySapAdm}
LD_LIBRARY_PATH=/usr/sap/{mySid}/{myInstErs}/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/{mySid}/{myInstErs}/exe/sapstartsrv pf=/usr/sap/{mySid}/{myInstErs}/profile/{mySid}_{myInstErs}_{myVipNErs} -D -u {mySapAdm}
----
////


==== Integrating the cluster framework using {s4sClConnector3}

Install the package *{s4sClConnector3}* version 3.1.0 from our
repositories:

[subs="attributes"]
----
# zypper in {s4sClConnector3}
----

NOTE: Be careful as there are two packages available. The package `{s4sClConnector}`
continues to contain the old version 1.1.0 (SAP API 1). The package
`{s4sClConnector3}` contains the new version 3.1.x (SAP API 3).
The package `{s4sClConnector3}` with version 3.1.x implements the SUSE SAP API
version 3. New features like SAP Rolling Kernel Switch (RKS) and the move of ASCS are
only supported with this new version.

For the ERS and ASCS instance edit the instance profiles
{mySid}_{myInstAscs}_{myVipNAscs} and {mySid}_{myInstErs}_{myVipNErs} in the
profile directory _/usr/sap/{mySid}/SYS/profile/_.

You need to tell `{sapStartSrv}` to load the HA script connector library and
to use the `{s4sClConnector3}`. On the other hand, please make sure the feature
_Autostart_ is *not* used.

[subs="attributes"]
----
service/halib = $(DIR_CT_RUN)/saphascriptco.so
service/halib_cluster_connector = /usr/bin/sap_suse_cluster_connector
----

Add the user {mySapAdm} to the unix user group haclient.

[subs="attributes"]
----
# usermod -aG haclient {mySapAdm}
----

==== Adapting {sap} profiles to match the {sapCert} certification

For the ASCS, change the start command from _Restart_Programm_xx_ to
_Start_Programm_xx_ for the enqueue server (enserver). This change tells the
{sap} start framework *not* to self-restart the enqueue process. Such a restart
would lead in loss of the locks.

.File /usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs}

[subs="attributes"]
----
Start_Program_01 = local $(_EN) pf=$(_PF)
----

Optionally you can limit the number of restarts of services (in the case of
ASCS this limits the restart of the message server).

For the ERS, change the start command from _Restart_Programm_xx_ to
_Start_Programm_xx_ for the enqueue replication server (enrepserver).

.File /usr/sap/{mySid}/SYS/profile/{mySid}_{myInstErs}_{myVipNErs}

[subs="attributes"]
----
Start_Program_00 = local $(_ER) pf=$(_PFL) NR=$(SCSID)
----

==== Starting ASCS and ERS

_On {myNode1}_

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myAscsIno} -function StartService {mySid}
# sapcontrol -nr {myAscsIno} -function Start
----

_On {myNode2}_

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myErsIno} -function StartService {mySid}
# sapcontrol -nr {myErsIno} -function Start
----

=== Installing DB on {myNode3} (example MaxDB)

The MaxDB needs minimum 40 GB. Use {myDevPartDb} and mount the partition to
_/sapdb_.

[subs="attributes"]
----
# ip a a {myVipADb}{myVipNM} dev eth0
# mount {myDevPartDb} {myMPDb}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNDb}
----

* Install SAP NetWeaver 7.40 SR2 -> MaxDB -> SAP-Systems ->
  Application Server ABAP -> High Availability System -> DB
* Profile directory is _/sapmnt/{mySid}/profile_
* DB ID is {mySid}
* Volume Media Type *keep* File (not raw)
* Deselect using FQDN
* Double-check during the parameter review if virtual name *{myVipNDb}* is used

=== Installing DB on {myNode3} (example SAP HANA)

The HANA DB has very strict HW requirements. The storage sizing depends on many
indicators. Check the supported configurations at
https://support.sap.com/en/release-upgrade-maintenance.html#section_1969201630[SAP HANA Hardware Directory]
and https://www.sap.com/documents/2016/05/e8705aae-717c-0010-82c7-eda71af511fa.html[SAP HANA TDI].

[subs="attributes"]
----
# ip a a {myVipADb}{myVipNM} dev eth0
# mount {bsDevPartDbS} {bsMPDb}/shared
# mount {bsDevPartDbL} {bsMPDb}/log
# mount {bsDevPartDbD} {bsMPDb}/data
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNDb}
----

* Install {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> Database Instance
* Profile directory is _/sapmnt/{mySid}/profile_
* Deselect using FQDN
* Database parameters: enter DBSID is {bsSidDB}; Database Host is {myVipNDb};
Instance Number is {bsDBIno}
* Database System ID: enter Instance Number is {bsDBIno}; SAP Mount Directory is
{bsMPDb}/shared
* Account parameters: change them in case custom values are needed
* Clean up: select *Yes*, remove operating system users from group'sapinst'....
* Double-check during the parameter review if virtual name *{myVipNDb}* is
  used

=== Installing the primary application server (PAS) on {myNode3}

[subs="attributes"]
----
# ip a a {myVipAPas}{myVipNM} dev eth0
# mount {myDevPartPas} {myMPPas74}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNPas}
----

or alternatively:

[subs="attributes"]
----
# ip a a {myVipAPas}{myVipNM} dev eth0
# mount {myDevPartPas} {myMPPas75}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNPas}
----

* SWPM option depends on {sapNW} version and architecture
** Installing SAP NetWeaver 7.40 SR2 -> MaxDB -> SAP-Systems ->
Application Server ABAP -> High-Availability System -> Primary Application Server Instance
(PAS)
** Installing {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> Primary Application Server Instance
(PAS)
* Use instance number {myPasIno}
* Deselect using FQDN
* For our hands-on setup use a default secure store key
* Do not install Diagnostic Agent
* No SLD
* Double-check during the parameter review if virtual name *{myVipNPas}* is used

=== Installing an additional application server (AAS) on {myNode3}

[subs="attributes"]
----
# ip a a {myVipADSec}{myVipNM} dev eth0
# mount {myDevPartSec} {myMPSec}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNDSec}
----

* SWPM option depends on {sapNW} version and architecture
** Installing SAP NetWeaver 7.40 SR2 -> MaxDB -> SAP-Systems ->
Application Server ABAP -> High-Availability System -> Additional Application Server
Instance (AAS)
** Installing {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> Additional Application Server
Instance (AAS)
* Use instance number {myDSecIno}
* Deselect using FQDN
* Do not install Diagnostic Agent
* Double-check during the parameter review if virtual name *{myVipNDSec}* is used

== Implementing the cluster

The main procedure to implement the cluster is as follows:

* Install the cluster software if not already done during the installation of
the operating system
* Configure the cluster communication framework corosync.
* Configure the cluster resource manager.
* Configure the cluster resources.
* Tune the cluster timing in special for the SBD.

///////////////////////////////
TODO: Do we really need to stop, unconfigure and unmount?
Maybe we find a way to configure the resources that the cluster just
accepts the already started resource groups - lets see ;-)
///////////////////////////////

NOTE: Before you continue to set up the cluster, first stop all SAP instances, remove
the (manually added) IP addresses on the cluster nodes and unmount the file systems
which will be controlled by the cluster later.

NOTE: The SBD device/partition needs to be created beforehand. In this setup
guide, partition _{myDevPartSbd}_ is already reserved for SBD usage.

.Tasks

. Setup Chrony (best with YaST) and enable it

. Install pattern ha_sles on both cluster nodes

[subs="attributes"]
----
# zypper in -t pattern ha_sles
----

=== Configuring the cluster base

.Tasks

- Install and configure the cluster stack at the first machine

You can use either YaST or the interactive
command line tool `ha-cluster-init` to configure the cluster base. The following script can be used for
automated setups.

[subs="attributes"]
----
# modprobe softdog
# echo "softdog" > /etc/modules-load.d/softdog.conf
# systemctl enable sbd
# ha-cluster-init -y -i eth0 -u -s {myDevPartSbd}
----

Keep in mind that a hardware watchdog is preferred instead of the softdog method.


- Join the second node

Find below some preparation steps on the second node.

[subs="attributes"]
----
# modprobe softdog
# echo "softdog" > /etc/modules-load.d/softdog.conf
# systemctl enable sbd
# rsync {myIpNode1}:/etc/sysconfig/sbd /etc/sysconfig
----

You can use either YaST to configure the cluster base or the interactive
command line tool `ha-cluster-join`. The following script can be used for
automated setups.


[subs="attributes"]
----
# ha-cluster-join -y -c {myIPNode1} -i {myHaNetIf}
----

- The _crm_mon -1r_ output should look like this:

[subs="attributes"]
----
Stack: corosync
Current DC: {myNode1} (version 1.1.18+20180430.b12c320f5-1.14-b12c320f5) - partition with quorum
Last updated: Wed Apr  3 13:53:40 2019
Last change: Wed Apr  3 13:44:40 2019 by root via cibadmin on {myNode1}

2 nodes configured
1 resource configured

Online: [ {myNode1} {myNode2} ]

Full list of resources:

 stonith-sbd    (stonith:external/sbd): Started {myNode1}
----

- After both nodes are listed in the overview, verify the property setting of the basic cluster configuration. 
Very important here is the setting: *record-pending=true*. 

[subs="attributes"]
----
# crm configure show
...
property cib-bootstrap-options: \
        have-watchdog=true \
        dc-version=1.1.18+20180430.b12c320f5-1.14-b12c320f5 \
        cluster-infrastructure=corosync \
        cluster-name=hacluster \
        stonith-enabled=true \
        last-lrm-refresh=1494346532
rsc_defaults rsc-options: \
        resource-stickiness=1 \
        migration-threshold=3
op_defaults op-options: \
        timeout=600 \
        record-pending=true

----

=== Configuring cluster resources

You need a changed SAPInstance resource agent for {sapNw} to *not* use
the multi-state construct but to move to a more cluster-like construct, to
start and stop the ASCS and the ERS itself and *not* the complete
multi-state construct.

For this there is a new functionality for the ASCS needed to follow the ERS.
The ASCS needs to mount the shared memory table of the ERS to avoid the loss of
locks.

.Resources and constraints
image::sles4sap_nw740_resources.svg[SVG]

The implementation is done using the new flag "runs_ers_$SID" within
the RA, enabled using the resource parameter "IS_ERS=TRUE".

Another benefit of this concept is that you can now work with local (mountable)
file systems instead of a shared (NFS) file system for the {sap} instance
directories.

==== Preparing the cluster for adding the resources

To avoid that the cluster starts partially defined resources, set the cluster
to the maintenance mode. This deactivates all monitor actions.

_As user root_

[subs="attributes"]
----
# crm configure property maintenance-mode="true"
----

==== Configuring the resources for the ASCS

First, configure the resources for the file system, IP address and the {sap}
instance. Make sure you adapt the parameters to your environment.

.ASCS primitive
================================================
[subs="attributes"]
----
primitive rsc_fs_{mySID}_{myInstAscs} Filesystem \
  params device="{myDevPartAscs}" directory="/usr/sap/{mySID}/{myInstAscs}" \
   fstype=xfs \
  op start timeout=60s interval=0 \
  op stop timeout=60s interval=0 \
  op monitor interval=20s timeout=40s
primitive rsc_ip_{mySID}_{myInstAscs} IPaddr2 \
  params ip={myVipAAscs} \
  op monitor interval=10s timeout=20s
primitive rsc_sap_{mySID}_{myInstAscs} SAPInstance \
  op monitor interval=11 timeout=60 on-fail=restart \
  params InstanceName={mySID}_{myInstAscs}_{myVipNAscs} \
   START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstAscs}_{myVipNAscs}" \
   AUTOMATIC_RECOVER=false \
  meta resource-stickiness=5000 failure-timeout=60 \
   migration-threshold=1 priority=10
----
================================================

.ASCS group
================================================
[subs="attributes"]
----
group grp_{mySID}_{myInstAscs} \
  rsc_ip_{mySID}_{myInstAscs} rsc_fs_{mySID}_{myInstAscs} rsc_sap_{mySID}_{myInstAscs} \
	meta resource-stickiness=3000
----
================================================

Create a txt file (like _crm_ascs.txt_) with your preferred text editor, enter
both examples (primitives and group) to that file and load the configuration to
the cluster manager configuration.

_As user root_

[subs="attributes"]
----
# crm configure load update crm_ascs.txt
----

==== Configuring the resources for the ERS

Second, configure the resources for the file system, IP address and the {sap}
instance. Make sure you adapt the parameters to your environment.

The specific parameter _IS_ERS=true_ should only be set for the ERS instance.

.ERS primitive
================================================
[subs="attributes"]
----
primitive rsc_fs_{mySID}_{myInstErs} Filesystem \
  params device="{myDevPartErs}" directory="/usr/sap/{mySID}/{myInstErs}" fstype=xfs \
  op start timeout=60s interval=0 \
  op stop timeout=60s interval=0 \
  op monitor interval=20s timeout=40s
primitive rsc_ip_{mySID}_{myInstErs} IPaddr2 \
  params ip={myVipAErs} \
  op monitor interval=10s timeout=20s
primitive rsc_sap_{mySID}_{myInstErs} SAPInstance \
  op monitor interval=11 timeout=60 on-fail=restart \
  params InstanceName={mySID}_{myInstErs}_{myVipNErs} \
   START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstErs}_{myVipNErs}" \
   AUTOMATIC_RECOVER=false IS_ERS=true \
  meta priority=1000
----
================================================

.ERS group
================================================
[subs="attributes"]
----
group grp_{mySID}_{myInstErs} \
  rsc_ip_{mySID}_{myInstErs} rsc_fs_{mySID}_{myInstErs} rsc_sap_{mySID}_{myInstErs}
----
================================================

Create a txt file (like _crm_ers.txt_) with your preferred text editor, enter
both examples (primitives and group) to that file and load the configuration to
the cluster manager configuration.

_As user root_

[subs="attributes"]
----
# crm configure load update crm_ers.txt
----

==== Configuring the colocation constraints between ASCS and ERS

The constraints between the ASCS and ERS instance are needed to define that the
ASCS instance starts exactly on the cluster node running the ERS
instance after a failure (loc_sap_{mysid}_failover_to_ers). This constraint is
needed to ensure that the locks are not lost after an ASCS instance (or node)
failure.

If the ASCS instance has been started by the cluster, the ERS instance should
be moved to an "other" cluster node (col_sap_{mysid}_no_both). This constraint
is needed to ensure that the ERS will synchronize the locks again and the cluster is
ready for an additional take-over.

.Location constraint
================================================
[subs="attributes"]
----
colocation col_sap_{mysid}_no_both -5000: grp_{mySID}_{myInstErs} grp_{mySID}_{myInstAscs}
location loc_sap_{mysid}_failover_to_ers rsc_sap_{mySID}_{myInstAscs} \
  rule 2000: runs_ers_{mySID} eq 1
order ord_sap_{mysid}_first_start_ascs Optional: rsc_sap_{mySID}_{myInstAscs}:start \
  rsc_sap_{mySID}_{myInstErs}:stop symmetrical=false
----
================================================

Create a txt file (like _crm_col.txt_) with your preferred text editor, enter
all three constraints to that file and load the configuration to the
cluster manager configuration.

_As user root_

[subs="attributes"]
----
# crm configure load update crm_col.txt
----

==== Activating the cluster

The last step is to end the cluster maintenance mode and to allow the
cluster to detect already running resources.

_As user root_

[subs="attributes"]
----
# crm configure property maintenance-mode="false"
----


////
############## REMOVING THIS FROM THE OUTPUT FOR NOW ###########

=== Installing SAP licenses

Most likely you have your own established method to maintain your {sap} licenses.
This section is only a reminder. You should have installed two license keys as
you need to failover the ASCS {sap} instance.

- Get the HWKEY of both cluster nodes.
- Get the license from the SAP launchpad.
- Install the two licenses per HWKEY using transaction SLICENSE.

###################
////

////
############## REMOVING THIS FROM THE OUTPUT FOR NOW ###########

=== Optional - Installing the HA test tool HATool

This is an optional task. If you like to check your cluster using {sap}s HATool,
you might find these notes helpful. Always follow {SAP}s documentation
how to install and use the HATool.

Install the test client at {myNode3}. The server part is imported
as an SAP Transport. In our environment, the {sap} dialog instance is also running
at {myNode3}, so also store the transport files there.

- Installing the HATool Client (as root)

[subs="attributes"]
----
# cd /root/herschel
# unzip HATool_v213.zip
(creates directories HATool/{bin,check,config,doc,event,server)
----

- Installing other SW parts of the HATool Client (as root).

See also sections 3.2 and 4.2 of the "High Availability Test Tool - Installation
& Operations Guide"

[subs="attributes"]
----
# cd /root/herschel
##
## SAP JVM8 - we skipped that part, because we have Oracle VM8
##
#######
##
## SAP JCO 3.0
##
# mkdir SAPJCO; cd SAPJCO
# Download from Marketplace
# unzip mkdir SAPJCOmkdir SAPJCO
# tar -xvzf sapjco3-linuxx86_64-3.0.16.tgz
# unzip sapjco30P16HF_1-10009485.zipsapjco30P16HF_1-10009485.zip
# ln -s $PWD/sapjco3.jar /root/herschel/HATool/bin
# ln -s $PWD/libsapjco3.so /root/herschel/HATool/bin
##
## d3.min.js - graphic library
##
# mkdir d3.min.js
# wget https://d3js.org/d3.v3.min.js
# mv d3.v3.min.js d3.min.js/d3.min.js
# ln -s /root/herschel/d3.min.js/d3.v3.min.js \
      /root/herschel/HATool/bin/d3.min.js
----

- Importing the HATool server part into the SAP system
This is described in section 4.3 of the "High Availability Test Tool -
Installation & Operations Guide".

- Creating and configuring user hatool
This is described in section 4.3 of the "High Availability Test Tool -
Installation & Operations Guide".

- Setting up the first property file for a smoke test
In the directory _/root/herschel/HATool/config_, change the file
haQuickStartTemplate.properties like the following patch does. This means set the values for
systemclient, user, password, messagehost and SID.

[subs="attributes"]
----
 --- haQuickStartTemplate.properties     2015-10-14 14:50:48.000000000 +0200
 +++ FHhaQuickStartTemplate.properties   2017-04-24 20:43:41.117512210 +0200
 @@ -53,20 +53,20 @@ resettestdata = 1
  ###################################
  ########## login information for server user ###########
 -systemclient = nnn
 -user = xxxxxxxxxx
 -password = xxxxxxxxx
 +systemclient = 001
 +user = hatool
 +password = {mySapPwd}
  language = en
  ################# RFC ###################
  # login via message server
  -msgserverhost = xxxxxxxxxx
  +msgserverhost = {myVipNAscs}
  # the port information is necessary, if there is no entry for sapms systemid in /etc/services resp. C:\Windows\System32\drivers\etc\services
  #msgserverport = 3600
  # for the logon group, PUBLIC is used by default; if no explicit group was created, use SPACE
  logongroup = SPACE
  # if msgserverport is not set, systemid is mandatory
 -systemid = SID
 +systemid = {mySid}
 ####################
 #   One HA Event   #
----

- Running the test
[subs="attributes"]
 java -cp haTestTool.jar:sapjco3.jar com.sap.test.haload.ClientDriver \
     file=FHhaQuickStartTemplate.properties

- Adapting the test properties
This is described in section 3.2 of the "SAP Application Server HA Interface
Certification" guide.

** Login.properties - set systemclient, user, password, messagehost and SID
** TEC05Event.properties - set eventcall to TEC05Event.sh
** TEC14Event.properties - set eventcall to TEC14Event.sh
** TEC05Event.sh - edit the _sapcontrol_ command

[subs="attributes"]
----
 /usr/sap/{mySid}/SYS/exe/uc/linuxx86_64/sapcontrol -prot NI_HTTP \
   -user {mySapAdm} {mySapPwd} \
   -host {myVipNAscs} -nr {myAscsIno} -function HAFailoverToNode ""
----
** TEC14Event.sh - edit the sapctrl command

[subs="attributes"]
----
 /usr/sap/{mySid}/SYS/exe/uc/linuxx86_64/sapcontrol -prot NI_HTTP \
   -user {mySapAdm} {mySapPwd} \
   -host {myVipNAscs} -nr {myAscsIno} -function UpdateSystem 120 300 1
----

NOTE: We might need to change the _rks-host_ (here {myVipNAscs}) after we get
feedback from SAP which instance should be referenced.

##############
////

////
:leveloffset: 2
include::MaxdbStudio.txt[]

include::MaxdbResizeDB.txt[]

:leveloffset: 0
////

////
############################
#
# ADMINISTRATION
#
############################
////

== Administration

=== Dos and don'ts

==== Never stop the ASCS instance

For normal operation, *do not stop* the ASCS {sap} instance with any tool such
as cluster tools or {sap} tools. The stop of the ASCS instance might lead to a loss of enqueue
locks. Because following the new {sapCert} certification the cluster must allow local restarts
of the ASCS. This feature is needed to allow rolling kernel switch (RKS) updates without
reconfiguring the cluster.

WARNING: Stopping the ASCS instance might lead to the loss of {sap} enqueue
  locks during the start of the ASCS on the same node.

==== How to move ASCS

To *move* the ASCS {sap} instance, you should use the {sap} tools such as
the {sap} management console. This will trigger {sapStartSrv} to use
`{s4sClConnector3}` to move the ASCS instance. As user _{mysapadm}_ you might call
the following command to move away the ASCS. The move-away will always
move the ASCS to the ERS side which will keep the {sap} enqueue locks.

_As {mysapadm}_

[subs="attributes"]
----
# sapcontrol -nr 00 -function HAFailoverToNode ""
----

==== Never block resources

With {sapCert} it is *not longer allowed to block resources* from being
  controlled manually. This using the variable _BLOCK_RESOURCES_ in
  _/etc/sysconfig/sap_suse_cluster_connector_ is not allowed anymore.

==== Always use unique instance numbers

Currently all {sap} *instance numbers controlled by the cluster must be unique*.
  If you need to have multiple dialog instances such as D00 running on different
  systems they should be not controlled by the cluster.

==== How to set cluster into maintenance mode

The procedure to set the cluster into maintenance mode can be done as _root_ or _sidadm_.

_As user root_

[subs="attributes"]
----
# crm configure property maintenance-mode="true"
----

_As user {mysapadm} (the full path is needed)_

[subs="attributes"]
----
# /usr/sbin/crm configure property maintenance-mode="true"
----

==== Procedure to end the cluster maintenance

_As user root_

[subs="attributes"]
----
# crm configure property maintenance-mode="false"
----

==== Cleaning up resources

Next is how to *clean up resource failures*. Failures of the ASCS will automatically be
  deleted to allow a failback after the configured period of time. For all other
  resources you can clean up the status including the failures:

_As user root_

[subs="attributes"]
----
# crm resource refresh RESOURCE-NAME
----

WARNING: You should not clean up the complete group of the ASCS resource as this
   might lead into an unwanted cluster action to take-over the complete group to
   the node where ERS instance is running.

=== Testing the cluster

We strongly recommend that you at least process the following tests before you
plan going into production with your cluster:

==== Checking product names with _HAGetFailoverConfig_

Check if the name of the SUSE cluster solution is shown in the output of
  sapcontrol or {sap} management console. This test checks the status of the
  {sapNW} cluster integration.

_As user {mysapadm}_

[subs="attributes"]
----
# sapcontrol -nr 00 -function HAGetFailoverConfig
----

==== Starting SAP checks using _HACheckConfig_ and _HACheckFailoverConfig_

Check if the HA configuration tests are showing no errors.

_As user {mysapadm}_

[subs="attributes"]
----
# sapcontrol -nr 00 -function HACheckConfig
# sapcontrol -nr 00 -function HACheckFailoverConfig
----

==== Manually moving ASCS

Check if manually moving the ASCS using HA tools works properly.

_As user root_

[subs="attributes"]
----
# crm resource move rsc_sap_{mySid}_{myInstAscs} force
## wait until the ASCS is been moved to the ERS host
# crm resource clear rsc_sap_{mySid}_{myInstAscs}
----

==== Migrating ASCS using _HAFailoverToNode_

Check if moving the ASCS instance using {sap} tools like {sapCtrl}  works properly

_As user {mysapadm}_

[subs="attributes"]
----
# sapcontrol -nr 00 -function HAFailoverToNode ""
----

==== Testing ASCS move after failure

Check if the ASCS instance moves correctly after a node failure.

_As user root_

[subs="attributes"]
----
## on the ASCS host
# echo b >/proc/sysrq-trigger
----

==== Inplacing restart of ASCS using stop and start

Check if the inplace restart of the {sap} resources have been processed
  correctly. The {sap} instance should not failover to an other node, it
  must start on the same node where it has been stopped.

WARNING: This test will force the SAP system to *lose* the enqueue locks.
   *This test should not be processed during production.*

_As user {mysapadm}_

[subs="attributes"]
----
## example for ASCS
# sapcontrol -nr 00 -function Stop
## wait until the ASCS is completely down
# sapcontrol -nr 00 -function Start
----

==== Additionally recommended tests

* Automated restart of the ASCS (simulating RKS)

* Check the recoverable and non-recoverable outage of the message server process

* Check the non-recoverable outage of the {sap} enqueue server process

* Check the outage of the {sapERS}

* Check the outage and restart of `{sapStartSrv}`

* Check the rolling kernel switch procedure (RKS), if possible

* Check the simulation of an upgrade

* Check the simulation of cluster resource failures


== References

For more information, see the documents listed below.

=== Pacemaker

- Pacemaker 1.1 Configuration Explained:
https://clusterlabs.org/pacemaker/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/

- Pacemaker 2.0 Configuration Explained:
https://clusterlabs.org/pacemaker/doc/en-US/Pacemaker/2.0/html-single/Pacemaker_Explained/index.html

:leveloffset: 2

include::SAPNotes_ha740.adoc[]
++++
<?pdfpagebreak?>
++++

:leveloffset: 0
////
############################
#
# APPENDIX
#
############################
////

:leveloffset: 0

== Appendix

=== CRM configuration

Find below the complete crm configuration for {sap} system {mySid}.
// TODO PRIO2: priority fencing with below priorities?

[subs="attributes"]
----
#
# nodes
#
node 1084753931: {myNode1}
node 1084753932: {myNode2}
#
# primitives for ASCS and ERS
#
primitive rsc_fs_{mySID}_{myInstAscs} Filesystem \
	params device="{myDevPartAscs}" directory="/usr/sap/{mySID}/{myInstAscs}" fstype=xfs \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=40s
primitive rsc_fs_{mySID}_{myInstErs} Filesystem \
	params device="{myDevPartErs}" directory="/usr/sap/{mySID}/{myInstErs}" fstype=xfs \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=40s
primitive rsc_ip_{mySID}_{myInstAscs} IPaddr2 \
	params ip={myVipAAscs} \
	op monitor interval=10s timeout=20s
primitive rsc_ip_{mySID}_{myInstErs} IPaddr2 \
	params ip={myVipAErs} \
	op monitor interval=10s timeout=20s
primitive rsc_sap_{mySID}_{myInstAscs} SAPInstance \
	op monitor interval=11 timeout=60 on-fail=restart \
	params InstanceName={mySID}_{myInstAscs}_{myVipNAscs} \
	 START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstAscs}_{myVipNAscs}" \
	 AUTOMATIC_RECOVER=false \
	meta resource-stickiness=5000 failure-timeout=60 migration-threshold=1 \
	 priority=10
primitive rsc_sap_{mySID}_{myInstErs} SAPInstance \
	op monitor interval=11 timeout=60 on-fail=restart \
	params InstanceName={mySID}_{myInstErs}_{myVipNErs} \
	 START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstErs}_{myVipNErs}" \
	 AUTOMATIC_RECOVER=false IS_ERS=true \
	meta priority=1000
#
# SBD with adapted timing
#
primitive stonith-sbd stonith:external/sbd \
	params pcmk_delay_max=30
#
# group definitions for ASCS and ERS
#
group grp_{mySID}_{myInstAscs} rsc_ip_{mySID}_{myInstAscs} rsc_fs_{mySID}_{myInstAscs} rsc_sap_{mySID}_{myInstAscs} \
	meta resource-stickiness=3000
group grp_{mySID}_{myInstErs} rsc_ip_{mySID}_{myInstErs} rsc_fs_{mySID}_{myInstErs} rsc_sap_{mySID}_{myInstErs}
#
# constraints between ASCS and ERS
#
colocation col_sap_{mySid}_not_both -5000: grp_{mySID}_{myInstErs} grp_{mySID}_{myInstAscs}
location loc_sap_{mySid}_failover_to_ers rsc_sap_{mySID}_{myInstAscs} \
	rule 2000: runs_ers_{mySID} eq 1
order ord_sap_{mySid}_first_ascs Optional: rsc_sap_{mySID}_{myInstAscs}:start rsc_sap_{mySID}_{myInstErs}:stop symmetrical=false
#
# crm properties and more
#
property cib-bootstrap-options: \
	have-watchdog=true \
	dc-version=1.1.18+20180430.b12c320f5-1.14-b12c320f5 \
	cluster-infrastructure=corosync \
	cluster-name=hacluster \
	stonith-enabled=true \
	last-lrm-refresh=1494346532
rsc_defaults rsc-options: \
	resource-stickiness=1 \
	migration-threshold=3
op_defaults op-options: \
	timeout=600 \
	record-pending=true
----

=== Corosync configuration of the two-node cluster

Find below a corosync configuration example for one corosync ring. Ideally, two rings would be used.

[subs="specialchars,attributes"]
----
# Read the corosync.conf.5 manual page
totem {
    version: 2
    secauth: on
    crypto_hash: sha1
    crypto_cipher: aes256
    cluster_name: hacluster
    clear_node_high_bit: yes
    token: 5000
    token_retransmits_before_loss_const: 10
    join: 60
    consensus: 6000
    max_messages: 20
    interface {
        ringnumber: 0
        mcastport: 5405
        ttl: 1
    }

    transport: udpu
}

logging {
    fileline: off
    to_stderr: no
    to_logfile: no
    logfile: /var/log/cluster/corosync.log
    to_syslog: yes
    debug: off
    timestamp: on
    logger_subsys {
        subsys: QUORUM
        debug: off
    }

}

nodelist {
    node {
        ring0_addr: {myIPNode1}
        nodeid: 1
    }

    node {
        ring0_addr: {myIPNode2}
        nodeid: 2
    }

}

quorum {

    # Enable and configure quorum subsystem (default: off)
    # see also corosync.conf.5 and votequorum.5
    provider: corosync_votequorum
    expected_votes: 2
    two_node: 1
}
----

=== /usr/sap/sapservices without native `systemd` integration

[subs="specialchars,attributes,quotes"]
----
#!/bin/sh
LD_LIBRARY_PATH=/usr/sap/{mySid}/{myInstAscs}/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/{mySid}/{myInstAscs}/exe/sapstartsrv pf=/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs} -D -u {mySapAdm}
LD_LIBRARY_PATH=/usr/sap/{mySid}/{myInstErs}/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/{mySid}/{myInstErs}/exe/sapstartsrv pf=/usr/sap/{mySid}/{myInstErs}/profile/{mySid}_{myInstErs}_{myVipNErs} -D -u {mySapAdm}
----

// Standard SUSE Best Practices includes
== Legal notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

////
Version 1.0 - Initial version based on SLE12 and NW 7.40
Version 1.1 - Including SLE15 preparation, NW 7.50, Unicast set-up
////
//
// REVISION 1.1  2018/04
//   - PowerLE
//   - corr: StartService
//   - removed no-quorum-policy=ignore
//   - corr: upper/lowercase of section titles
// Revision 1.1a 2020/04
//   - migration to adoc build process
//   - small adaption based on customer feedback
// Revision 1.1b 2022/02
//   - SAP native systemd support
// Revision 1.1b 2022/02
//  - small adaption based on customer feedback
//
