== Operating a Cloud

Setting up a highly scalable cloud environment is a demanding task. But 
it is equally important to run it effectively and to maintain it in a way 
that allows the provider to benefit from the economy of scale.
_Economy of scale_ is a term often used to describe a major benefit
that results from running a cloud. If all processes are standardized and
automated properly, adding more nodes to the installation is easy and
can be done without a lot of effort. The goal is to get all required
functionality in place right from the start and then profit from it
whilst the cloud can be scaled.

This chapter focuses on the most important aspects of running and
maintaining a large scale cloud environment. It covers the major tasks
of the operational everydays business: system maintenance, monitoring,
and log files.

=== Maintaining Hundreds of Hosts

Automation is no new trend in IT. Until 10 years ago, many companies 
ignored the need for automation because they had simply not been used 
to the concept. But solutions such as Puppet, Salt, and Ansible have made 
clear that automation is the key to maintaining the own infrastructure 
more effectively and cheaper. It is no longer recommended to pay for highly 
skilled engineers and architects to perform tasks that can be 
automated. Automation frees up skilled personnel to focus on developing 
new product features and technologies.

Automation is essential for a successful cloud implementation. In this case,
what is true for small and conventional setups is even more true for a typical
cloud environment that consists of dozens, hundreds or thousands of nodes. 

==== Automation Is Key

One of the central design targets of clouds is the ability to scale out.
This is from particular importance for public cloud environments.
At any point in time, a new customer can show up and request to get a
higher amount of resources. Even if the cloud provider manages to
provide resources to a customer because of the resource buffer they had
planned for the platform (usually 25-30% of the total 
resources available), they still need to scale out quickly to re-create new 
buffer and to allow for further growth.

If performed manually, adding dozens or hundreds of nodes in parallel to a 
setup can be a tedious task. However, it can also be a quite convenient 
task that does not involve a lot of human intervention.

In cloud environments, the entire lifecycle of physical machines must
be automated as much as possible. This means that automation technologies 
must be in place long before the Linux operating system is installed on 
the target system. The only manual part in the process of adding new nodes 
should be the physical installation of a machine in the rack. All the other
tasks should be included in an automated process.

First, the machine boots into a network-based tool to inventory and 
prepare the system. Then, an automated installation of the operating 
system is performed. Finally, required software along with the necessary 
configuration is installed on the machine via automation tools such as Salt 
or Ansible.

SUSE OpenStack Cloud comes with tools that allow administrators to capitalize
on an automated work stream, supporting features such as the automation of the
following:

- Discovery of (new) hardware
- Installation of an operating system
- Management of system and application configuration
- Maintenance of installed software (Updates / Upgrades)

==== A Working Maintenance Concept

Getting systems up is not the only relevant task. Having a complete 
and well-designed maintenance scheme in place is just as important. One 
big challenge are security updates. Many security updates require the 
installation of new packages on the affected systems. Some updates 
require a new version of the Linux kernel which results in the affected servers
to be rebooted. Rebooting hundreds or thousands of servers in a coordinated
and controlled manner is a complex task. For this purpose, SUSE OpenStack
Cloud offers tools that not only support the installation of updates in a
centralized way but also perform complete reboots of the environment.

In a working maintenance concept, the central system management service takes
on the key role. It ensures a stable and controlled software distribution, 
the staging of updates, and the automatic roll-out of patches and configurations. 
SUSE Manager offers all these functionalities and helps with compliance 
features like the _CVE search_ to handle a large number of Linux nodes.

A working maintenance concept for clouds also needs to define several
processes specific to the setup in question. Processes are usually highly 
adapted to the environment where they are used, including factors such 
as the company running the setup.

==== Some Approaches Will Fail

Because of the specific functionalities shipped with SUSE OpenStack Cloud 
and SUSE Enterprise Storage, most work related to installing and maintaining 
the platform is done in an automated manner. Administrators need to make 
sure though that they do not make void some of these functionalities by 
design decisions related to their scale-out cloud projects.

[[Ephemeral-Issues]]
One regular issue, for example, is local storage in virtual machines
inside the cloud. As explained in the storage chapter, the local
storage of virtual machines in clouds is called _ephemeral_
storage. On the systems, the _temporary_ local storage devices of VMs
are usually local images residing on a storage device inside the host
where the VM is running. This is often a fast Solid State Drive (SSD). 
That is why, from the user perspective, running production workload based 
on ephemeral storage is tempting. It features much higher IOPS values and 
notably less latency than network-connected storage such as Ceph volumes. 
The issue with local storage in individual nodes of the setup
is that they are not redundant and not replicated somewhere. Cloud 
providers cannot make guarantees on the availability of the ephemeral disks 
of individual VMs.

A conclusive example for why using local, ephemeral storage in VMs is not
an option is a cloud-wide reboot of all servers because of necessary
security updates. The large cloud providers can reboot their
machines and assume that applications deployed on top of the cloud are
designed in a redundant way. Companies serving customers with less modern 
applications might choose a compromise and migrate the individual VMs away 
from physical servers that are about to be rebooted. But this live migration 
can only be done successfully if the VM's data reside on volumes that can 
easily be detached from one host and attached to another. VMs using ephemeral
storage only cannot be migrated. At worst, they make it even impossible to 
reboot individual physical servers. This heavily violates the principle of 
the economy of scale.

It is important to ensure that ephemeral storage is not abused by users for 
purposes it does not fit for, and to ensure that useful alternatives such 
as highly-performing Ceph storage are available.

Companies planning to set up a cloud need to determine their target 
architecture carefully, including their requirements for storage. 
Implementations with mixtures of local and shared ephemeral storage are, 
while technically possible, a complex matter and need thorough consideration. 
Getting support from knowledgeable experts on the matter is recommended.

=== Monitoring OpenStack Environments

When SUSE OpenStack Cloud and SUSE Enterprise Storage are up and running,
it is important to monitor both solutions properly. Efficient monitoring 
is the only way to ensure that faults and issues are detected immediately
and can be handled appropriate.

When talking about _monitoring_, administrators used to conventional setups 
will think of well-known and conventional monitoring tools such as Nagios 
or Zabbix. These tools are excellent choices for event-based monitoring in 
conventional setups, as they reliably identify failure events and trigger 
alarms.

Most of these tools, however, are not state-of-the-art anymore. In addition,
event-based monitoring is not the only kind of surveillance required in
a cloud. Another very important requirement in clouds is the ability to have
an overview of the current load, the historical load and the likely
development of load in the nearer future. This functionality is called
_trending_ and requires the collection of respective metric values from all
nodes in the cloud.

Solutions suitable for monitoring cloud environments and recording the
metric values required for trending are referred to as _Monitoring, Alerting, 
Trending_ (MAT).

==== VMs and the Platform Itself

When the subject is _Monitoring the cloud_, a clear distinction must be
made between monitoring individual VMs in the cloud and monitoring the
underlying infrastructure of the cloud. Many public cloud providers do
not care about the status of their customers' VMs. But they offer 
a software service which their cloud customers can configure to monitor
their VMs and other services. This additional service is called 
_Monitoring as a Service_. OpenStack offers a solution for it, which
is discussed further down in this chapter.

In contrast to monitoring the VMs stands the necessity to monitor all 
components that form the physical and the logical infrastructure of the 
cloud. Besides OpenStack and Ceph, all accompanying services such as 
RabbitMQ, MariaDB, and all devices for network infrastructure and for 
power supply must be monitored. 

The following paragraphs describe solutions for both the monitoring of 
VMs in the cloud and the monitoring of the cloud itself.

==== Disadvantages of Conventional Tools in Cloud Environments

Prior to this, however, it is necessary to explain the differences between
cloud monitoring tools and conventional solutions such as Nagios or Zabbix. 
As mentioned before, trending is an important aspect of keeping control of
the platform. Conventional solutions often provide features for trending. 
Nagios for example offers PNP4Nagios, Zabbix also comes with built-in
trending capabilities. Most of these solutions suffer from an inherent
design flaw; they store trending data in relational databases
such as MariaDB or PostgreSQL. However, this represents a serious 
performance bottleneck. The data model of said databases does not match 
the format of metric data required for cloud trending.

The example below explains what this means in detail.

An administrator wants to know how the usage of virtual CPUs has developed in 
a specific platform during the last year. The monitoring solution 
has recorded the required data and stored all values in MariaDB. But to 
generate a concise and understandable outcome in the form of a graphic, the 
monitoring software needs to run an utterly large MariaDB query that reads 
individual lines from those tables in MariaDB that hold the data. All collected 
data is then drawn into a graphic and displayed to the user.

The database query is very resource-intensive, and the example above covers 
only the _read_ aspect of trending. The _write_ aspect is even worse. If every 
system has 200 metric values that the administrator wants to fetch every 15 
seconds, they can end up with hundreds of thousands of SQL queries per minute,
depending on the overall amount of nodes in the setup. Such a load 
quickly brings every MariaDB instance to its limits. Even if the MariaDB 
instance survives the load, the generation of the graphic and trending in 
general are slow and tedious.

==== An Introduction to Monitoring, Alerting and Trending

Modern solutions for Monitoring, Alerting and Trending (MAT) also use 
databases to store data, but in contrast to conventional solutions they do 
not use relational databases such as MariaDB. Instead, they use Time Series 
Databases (TSDB), which operate completely different. TSDBs are not based 
upon tables and rows but align all data on a single root element which is 
the timeline itself. Queries like the one mentioned previously are very easy 
to serve that way. Because data is stored in the database in the same format 
that it is supposed to be displayed in, gathering metric data on a certain 
time period from TSDBs is easy and convenient from both the 
administrator's and database's point of view.

One advantage of this kind of trending is that also basic monitoring can
be done using the same technology. Metrics can almost arbitrarily
be defined in modern TSDB implementations as long as they can be expressed 
in a numeric value. For example, one metric could be "number of Apache 
Web server processes in operation on a host". If said number falls
below the desired value, the TSDB triggers an alarm. It is important to 
understand that, while metric-based monitoring can by done by all TSDB 
implementations, event-based alerting is not available in every TSDB 
implementation. Further down in this chapter it is explained why 
that is not necessarily a disadvantage in massive scale-out setups.

==== Variant 1: Monasca

Monasca is OpenStack's solution for both _Monitoring as a Service_ and the
monitoring of the OpenStack platform itself. Monasca is an official
OpenStack service and supported by SUSE OpenStack Cloud. Monasca consists of 
many components that work hand in hand to ensure an efficient and
smooth-running monitoring solution.

Several components such as the _Kafka_ stream processing engine play a role 
in the Monasca monitoring environment. The persistent storage of data for 
long-term trending is done using a TSDB and follows modern standards. The 
_monasca-agent_ component collects every metric available on the target 
systems (physical machines or VMs) and transports it back to the central 
Monasca engine.

As an OpenStack service, Monasca is deeply integrated with all other OpenStack 
services. It uses Keystone for authentication and works well with the other 
OpenStack components. Monasca can also be accessed using Grafana, the leading 
open source solution for visualizing trending data.

==== Variant 2: Prometheus and Its Components

If Monasca is not the ideal solution for a particular setup, a good 
alternative is Prometheus. It features a TSDB that comes 
with several additional components to allow for a smooth monitoring 
experience. Prometheus itself is the core of the environment and takes
care of storing collected metrics from the individual physical hosts in 
the cloud.

Prometheus comes with a separate program to collect metric data on the
target systems, the so called Prometheus _Node Exporter_. _Exporter_ is 
a synonym for _agent_ in the Prometheus universe because the exporters 
act like agents. They communicate with Prometheus via a standardized API. 
Because Prometheus is, like Monasca, open source software, that API is 
openly checkable and fully documented. Consequently, a lot of open source 
projects are defining interfaces for metric data aggregation in their 
applications or provide separate exporters for their programs that can be 
combined with Prometheus. In this regard, Prometheus can be considered more
versatile than Monasca, which is very much OpenStack-specific.

Prometheus also comes with an AlertManager that generates alerts based on 
pre-defined rules. For these rules, Prometheus developers have invented 
a new query language that is similar to but not identical with SQL.

The previously mentioned Grafana visualization solution for metric data has
a back-end driver for Prometheus and can connect to it natively. The same
holds for Ceph, which offers a Prometheus-compatible interface that the
solution can read Ceph metric data from without using any additional exporter, 
because Ceph has a Prometheus metric data exporter built-in.

Lastly, Prometheus can easily be combined with all the tools
in the _TICK_ stack created by InfluxDB. This is especially helpful for
the storage of trending-data on a long-term base (this means several years of
all kinds of historical metric data). InfluxDB is better suited
for this task than Prometheus. Combining both solutions allow
administrators to get the best from both worlds.

==== Monasca or Prometheus: There Is Choice

Monasca and Prometheus are only two examples for the many options
to properly monitor an OpenStack installation. If you have a time 
series-based monitoring solution in place, it is possible to extend 
the solution to support OpenStack. An important question is whether you 
want to monitor the OpenStack setup only or also VMs running on it. If you
want to monitor both, Monasca is likely the best choice. If flexibility, in
respect of the collection of metrics is relevant, Prometheus offers more 
options than Monasca, which is precisely tailored to the OpenStack use case.

Whatever solution you choose, it is important to understand that large scale 
environments need monitoring, alerting and trending. Solutions that 
administrators are used to for historical reasons might not be suitable for 
this purpose.

=== Knowing What Is Going On: Logging

Many MAT solutions are good for trending-based metric types but not for
event-based alerting. That is because a scale-out environment can produce
a much higher number of alerts a conventional monitoring solution can handle.

==== The Need for Centralized Logging

Large environments must have a central solution for logging in place. 
When debugging an issue under stress, an administrator
cannot login to dozens or hundreds of servers and search the local logs on 
these machines for certain indicators. Instead, administrators need a 
solution that aggregates relevant logs from all machines and makes them 
available through an indexed, searchable database.

==== How Monitoring & Logging Go Hand in Hand

Having a solution for centralized logging in place makes monitoring events 
by means of a TSDB easier. When a valid metric is defined for a certain event, 
and when that event triggers an alert in the monitoring system, the 
administrator can log in to the centralized log aggregation system and 
examine the logs of the affected system. Tedious SSH jumping is not necessary 
anymore.

==== Variant 1: ELK

A variant to create centralized logging based on open source
software is the _ELK_ stack. ELK is an acronym for _Elasticsearch_, 
_Logstash_ and _Kibana_ and refers to three components that are deployed 
together. Elasticsearch is the indexing and search engine that received log 
entries from systems. Logstash collects the log files from the target systems 
and sends them to Elasticsearch. Kibana is a concise and easy-to-use interface 
to Logstash and Elasticsearch and allows for Web-based access.

Although these three components are not always combined, the acronym _ELK_ 
has become an established term for this solution. Sometimes, for example the
Logstash component is replaced by _Fluentd_ or other tools for log aggregation. 
The excellent versatility of this solution is one of its biggest advantages.

When using Monasca for MAT, ELK is recommended for logging. Monasca integrates 
well with ELK and you can combine these two tools to get an efficient solution. 

==== Variant 2: Splunk

A commercial alternative to the ELK stack is _Splunk_. It
is recognized by system administrators for its simple setup and usability. 
It can easily be extended with new features, and there is an entire ecosystem 
for the solution, boosted by the company behind Splunk.

The disadvantage is that Splunk has a charging model based on the amount of
transferred log files. As OpenStack tends to generate a lot of logs,  
in large scale environments, the amount of logs in these setups is very high. 
Splunk licenses become a relevant cost factor in budget plannings. 
A huge advantage however is that administrators get a well-working solution 
for large scale environments.

// vim:set syntax=asciidoc:
