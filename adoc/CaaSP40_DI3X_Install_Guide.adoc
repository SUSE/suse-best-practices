:docinfo:

// defining article ID
[#art-caasp40-sapdi3x-install]

= SAP Data Intelligence 3 on SUSE CaaS Platform 4: Installation Guide


////
++++
<?pdfpagebreak?>
++++
////

== Introduction

Today, business and industries create more and more data. As the amount of data
grows, so does the need to manage and get the best out of the data. SAP Data
Intelligence 3 is a tool to make it easier to deal with such amounts of data,
and with SUSE CaaS Platform 4, SUSE delivers the foundation on top of which to
run SAP Data Intelligence 3.

== Requirements

To install SAP Data Intelligence 3 on SUSE CaaS Platform 4, we need to
meet certain requirements. Consult the relevant documentation:

* SUSE

** link:https://documentation.suse.com/suse-caasp/4/[SUSE CaaS Platform 4]
** link:https://www.suse.com/documentation/suse-enterprise-storage-5[SUSE Enterprise Storage]

* SAP

** link:https://help.sap.com/viewer/product/SAP_DATA_HUB/2.7.latest/en-US[SAP Data Hub]

** link:https://launchpad.support.sap.com/#/notes/2686169[SAP Note 2686169]

** link:https://launchpad.support.sap.com/#/notes/2865345[SAP Note 2865345]

=== Hardware and software requirements

==== SUSE CaaS Platform 4 cluster

Hardware requirements:

* see https://help.sap.com/viewer/product/SAP_DATA_HUB/[SAP Data Hub Install Guide]

See SAP's sizing recommendations:

* link:https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.7.latest/en-US/79724de552db4b2b81c4a893f2c7ed18.html[SAP Data Hub 2.7]

The minimum hardware requirements for installing SAP Data Intelligence 3 for
on-premises production use are:

* 7 Kubernetes cluster nodes (three master nodes and four worker nodes)

** All machines should have at least four CPU cores

** The master nodes should have > 32 GiB RAM

** The worker nodes should have > 64 GiB RAM

* Load-balancer

* Management host

// TODO check network requirements The faster the better 1 GiB/s or more
// TODO disk requirements, Master nodes and worker nodes need at least 100 GiB so called ephemeral storage so 150 GiB for / /var/lib and /var/log are needed.
Environments for non-production use may run with one master, three workers,
and a management node.

NOTE: Having only one master node is not recommended for production environments.
For production use, three master nodes are recommended.


* Access to a SUSE Enterprise Storage 5 or SUSE Enterprise Storage 6 system (see SAP Note link:https://launchpad.support.sap.com/#/notes/2686169[Pre-requisites for installing SAP Data Hub])

==== Management host

//It is recommended to do the installation of SAP Data Hub Foundation from an external "Jump" host and not from within the SUSE CaaS Platform Cluster.
//The hardware and operating system specifications for the jump host can be e.g.

//- SUSE Linux Enterprise Server 12 SP5 or SUSE Linux Enterprise Server 15 SP1 (or even openSUSE Leap 15.X)
* Two cores
* 16 GiB RAM
* Disk space:
** 80 GiB for `/`, including the space for the SAP Data Intelligence 3 software
** At least 100 GiB for `/var/lib/docker` (needed for the SAP Data Intelligence 3 installation)
* Network connectivity to the SUSE CaaS Platform cluster (1 GBit/s)

// FIXME what about replacing docker with podman?

=== Software requirements

The following software is needed:

* SUSE CaaS Platform 4.0.3
+
NOTE: For SAP Data Hub, it is required to stay on SUSE CaaS Platform 4.0.0—4.0.3,
because of the Kubernetes version delivered with our CaaSP product. SAP Data
Intelligence 3 needs a Kubernetes version not higher than 1.15.

* SAP Data Hub 2.7.1 or higher

* SAP Hostagent

* Access to SAP Maintenance Planner

* Connection to a secure private Docker registry

* Optional Hadoop/Spark, see Vora's Spark extensions


== Installing SUSE CaaS Platform 4

=== Getting the installation media

All installation media can be found at:
link:++https://download.suse.com++[SUSE CaaS Platform ISO images]

=== Getting a subscription for SUSE CaaS Platform 4

To be able to get all maintenance updates for your SUSE products, you
need a valid subscription for the particular product:
* https://www.suse.com/support/?id=SUSE_CaaS_Platform[Order subscription for SUSE CaaS Platform]


=== Reading the Deployment Guide for SUSE CaaS Platform 4

SUSE CaaS Platform is designed to make the installation of Kubernetes easy. To
get a deeper understanding you should read the Deployment Guide for SUSE CaaS
Platform 4,  available at https://www.suse.com/documentation/
For further reference, there are also a Quick Start Guide and an Administrator's
Guide.

=== Installing SUSE CaaS Platform 4 on SUSE Linux Enterprise

In this guide, we describe the installation of SUSE CaaS Platform 4 on top of
SUSE Linux Enterprise 15 SP1 on your premises. We will use the SUSE Linux Enterprise 15 SP1 installer ISO image for
starting the installation.

Make sure that the host names you will use for the installation can be resolved
via DNS. It is preferred to have a static network setup.

You will need to have the FQDN or IP address of your time server. A reliable
system time is required.

Connect the media to your hardware and boot from the media.
// Screenshot?

Select *Installation* from the GRUB menu.

==== Preparation

Install SUSE Linux Enterprise 15 SP1 or higher (as released for CaaS Platform 4.x) on all nodes.

The following Modules/Products are required on the respective hosts:

* Management host:

** SUSE Linux Enterprise 15 SP1
** SUSE Linux Enterprise 15 SP1 Containers Modules
** SUSE Linux Enterprise 15 SP1 Public Cloud
** SUSE CaaSP 4


* Kubernetes master nodes:

** SUSE Linux Enterprise 15 SP1
** SUSE Linux Enterprise 15 SP1 Public Cloud
** SUSE CaaSP 4

* Kubernetes worker nodes:

** SUSE Linux Enterprise 15 SP1
** SUSE Linux Enterprise 15 SP1 Public Cloud
** SUSE CaaSP 4

* Load balancer host:

** SUSE Linux Enterprise Server for SAP applications 15 SP1
+
or
+
** SUSE Linux Enterprise 15 SP1 plus High Availability Extension


==== Installing the management node

* Install SUSE Linux Enterprise 15 SP1

* Add the following software:
// what repositories? No repos listed!

** Install software pattern "CaaSP management"

** Install Python2 and PyYaml-python2 (needed for installing DH2.7)

** Install Docker (needed for installing DH2.7)


==== Installing the load balancer

This is only necessary if no other load-balancing device is available.
In our setup, we describe the installation of a `haproxy` instance.
`haproxy` is available in the SUSE Linux Enterprise High Availability Extension or SUSE Linux Enterprise Server for SAP applications.

* Install SUSE Linux Enterprise 15 SP1

* Add the "SLE 15 SP1 HA" repository
+
(in case of SUSE Linux Enterprise Server for SAP applications, this is added automatically)

* Create the `haproxy` configuration

* Enable and start `haproxy`


==== Installing the master nodes

* Install SUSE Linux Enterprise 15 SP1

** Set up static IP addresses

** Use the "Expert Partitioner" to disable any swap partition

** Disable `firewalld`

** Enable `ssh`

* Add the necessary software repositories:

** SUSEConnect -p caasp/4.0/x86_64

** SUSEConnect -p ...
// What does "..." denote here? It looks like some info is missing — LGHP

* Create the directory `/var/lib/docker/containers`


==== Installing the worker nodes


* Install SUSE Linux Enterprise 15 SP1

** Set up static IP addresses

** Use the "Expert Partitioner" to disable any swap partition

** Disable `firewalld`

** Enable `ssh`

* Add the necessary software repositories:

** SUSEConnect -p caasp/4.0/x86_64

** SUSEConnect -p ...
// What does "..." denote here? It looks like some info is missing — LGHP


==== Creating the Kubernetes cluster on SUSE CaaS Platform 4

* Log in to the management host

** Ensure that `ssh-agent` is working as expected
+
(for example `ssh -A root@managementhost.example.com`)
// FIXME: ssh-agent

** Initialize the Kubernetes cluster
+
----
skuba cluster init --control-plane <LB_IP/FQDN> my-cluster
----

** Create the first master node of the Kubernetes Cluster
+
----
cd my-cluster
skuba node bootstrap --user sles --sudo --target <IP/FQDN> <NODE_NAME>
----

** Add additional master nodes
+
----
skuba node join --role master --user sles --sudo --target <IP/FQDN> <NODE_NAME>
----

** Add worker nodes to the Kubernetes cluster
+
----
skuba node join --role worker --user sles --sudo --target <IP/FQDN> <NODE_NAME>
----

** verify your Kubernetes cluster
+
----
skuba cluster status
----

//FIXME command output????

* Modifications needed to install SAP Data Haub 2 on a CaaSP 4 cluster

** Change the `pid` parameter in `/etc/crio/crio.conf` from 1024 to 8192

** Edit `/etc/containers/registry.conf` to reflect your private registry
+
// e.g.:
// add example configuration



== Installing SAP Data Intelligence 3

The following sections describe the preparation and installation of SAP Data Intelligence 3 on a SUSE CaaS Platform 4
cluster.


=== Preparing the SAP Data Intelligence 3 installation

The steps that are needed to successfully install SAP Data Hub on SUSE
CaaS Platform are as follows:


==== Downloading the SAP Data Intelligence 3 software archive

To download and install SAP Data Intelligence 3:

1. Go to the SAP Software Download Center, log in with your SAP account, and search for
"SAP DATA Intelligence 3".
// or use this link. ← commented out because no link provided — LGHP

2. Download the SAP Data Hub Foundation file, for example:
* `DHFOUNDATION07_3-80004015.ZIP` (SAP DATA HUB - FOUNDATION 2.7)

3. Unzip the software archive on to your management host.
+
There are three ways to install SAP Data Intelligence 3:
+
* Use the SL Plugin. There are two variants of it:

** SL Plugin with Maintenance Planner (`mpsl`)
** SL Plugin only (`mpfree`)

* Use the command line `install.sh` script.

+
This document will focus on the Maintenance Planner and SL Plugin installation
methods.


[id="prerequisites_caasp_cluster"]
==== Prerequisites on the SUSE CaaS Platform 4 cluster

The following steps are done on the jump host if not stated otherwise:

1. Create a new namespace in the Kubernetes cluster, into which to install SAP Data
Intelligence 3:
+
----
$ kubectl create namespace datahub
----

2. Create the storage class to provide volumes for SAP Data Intelligence 3 on SUSE Enterprise Storage:
+
Make sure you have the connection data for your SUSE Enterprise Storage at hand:

* IP addresses and port number (default: 6789) of the monitor nodes of your SUSE
Storage cluster
* Create a data pool (`datahub` in this example) on your SUSE Enterprise Storage
for use with SAP Data Intelligence 3

3. Edit the example below to fit your environment.
+
----
$ cat > storageClass.yaml <<EOF
apiVersion: storage.kubernetes.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
  name: datahub
  namespace: default
parameters:
  adminId: admin
  adminSecretName: ceph-admin-secret
  adminSecretNamespace:  default
  imageFeatures: layering
  imageFormat: "2"
  monitors: <IP ADDRESS OF MONITOR 1>:6789, <IP ADDRESS OF MONITOR 2>:6789, <IP ADDRESS OF MONITOR 3 >:6789
  pool: datahub
  userId: admin
  userSecretName: ceph-user-secret
provisioner: kubernetes.io/rbd
reclaimPolicy: Delete
volumeBindingMode: Immediate
EOF

$ kubectl create -f storageClass.yaml
----

4. Create the secrets needed to access the storage:

+
Obtain the keys from your SUSE Enterprise Storage cluster. 
They are located in `ceph.admin.keyring` and `ceph.user.keyring`.

+
You must encode the keys using `base64`:
+
----
echo <YOUR KEY HERE> | base64
----
+
----
$ cat > ceph-admin-secret.yaml <<EOF
apiVersion: v1
kind: Secret
metadata:
    name: ceph-admin-secret
type: "kubernetes.io/rbd"
data:
   key: <YOUR BASE64 ENCODED KEY HERE>
EOF
image::002-SCT-CaaSP.png
$ cat > ceph-user-secret.yaml <<EOF
apiVersion: v1
kind: Secret
metadata:
    name: ceph-user-secret
type: "kubernetes.io/rbd"
data:
   key: <YOUR BASE64 ENCODED KEY HERE>
EOF

$ kubectl create -f ceph-admin-secret.yaml
$ kubectl create -f ceph-user-secret.yaml
----


=== Installing SAP Data Intelligence 3 using the SAP Maintenance Planner with SL Plugin

SAP recommends the "SAP Maintenance Planner with SL Plugin" (`mpsl`) installation method as the best approach to install
SAP Data Intelligence 3.

SAP recommends this Web-based installation method because it offers
an option to send analytic data and feedback to SAP. All the necessary
prerequisites have been satisfied by applying all the steps described above.

NOTE: You need to install the latest SAP Host Agent on the management host. You
can use the `rpm` package downloadable from the SAP Software Download center.

* Install the SAP Host Agent on the management host

** Download the `rpm` package from the SAP Software Download Center
** Install SAP Host Agent:
+
----
zypper in path to rpm
----

** Start SAP Host agent
+
----
/etc/init.d/sapinit start
----
//TODO Check systemd integration

** Configure SAP Host Agent to be used with SAP Maintenance Planner

*** Create certificates according to the documentation provided by SAP

*** Enable CORS Web service in SAP Host Agent

** Navigate to `apps.sap.com/mp` with your browser

*** Create a new plan

*** Select "container based system"

*** Connect to your SAP Host Agent running on the management host

*** Follow the steps given by the wizard
//TODO add screenshots

*** Fill in values for the installer as needed.
// See screenshots. ← commented out pending their addition
//TODO add screenshots

*** Wait for successful deployment



=== Installing SAP Data Intelligence 3 using the SL Plugin (`mpfree` method)

This is an alternative command-line-based installation method. 
Refer to the SAP Data Hub documentation (2.7) for more information and the exact procedure.
//TODO insert URLs


=== Installing SAP Data Intelligence 3 using the command line (manual installation)

Unpack the SAP Data Hub software archive on the jump host — for example:

----
$ unzip DHFOUNDATION07_3-80004015.ZIP
----

Run the installation command as described in the SAP Data Intelligence 3 install guide:
// TODO URL help.sap.com/datahub

* link:https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.7.latest/en-US[Installation of SAP Data Intelligence 3]

----
$ cd SAP-Datahub-2.7.155-Foundation
$ export DOCKER_REGISTRY=<URI of your registry>
$ export NAMESPACE=<YOUR NAME SPACE HERE>
// $ ./install.sh --enable-kaniko=yes  --docker-log-path='/var/log/containers'  -e vora-vsystem.vRep.nfsv4MinorVersion=1 -e vora-diagnostic.fluentd.logDriverFormat=regexp
$ ./install.sh --enable-kaniko=yes  --docker-log-path='/var/log/containers' -e vora-diagnostic.fluentd.logDriverFormat=regexp
----

This interactive script configures the installation of SAP Data Intelligence 3.
You should have the following information at hand:
//TODO namespace, login on registry, S-User

* Name and credentials of your SAP `S-User`
* Your namespace
* Login credentials to your secure registry

//TODO explain cmdline options


=== Post-installation tasks

After successful installation, you can connect to the SAP Data Hub Web UI.
You need to identify the service IP and port of the SAP Data Hub UI.

// Refer to SAP Documentation, create ingress, or edit service vsystem to be of type NodePort.
// Or create software loadbalancer, e.g. Metalllb, see nginx ingress docs.

----
kubectl -n $NAMESPACE get services
kubectl -n $NAMESPACE describe service vsystem
----

Point your browser to the IP and port you obtained from the steps above.
// TODO Example.

Use the login data you defined during the installation.

==== Post-installation steps

Follow the link:https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.7.latest/en-US/4c472c40595b450283a6ce039f71cfc6.html[documentation provided by SAP] to the
post-installation steps.

* Create the `vflow-secret` for the modeller app, as pointed out in the SAP
documentation.

* Import any necessary certificate authority, for example the CA that signed
the certificate of the secure registry.


== Upgrading SAP Data Intelligence 3
// Does this need to be adjusted to Data Intelligence 3? It talks about Data Hub 2 .. 

This section describes the update of an existing SAP Data Hub 2 installation to
a higher version (e.g. 2.3 to 2.4)

Execute the SAP Data Hub upgrade as described in the official instructions. One can choose between different upgrade methods:

* Maintenance Planner:
+
link:https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.6.latest/en-US/31079833a65f4f379d5a76957ff8073c.html[Upgrade SAP Data Hub 2 using the Maintenance Planner / SL Plugin and SAP Host Agent]
* SL Plugin method:
+
link:https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.6.latest/en-US/ff37f3ccf6504bb38d7db53936fe8017.html[Upgrade SAP Data Hub 2 using the SL Plugin and SAP Host Agent]
* Command line method:
+
link:https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.6.latest/en-US/aec679bc0209443ba4ae03a9018d4bd8.html[Upgrade SAP Data Hub 2 using the install.sh script]

== Appendix


=== Secure Private Registry

To satisfy the requirements of SAP Data Hub, you also need a Docker Registry.
The easiest way to build and manage one is with the link:http://port.us.org/[Portus
project].

First, you need to create a dedicated server for your Docker registry and Portus
stack.

----
# sudo virt-install --name portus-dr --ram 8192 --disk path=/var/lib/libvirt/VMS/portus-dr.qcow2,size=40 --vcpus 4 --os-type linux --os-variant generic --network bridge=common --graphics none --console pty,target_type=serial --location '/var/lib/libvirt/isos/SLE-12-SP4-Server-DVD-x86_64-GM-DVD1.iso' --extra-args 'console=ttyS0,115200n8 serial ifcfg=eth0=10.10.10.11/24,10.10.10.1,10.10.10.11,suse-sap.net hostname=portus-dr domain=suse-sap.net Textmode=1'
----

In our example, this server will be connected to another local bridge which
provides common services (DNS, SMT, Docker-registry) for the Data Hub stack.

Our Portus deployment will be based on a container, and orchestrated locally with
`docker-compose`.

NOTE: Portus deployments using `docker-compose` require an up-to-date release of
the `docker-compose` tool.

----
sudo curl -L "https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
----

Now you can simply clone the Portus repository. Adapt the `.env` and the
`nginx` configurations to your naming convention.

----
# git clone https://github.com/SUSE/Portus.git /tmp/Portus-DR
# mv /tmp/Portus-DR/examples/compose ./portus
# cd portus
----

You can edit both `.env` and `nginx/nginx.conf`.
This is how our configuration looks:

----
# cat .env
MACHINE_FQDN=portus-dr.suse-sap.net
SECRET_KEY_BASE=b494a25faa8d22e430e843e220e424e10ac84d2ce0e64231f5b636d21251eb6d267adb042ad5884cbff0f3891bcf911bdf8abb3ce719849ccda9a4889249e5c2
PORTUS_PASSWORD=XXXXXXXX
DATABASE_PASSWORD=YYYYYYYY
----

In the `nginx/nginx.conf` file, you should adapt the following section:

----
server {
    listen 443 ssl http2;
    server_name portus-dr.suse-sap.net;
    root /srv/Portus/public;
----

Pull the latest `docker-compose.yml`:

----
# rm docker-compose.*
# wget https://gist.githubusercontent.com/Patazerty/d05652294d5874eddf192c9b633751ee/raw/6bf4ac6ba14192a1fe5c337494ab213200dd076e/docker-compose.yml
----

To avoid dealing with Docker's insecure registry configuration, add SSL
to your setup.

----
echo "subjectAltName = DNS:portus-dr.suse-sap.net" > extfile.cnf
openssl genrsa -out secrets/rootca.key 2048
openssl req -x509 -new -nodes -key secrets/rootca.key -subj "/C=FR/ST=FR/O=SUSE"  -sha256 -days 1024 -out secrets/rootca.crt
openssl genrsa -out secrets/portus.key 2048
openssl req -new -key secrets/portus.key -out secrets/portus.csr -subj "/C=FR/ST=FR/O=SUSE/CN
openssl req -new -key secrets/portus.key -out secrets/portus.csr -subj "/C=FR/ST=FR/O=SUSE/CN=portus-dr.suse-sap.net"
openssl x509 -req -in secrets/portus.csr -CA secrets/rootca.crt -extfile extfile.cnf -CAkey secrets/rootca.key -CAcreateserial  -out secrets/portus.crt -days 500 -sha256
----

Next, all you need to do is to make the servers aware of this certificate:

----
cp -p secrets/rootca.crt /etc/pki/trust/anchors/.net-ca.crt
scp secrets/rootca.crt root@jumpbox.suse-sap.net:/etc/pki/trust/anchors/portus-dr.suse-sap.net-ca.crt
----

Then, on all servers that need to interact with the Docker registry, do the following:

----
sudo update-ca-certificates
sudo systemctl restart docker
----

Start your Portus setup:

----
docker-compose up -d
----

Finally, you can log in to Portus and configure the registry.

image::portus-registry.png[title="Portus Registry",scaledwidth=95%]


==== Installing and configuring a secure private registry using SUSE Linux Enterprise Server and the SLE-Container-Module

The needed components are Docker, a registry, and Portus. 
Create SSL certificates as needed. Distribute the CA certificates to all your Kubernetes nodes.

Run:

----
# update-ca-certificates
# systemctl restart docker
----


Create the namespaces on your registry that are needed for SAP Data Intelligence
3:

* `com.sap.hana.container`

* `com.sap.datahub.linuxx86_64`

* `com.sap.datahub.linuxx86_64.gcc6`

* `consul`

* `elasticsearch`

* `fabric8`

* `google_containers`

* `grafana`

* `kibana`

* `prom`

* `vora`

* `kaniko-project`

* `com.sap.bds.docker`
//TODO Liste der Namespaces


=== SUSE Enterprise Storage

//SAP Data Hub 2 installation on premise require SUSE Enterprise Storage 5 or higher.
//Follow the installation documentation to create SUSE Enterprise Storage.
//Create a storage pool on your SES for the use with SAP Data Hub 2.
//You can also leverage the S3 API to create buckets on SES that can be used from applications from within SAP Data Hub 2.

An on-premises installation of SAP Data Intelligence requires SUSE Enterprise
Storage 5 or higher.

If you plan to use SUSE Enterprise Storage not only for your Kubernetes dynamic
storage class, but also for your Kubernetes Control plan (virtualized or not),
you should reserve enough resources to address the 
link:https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md.[`etcd` hardware requirements]

The following steps will deploy a minimalist, virtualized, test-oriented instance of SUSE Enterprise Storage 5.5.
In the our example, we will build a four-node (1 admin + 3 OSD) Ceph cluster.

Before you start:

* Obtain registration codes for SUSE Linux Enterprise Server 12 SP3 and SUSE
Enterprise Storage from https://scc.suse.com, or have an SMT/RMT properly set up
and already mirroring these products:

++++
<?pdfpagebreak?>
++++

** link:https://scc.suse.com[SCC]
+
image::scc-sle.png[title="SUSE Customer Center: SUSE Linux Enterprise Server Overview",scaledwidth=99%]

++++
<?pdfpagebreak?>
++++

** SMT
+
image::scc-ses.png[title="SUSE Customer Center: SUSE Enterprise Storage Overview",scaledwidth=99%]


* You should already have set up a DNS zone. In our example, where all Data Hub
components are in the same DNS zone and the same subnet, it should look like:
+
image::dns.png[title="DNS Zone",scaledwidth=95%]


* To be as efficient as possible when using interactive shell-scripted infrastructure deployment, 
we advise to use an advanced terminal client or multiplexer which will allow you to address multiple shells at once:
+
image::multi-s-virtinstall.png[title="Multiple Shells View",scaledwidth=95%]


Now you can create the virtual machines.

* Create

** First the Admin Node:
+
----
# sudo virt-install --name ses55-admin --ram 16384 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-admin.qcow2,size=40 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-admin-osd0.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-admin-osd1.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-admin-osd2.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-admin-osd3.qcow2,size=20 --vcpus 4 --os-type linux --os-variant generic --network bridge=caasp3 --graphics none --console pty,target_type=serial --location '/var/lib/libvirt/ISOS/SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso' --extra-args 'console=ttyS0,115200n8 serial autoyast-ses5=http://10.10.10.101/autoyast-ses5 ifcfg=eth0=10.18.10.200/24,10.18.10.1,10.10.10.11,suse-sap.net domain=suse-sap.net Textmode=1'
----

** Then the OSD Nodes:
+
----
# sudo virt-install --name ses55-osd0 --ram 16384 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd0.qcow2,size=40 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd0-osd0.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd0-osd1.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd0-osd2.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd0-osd3.qcow2,size=20 --vcpus 4 --os-type linux --os-variant generic --network bridge=caasp3 --graphics none --console pty,target_type=serial --location '/var/lib/libvirt/ISOS/SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso' --extra-args 'console=ttyS0,115200n8 serial autoyast-ses5=http://10.10.10.101/autoyast-ses5 ifcfg=eth0=10.18.10.230/24,10.18.10.1,10.10.10.11,suse-sap.net domain=suse-sap.net Textmode=1'

# sudo virt-install --name ses55-osd1 --ram 16384 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd1.qcow2,size=40 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd1-osd0.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd1-osd1.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd1-osd2.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd1-osd3.qcow2,size=20 --vcpus 4 --os-type linux --os-variant generic --network bridge=caasp3 --graphics none --console pty,target_type=serial --location '/var/lib/libvirt/ISOS/SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso' --extra-args 'console=ttyS0,115200n8 serial autoyast-ses5=http://10.10.10.101/autoyast-ses5 ifcfg=eth0=10.18.10.231/24,10.18.10.1,10.10.10.11,suse-sap.net domain=suse-sap.net Textmode=1'

# sudo virt-install --name ses55-osd2 --ram 16384 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd2.qcow2,size=40 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd2-osd0.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd2-osd1.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd2-osd2.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd2-osd3.qcow2,size=20 --vcpus 4 --os-type linux --os-variant generic --network bridge=caasp3 --graphics none --console pty,target_type=serial --location '/var/lib/libvirt/ISOS/SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso' --extra-args 'console=ttyS0,115200n8 serial autoyast-ses5=http://10.10.10.101/autoyast-ses5 ifcfg=eth0=10.18.10.232/24,10.18.10.1,10.10.10.11,suse-sap.net domain=suse-sap.net Textmode=1'
----

+
image::multi-s-smt.png[title="Multiple SMT View",scaledwidth=95%]

* Select the SUSE Enterprise Storage 5 extension:
+
image::multi-s-addon.png[title="Selecting SES Extensions on all Nodes",scaledwidth=95%]

* On the hypervisor, you should also be able to route or bridge your upcoming
SUSE Enterprise Storage 5.5 network segment. In our example, for simplicity, we are
using the same bridge and network address as our CaaSP cluster:
`--network bridge=caasp3`

* In our example below, each node is powered by 16 GiB of RAM, 4 vCPUs,
40 GiB for the root disk, and 4 × 20 GiB OSDB disks:
+
image::multi-s-default.png[title="Selecting System Role on all Nodes",scaledwidth=95%]

* NTP must be configured on each node:
+
image::multi-s-ntp.png[title="Configuring NTP on all Nodes",scaledwidth=95%]

* Deselect "AppArmor" and the unnecessary "X" and "GNOME" patterns, but select
the "SUSE Enterprise Storage" pattern:
+
image::multi-s-patterns.png[title="Selecting Patterns on all Nodes",scaledwidth=95%]

* De-activate the firewall on the nodes.

* Start the installation on all nodes:
+
image::multi-s-install.png[title="Starting Installation on all Nodes",scaledwidth=95%]

* When the nodes have rebooted, log in and finish the network/host name and NTP
configurations, so that `hostname -f` returns the FQDN of the nodes, and
`ntpq -p` returns a stratum less than 16:
+
image::multi-s-hostname-ntp.png[title="Finishing NTP configuration",scaledwidth=95%]

* Using `ssh-keygen` then `ssh-copy-id`, spread your SUSE Enterprise Storage Admin
node `ssh` public key to all other nodes.

* Verify that the drives you will allocate for SUSE Enterprise Storage OSDs are clean by wiping them. 

* On all nodes, including the Admin Node, install `salt-minion`.

* On the Admin Node only (in our example, `ses55-admin.suse-sap.net`), also
install `salt-master` and `deepsea`.

* Then, restart `salt-minion` on all nodes, and restart `salt-master` on the Admin Node:
+
image::multi-s-salt-install-restart.png[title="Installing Salt on all Nodes",scaledwidth=95%]

* Accept the related pending Salt keys:
+
image::salt-key.png[title="Accepting Salt Keys",scaledwidth=30%]

* Verify that `/srv/pillar/ceph/master_minion.sls` points to your Admin Node.
In our example, it contains the FQDN of our `salt-master`:
+
`master_minion: ses55-admin.suse-sap.net`

* Prepare the cluster:
+
----
# salt-run state.orch ceph.stage.0
----
+
image::ceph-stage-0.png[title="Preparing the Cluster",scaledwidth=90%]

* Collect information about the nodes:
+
----
# salt-run state.orch ceph.stage.1
----
+
image::ceph-stage-1.png[titel="Collecting Information about Nodes",scaledwidth=90%]

* Adapt the file `/srv/pillar/ceph/proposals/policy.cfg` to your needs. In our
example, where the only deployed service is OpenAttic, it contains the following:
+
----
cluster-ceph/cluster/ses55-osd2.suse-sap.net.sls
config/stack/default/ceph/cluster.yml
config/stack/default/global.yml
profile-default/cluster/ses55-admin.suse-sap.net.sls
profile-default/cluster/ses55-osd0.suse-sap.net.sls
profile-default/cluster/ses55-osd1.suse-sap.net.sls
profile-default/cluster/ses55-osd2.suse-sap.net.sls
profile-default/stack/default/ceph/minions/ses55-admin.suse-sap.net.yml
profile-default/stack/default/ceph/minions/ses55-osd0.suse-sap.net.yml
profile-default/stack/default/ceph/minions/ses55-osd1.suse-sap.net.yml
profile-default/stack/default/ceph/minions/ses55-osd2.suse-sap.net.yml
role-admin/cluster/ses55-admin.suse-sap.net.sls
role-admin/cluster/ses55-osd0.suse-sap.net.sls
role-admin/cluster/ses55-osd1.suse-sap.net.sls
role-admin/cluster/ses55-osd2.suse-sap.net.sls
role-master/cluster/ses55-admin.suse-sap.net.sls
role-mgr/cluster/ses55-osd0.suse-sap.net.sls
role-mgr/cluster/ses55-osd1.suse-sap.net.sls
role-mgr/cluster/ses55-osd2.suse-sap.net.sls
role-mon/cluster/ses55-osd0.suse-sap.net.sls
role-mon/cluster/ses55-osd1.suse-sap.net.sls
role-mon/cluster/ses55-osd2.suse-sap.net.sls
role-openattic/cluster/ses55-admin.suse-sap.net.sls
----


* Prepare the final state of configuration files set:
+
----
# salt-run state.orch ceph.stage.2
----
+
image::ceph-stage-2.png[title="Preparing Configuration Files",scaledwidth=90%]

* You can now safely deploy your configuration:
+
----
# salt-run state.orch ceph.stage.3
----
+
image::ceph-stage-3.png[title="Deploying Configuration Files",scaledwidth=90%]

* When Stage 3 has completed successfully, check the cluster's health to
ensure that everything is running properly:
+
----
# ceph -s
----
+
image::ceph-health.png[title="Checking Cluster Health",scaledwidth=90%]

* To get the benefits of the OpenAttic WebUI, you must now initiate
`ceph.stage.4`, which will install the OpenAttic service:
+
----
# salt-run state.orch ceph.stage.4
----
+
image::ceph-stage-4.png[title="Installing OpenAttic service",scaledwidth=90%]


* You can now manage your cluster through the WebUI:
+
image::openattic-dash.png[title="SUSE Enterprise Storage WebUI",scaledwidth=90%]


* To provide a Data Hub RBD device, you first need to create a related pool:

image::openattic-pool.png[title="Creating Ceph Pool",scaledwidth=90%]


* Then provide access to this pool through an RBD device:
+
image::openattic-rbd.png[title="Accessing Ceph Pool",scaledwidth=90%]
 

You can now go to <<prerequisites_caasp_cluster>> and follow the prerequisites
for a SUSE CaaS Platform Cluster.


// === Troubleshooting ← commented out because empty — LGHP


// :leveloffset: 0
// Standard SUSE Best Practices includes
== Legal notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
include::common_gfdl1.2_i.adoc[]
