= SAP NetWeaver High Availability Cluster 7.40 for the AWS Cloud - Setup Guide
Fabian Herschel, Bernd Schubert, Stefan Schneider (AWS)
2019/6/28


:Revision: 1.2

Revision {Revision} from {docdate}

// Standard SUSE includes
include::common_copyright_gfdl.adoc[]

:toc:

include::VariablesAWS.adoc[]

////
TODO maybe the whole setup could be divided in two parts: Enqueue Replication
cluster which is the core part, and the auxiliary pieces which might come from somewhere?
DONE cover: SAP SAP HA 740 Setup Guide -> (better title)
DONE p.1 solution e.g. as provided -> solution, e.g. as provided
DONE p.2 This mean we -> This means we
DONE p.2 it's -> its (2x)
DONE p.2 added t the ->  added to the
REJC p.2 either adding the third node to the cluster -> (would change the basic setup)
DONE p.3 SAP HANA we recommend -> SAP HANA, we recommend
DONE p.2,3,17 to setup -> to set up
DONE p.3 be setup -> be set up
DONE p.4 sap -> SAP NetWeaver
DONE p.4 in a highly -> in an highly
DONE p.4 software than the -> software then the
TODO p.4 https://blogs.sap.com/2014/05/08/using-sapvendorclusterconnector-for-interaction-between-cluster-framework-and-sapstartsrv/comment-page-1/ -> (check link)
DONE p.5 need to shared -> need to be shared
DONE p.5 to the cluster node hacert01 -> to the cluster nodes hacert01
DONE p.5 as SBD. -> as SBD. For production a more elaborated disk setup might be used.
DONE p.6 /dev/sda -> (really sda ?)
DONE p.6 # partprobe -> # partprobe; fdisk -l /dev/sda
TODO p.7 (somewhere explain that the D02 and DVEBMGS01 on the DB host just to simplify this lab environment)
DONE p.7 MEDIEN -> MEDIA
TODO p.7,8 (only short hostnames in /etc/hosts?)
REJC p.8 on a NFS -> on an NFS
DONE p.9 Server 2 -> server 2
DONE p.9 Server 3 -> server 3
DONE p.9 # l -> # ls -l
DONE p.9 insgesamt 0 -> total 0
DONE p.10 takeover -> take-over
DONE p.10 installation illustrated here -> installation as illustrated here
DONE p.11 Temporary we -> Temporarily we (2x)
DONE p.11 or use it. -> or use it. Please make sure to use the right virtual hostname for each installation step. (2x)
DONE p.12  instance please -> instance, please
DONE p.12 As user ha1adm.  -> (font)
TODO p.13 TODO/TBD ->
DONE p.13 product versions and currently -> product versions. Currently
DONE p.14 usermod -a -> # usermod -a
DONE p.14 leed in loss  -> lead in loss
TODO p.14 File /usr/sap/HA1/SYS/profile/HA1_ASCS00_sapha1as. -> (font)
TODO p.14 File /usr/sap/HA1/SYS/profile/HA1_ERS10_sapha1er. -> (font)
DONE p.15 GB and we used -> GB. We use
TODO p.17 TODO/TBD ->
DONE p.17 in forehand -> beforehand
DONE p.17 install patterns ha_sles -> install pattern ha_sles
TODO p.17 <nul> -> refer to sle-ha quickstart guide on our webpage
TODO p.18 <nul> -> (run ha-cluter-join or do something to enable cluster?)
TODO p.18 <nul> -> some notes on adpating resulting config, depending on environment
DONE p.19,20,21,22 crm configure -> # crm configure
DONE p.22,23 cd -> # cd
DONE p.22,23 unzip -> # unzip
DONE p.23 cd -> # cd
DONE p.23 tar -> # tar
DONE p.23 ln -s -> # ln -s
DONE p.23 wget -> # wget
DONE p.23 mv -> # mv
DONE p.23 hatool This -> hatool. This
DONE p.23 Setup the -> Set up the
DONE p.24 java -cp -> # java -cp
TODO p.24 /usr/sap/HA1/SYS/exe/uc/linuxx86_64/sapcontrol -> # linuxx86_64/sapcontrol
DONE p.25 lead into loss of enqueue locks, because -> lead into loss of enqueue locks. Because
TODO p.25 As ha1adm. -> (font)
TODO p.25,26,27,28,29 sapcontrol -nr -> # sapcontrol -nr
DONE p.26 As user root or ha1adm. -> (font 3x)
DONE p.26 crm configure -> # crm configure   (2x)
DONE p.26,28 crm resource -> # crm resource
DONE p.26 proceess -> process
DONE p.26 plan to go to -> plan going into
DONE p.26,27 * As user  ->  As user  (no bullet)
DONE p.28 echo b -> # echo b
DONE p.29 irecoverable -> non-recoverable (2x)
DONE p.29 rollong -> roll-on/roll-off (or just rolling?)
TODO p.31 in HA-Umgebungen -> in HA environments (links without "D"?)
TODO p.32 ... -> TODO/TBD
TODO p.32 -> SLE-HA release notes https://www.suse.com/releasenotes/x86_64/SLE-HA/12-SP2/
TODO p.32 SLE-HA quick setup guide
TODO p.32 SLE-HA product docu
TODO p.32 -> TODO SLES-for-SAP release note
TODO p.32 -> TODO product docu
TODO -> NFS SAP layout for instance
TODO: non line break with in command or variables
TODO: table of requiered values
////

== About this Guide

=== Introduction

{sles4sapReg} is the optimal platform to
run {sapReg} applications with high availability. Together with a redundant layout
of the technical infrastructure, single points of failure can be eliminated.

{sapBSReg} is a sophisticated application platform for large enterprises
and mid-size companies. Many critical business environments require the highest
possible {sapReg} application availability.

The described cluster solution could be used for {sapReg} S/4 HANA as well as for
{sapReg} {sapNW}.

{sapNw} is a common stack of middleware functionality used to support the SAP
business applications. The {sapERS} constitutes application
level redundancy for one of the most crucial components of the {sapNw} stack,
the enqueue service. An optimal effect of the enqueue replication mechanism can
be achieved when combining the application level redundancy with a high
availability cluster solution e.g., as provided by {sles4sap}. The described
concept has proven its maturity over several years of productive operations for
customers of different sizes and branches.


=== Additional Documentation and Resources

Chapters in this manual contain links to additional documentation resources that
are either available on the system or on the Internet.

For the latest documentation updates, see http://www.suse.com/documentation.

You can also find numerous whitepapers, a best-practices guide, and other
resources at the SUSE Linux Enterprise Server for SAP Applications resource
library: https://www.suse.com/products/sles-for-sap/resource-library/.

This guide and other SAP specific best practices could be downloaded via
https://www.suse.com/products/sles-for-sap/resource-library/sap-best-practices/.
You can find at this landing page guides for {SAPHANA} system replication
automation and HA scenarios for {SAPNw} and {s4hana}.

// Standard SUSE includes
=== Feedback
include::common_intro_feedback.adoc[]

//=== Documentation Conventions
//TODO work on SUSE doc standard conventions file
//include::common_intro_typografie.adoc[]

== Scope of this Document

This guide will show you how to:

- Plan a {sleHA} platform for {sapNw},
  including {sapERS}.
- Set up a {linux} high availability platform and perform a basic {sapNW}
  installation including {sapERS} on {sle}.
- Integrate the high availability cluster with the {sap} control framework via
  {s4sClConnector}, as certified by {sap}.
- Install HA cluster solutions for the SAP HANA database on AWS as being
  described in SAP note 2309342 (SUSE Linux Enterprise High Availability
  Extension on AWS).

This guide focuses on the high availability of the central services. HA cluster
solutions for the database and {sapNW} instances are described in the best
practice "Simple Stack" available on our landing page (see section "Additional
documentation and resources"). For {saphana} system replication please follow
the guides for the performance- or cost-optimized scenario.

== Overview

This guide describes how to set up a pacemaker cluster using {sles4sap}
{slesProdVersion} for the Enqueue Replication scenario on the AWS platform.
This guide does not document how to install on premises pacemaker clusters.
The goal is to match the {sapCert} certification specifications and goals.

These goals include:

- Integration of the cluster with the {SAP} start framework _sapstartsrv_ to
  ensure that maintenance procedures do not break the cluster stability
- Rolling Kernel Switch (RKS) awareness
- Standard {sap} installation to improve support processes

The updated certification {sapcert} has redefined some of the test procedures
and described new expectations how the cluster should behave in special
conditions. These changes allowed us to improve the cluster architecture and to
design it for easier usage and setup.

Shared SAP resources are being managed in AWS Elastic File Systems (EFS). The
SAP instances themselves are installed on EFS file systems to allow switching over
the file systems for proper functionality.

=== Using AWS Architectures in SLES Pacemaker Clusters

SLES pacemaker clusters will be installed in an AWS region. An AWS region
consists of multiple availability zones. Availability zones are located in different
data centers which are 10 to 50km apart. Availability zones have independent
flood levels, electricity and network hookup. They are supposed to be
independent. AWS recommends architectural patterns where redundant cluster nodes
are being spread across availability zones (AZs) in order to allow a customer
to overcome individual AZ failures.

An AWS Virtual Private Network (VPC) is spanning all AZs. We assume that a
customer will have.

-	Identified two availability zones to be used
-	Created subnets in the two AZs which can host the two nodes of a SLES HAE
   cluster
-	Use a routing table which is attached to the two subnets
-	Optionally: host a Route53 private hosted naming zone to manage names in the
VPC

The AWS specific components can be installed in two configurations. Both
configurations use the AWS Overlay IP address. An Overlay IP address is an
AWS specific routing entry which can send network traffic to an instance,
no matter which AZ the instance is located in.

The SLES HAE cluster will update this routing entry as it is required.
All SAP system components in the VPC will be able to reach an AWS instance with
a SAP system component inside a VPC through this Overlay IP address.

Overlay IP addresses have one disadvantage: they have to come from a CIDR range
which is outside of the VPC. Otherwise they would be part of a subnet and a
given availability zone.

On premises users like SAP GUIs will not be able to reach this IP address
since the AWS Virtual Private Network (VPN) gateway will not route traffic
to such an IP address.
A customer has two options to overcome this limitation.

1.	Use a SAP Router in the VPC. On premises users can reach it. The SAP router
can relay traffic to the ASCS system.
2.	Configure the additional Route 53 agent. Route 53 is the AWS specific name
service. The cluster agent will change the IP address for a given name of the
ASCS service. The on premises name server will have to delegate requests to
the sub domain in the AWS VPC to this name service. On premises SAP GUI user
will contact the ASCS through a name. Section TBD in the appendix explains how
to integrate Route 53 with your local naming services.

=== Prerequisites for the AWS specific HA Installation
There are a number of prerequisites which have to be met before starting
the installation:

* Have an AWS account
* Have an AWS user with admin rights. At least rights to
**	Create security groups
** Create EFS file systems
** Modify AWS routing tables
** Create policies and attach them to IAM roles
** Optional for Route53 agent installation
*** Create and modify A-records in a private hosted zone
*	Understand your landscape
**	Know your region and it's AWS name
**	Know your VPC and it's AWS id
** Know which availability zones you want to use in your VPC
** Have a subnet in each of the availability zones
*** Have a routing table which is implicitly or explicitly attached to the two
subnets
*** Have free IP addresses in the two subnets for your SAP installation and EFS
mount points
*** Allow network traffic in between the two subnets
*** Allow outgoing Internet access from the subnets
** Optionally: Have a Route 53 private hosted zone which hosts a subdomain for
instances in the two subnets
** Have a resource record with a name and the IP address for the SAP central
instance

Please use the check list in the appendix to note down all information needed
before starting the installation.

==== Tagging the EC2 Instances

The EC2 instances will have host names which are automatically generated.
Select host names which comply with SAP requirements, see SAP note 611361.

The cluster agents will have to be able to identify the EC2 instances in the
correct way. This happens through instance tags.

Tag the two EC2 instances
through the console or the AWS Command Line Interface (CLI) with arbitrarily
chosen tag like _cluster_ and the host name as it will be shown in the command
_uname_ . Use the same tag (like _cluster_) and the individual host names
for both instances. The AWS documentation
(http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html) explains
how to tag EC2 instances.

NOTE: Refrain from using non ASCII characters in any tag assigned to cluster
managed resources.

==== Security Groups

This section does not cover a discussion of SAP related ports in security groups.
This section lists the ports which need to be available for the SUSE cluster only.

The following ports and protocols need to be configured to allow the two
cluster nodes to communicate with each other:

* Port 5405 for inbound UDP: Used to configure the corosync communication layer.
Port 5405 is being used in common examples. A different port may be used
depending on the corosync configuration.

* Port 7630 for inbound TCP: Used by the SUSE "hawk" web GUI.

* enable ICMP: Used through a ping command in the AWS IP-move agent of the
SUSE cluster.

We assume that there are no restriction for outbound network communication.

==== Creating an AWS CLI Profile on both EC2 Instances

The SLES agents use the AWS Command Line Interface (CLI).
They will use an AWS CLI profile which needs to be created for
the root account _root_ on both
instances. The SUSE resources require a profile which creates output in the
text format. The name of the profile is arbitrary. The name chosen in this
example is _cluster_. The region of the instance needs to be added as well.
Replace the string _region-name_ with your target region in the following example.

One way to create such a profile is to create a file _/root/.aws/config_
with the following content:

[subs="attributes"]
----
[default]
region = region-name
[profile cluster]
region = region-name
output = text
----

The other way is to use the _aws configure_ CLI command in the following way:

[subs="attributes"]
----
# aws configure
AWS Access Key ID [None]:
AWS Secret Access Key [None]:
Default region name [None]: region-name
Default output format [None]:

# aws configure --profile cluster
AWS Access Key ID [None]:
AWS Secret Access Key [None]:
Default region name [None]: region-name
Default output format [None]: text
----

This command sequence generates a default profile and a cluster profile.

==== Configure http Proxies

This action is not needed if the system has transparent access to the Internet.
The resource agents execute AWS CLI (Command Line Interface) commands. These
commands send http/https requests to an access point in the Internet. These
access point are usually directly reachable. Systems which don't offer
transparent Internet access will have to provide a http/https proxy.
The configuration of the proxy access is described in full detail in the
AWS documentation.

Please add the following environment variables to the root user's _.bashrc_
file:

[subs="attributes"]
----
export HTTP_PROXY=http://a.b.c.d:n
export HTTPS_PROXY=http://a.b.c.d:m
----

Please add the following environment variables instead of the ones above
if authentication is required:

[subs="attributes"]
----
export HTTP_PROXY=http://username:password@a.b.c.d:n
export HTTPS_PROXY=http://username:password@a.b.c.d:m
----

The AWS Data Provider for SAP will need to reach the instance meta data service
directly. Please add the following environment variable to the root user's
_.bashrc_ file:

[subs="attributes"]
----
export NO_PROXY=169.254.169.254
----

==== Disable the Source/Destination Check for the Cluster Instances

The source/destination check can be disabled through scripts using
the AWS command line interface (AWS-CLI). The following command needs to be
executed one time for both EC2 instances, which are supposed to receive traffic
from the Overlay IP address:

[subs="attributes"]
----
# aws ec2 modify-instance-attribute --profile cluster --instance-id EC2-instance --no-source-dest-check
----

The system on which this command gets executed needs temporarily a role with
the following policy:

[subs="attributes"]
----
{
   "Version": "2012-10-17",
   "Statement": [
   {
      "Sid": "Stmt1424870324000",
      "Effect": "Allow",
      "Action": [ "ec2:ModifyInstanceAttribute"],
      "Resource": [
      "arn:aws:ec2:region-name:account-id:instance/instance-a",
      "arn:aws:ec2:region-name:account-id:instance/instance-b"
      ]
   }
   ]
}
----

Replace the individual parameter for the region, the account
identifier and the two identifiers for the EC2 instances with appropriate
values.

The source/destination check can be disabled as well from the AWS console.
It takes the execution of the following pull down menu in the console for
both EC2 instances (see below).

.Disable Source/Destination Check at Console
image::SourceDestinationCheck.png[PNG]

==== Avoid Deletion of Cluster Managed IP Address on the eth0 Interface

SLES 12 SP3 is the first SLES version which ships the cloud-netconfig package.
This package will remove any secondary IP address which is managed by the cluster
agents from the eth0 interface. This can cause service interruptions for
users of the HA service. Perform the following task on all cluster nodes.

Check whether the package cloud-netconfig-ec2 is installed with the command
[subs="attributes"]
----
# zypper info cloud-netconfig-ec2
----

Update the file _/etc/sysconfig/network/ifcfg-eth0_ if this package is installed.
Change the following line to a „no“ setting or add the line if the package
isn't yet installed:
[subs="attributes"]
----
CLOUD_NETCONFIG_MANAGE='no'
----

==== AWS Roles and Policies
The SAP ASCS and ESR will run the SLES Pacemaker software and the agents.
This software needs a number of AWS IAM privileges to operate the cluster.
Create a new Role for every ASCS/ESR cluster and associate this role to the
two instances. Attach the following policies to this role:

===== AWS Data Provider Policy
Every cluster node will operate a SAP system. SAP systems on AWS require an
installation of the “AWS Data Provider for SAP”. The data provider needs a
policy to access AWS resources. Use the policy as described in the
“AWS Data Provider for SAP Installation and Operations Guide“  section IAM
Roles and attach it to the role of the instance. This policy can be used by
all SAP systems. It takes only one policy in an AWS account. The policy doesn’t
contain any instance specific privileges.

[subs="attributes"]
----
{
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "EC2:DescribeInstances",
                "EC2:DescribeVolumes"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": "cloudwatch:GetMetricStatistics",
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::aws-data-provider/config.properties"
        }
    ]
}
----

===== STONITH Policy
The instances of the SUSE cluster will need the privilege to start and stop
the other nodes in the cluster. Create a policy with a name like _stonith-policy_
with the following content and attach it to the cluster role:

[subs="attributes"]
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "Stmt1424870324000",
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeInstances",
                "ec2:DescribeInstanceAttribute",
                "ec2:DescribeTags"
            ],
            "Resource": "*"
        },
        {
            "Sid": "Stmt1424870324001",
            "Effect": "Allow",
            "Action": [
                "ec2:ModifyInstanceAttribute",
                "ec2:RebootInstances",
                "ec2:StartInstances",
                "ec2:StopInstances"
            ],
            "Resource": [
                "arn:aws:ec2:region-name:aws-account:instance/i-node1",
                "arn:aws:ec2:region-name:aws-account:instance/i-node2"
            ]
        }
    ]
}
----
Replace the variable _aws-account_ with the appropriate AWS account identifier.
Replace the variables _i-node1_ and _i-node2_ with the AWS instance-ids of
your two  cluster nodes of ({myNode1}) and ({myNode2}).
Replace the variable _region-name_ whith the name of your AWS region (Example:
us-east-1 for the N. Virginia region).
This policy is dependent
of the instances of your cluster. You will need a separate policy for
every cluster!

==== Overlay IP Agent Policy
The Overlay IP agent will change a routing entry in an AWS routing table.
Create a policy with a name like _Manage-Overlay-IP-Policy_ and attach it to
the role of the cluster instances:

[subs="attributes"]
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "Stmt1424860166260",
            "Action": [
                "ec2:DescribeRouteTables",
                "ec2:ReplaceRoute"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:ec2:region-name:account-id:route-table/rtb-XYZ"
        }
    ]
}
----
This policy allows the agent to update the routing tables which get used.
Replace the following variables with the appropriate names:

- region-name : the name of the AWS region

- account-id : The name of the AWS account in which the policy is getting used

- rtb-XYZ : The identifier of the routing table which needs to be updated

==== Route 53 Updates
It is optional to install the Route 53 agent in the cluster. This policy is
needed only when the Route 53 agent will be used. Create a policy with the
name _Route53-Update_ and attach it to the role of the two cluster nodes:

[subs="attributes"]
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "Stmt1471878724000",
            "Effect": "Allow",
            "Action": "route53:GetChange",
            "Resource": "arn:aws:route53:::change/*"
        },
        {
            "Sid": "Stmt1471878724001",
            "Effect": "Allow",
            "Action": "route53:ChangeResourceRecordSets",
            "Resource": "arn:aws:route53:::hostedzone/hosted zone ID/full name"
        },
        {
            "Sid": "Stmt1471878724002",
            "Effect": "Allow",
            "Action": [
                "route53:ListResourceRecordSets",
                "route53:ChangeResourceRecordSets"
            ],
            "Resource": "arn:aws:route53:::hostedzone/hosted zone ID"
        }
    ]
}

----
This policy is specific to the hosted zone and the resource record set. Replace
the variables _hosted zone ID_ with the AWS id of the hosted zone. Replace
 _full name_ with the name of the entry.
 An individual Route 53 policy needs to be created individually for every cluster.

=== Add Overlay IP Addresses to Routing Table
Manually add two routing entries to the routing table which is assigned to the
two subnets. The IP addresses have to be outside of the CIDR range of the VPC.
Use the AWS console and search for “VPC”.

*	Select VPC
*	Click on “Route Tables” in the left column
*	Select route table used for SAP ASCS subnets
*	Click on tabulator “Routes”
*	Click on “Edit”
* Scroll to the end of the list and click on “Add another route”

==== Add the Service IP Address for your ASCS Service
Add the service IP address of the ASCS service (node {myNode1}). Use as
filter /32 (example: 192.168.10.1/32). Add the Elastic Network Interface (ENI)
name of your instance
which is initially serving as ASCS. Save your changes by clicking on “Save”.

This is the service IP address with the name {myNodeServiceASCS}.

==== Add the Service IP Address for your ERS Service
Add the service IP address of the ERS service (node {myNode2}).
Use as filter /32 (example: 192.168.10.2/32). Add the Elastic Network Interface
(ENI) name of your instance which is initially serving as ERS. Save your changes
by clicking on “Save”.

This is the IP address with the name {myNodeServiceERS}.

=== EFS File System
The cluster will need an NFS file system being provided by AWS Elastic File
System (EFS). The file system will manage

* /usr/sap/HA1 data for ASCS00, ERS10, D02, DVEBMGS01 and the other
application servers and the SYS directory
* /sapmnt

You will need the identifier of your VPC and the subnet identifiers of the
subnets in which you plan to operate the two cluster nodes. It is Okay to pick
other subnets. These subnets have to be reachable by the two cluster nodes
and they have to be in the same availability zone (AZ) for high availability reasons.
The option “General Purpose” will be sufficient.

Note down the DNS name of your specific EFS server. AWS name services will
resolve it internally to your VPC to an IP address which is in your availability
zone. We will refer to this name as "efs-name" when we will have to mount the
file systems.

We will use one file system for two future mount points (/usr/sap/HA1/ASCS00,
/usr/SAP/HA1/ESR10). This keeps the administration level low and it will provide
more throughput. AWS throughput in EFS is based on the total size of the
file system.

Login into one of the two cluster nodes and create a number of directories in
the EFS file system through a temporary mount. Become root and execute the following commands:

[subs="attributes"]
----
# mount efs-name: /mnt
# mkdir -p /mnt/ASCS00 /mnt/ERS10 /mnt/D01 /mnt/DVEBMGS01 /mnt/D02 /mnt/SYS /mnt/sapmnt /mnt/sapcd
# umount /mnt
----
Create additional directories for other application servers. This NFS
file system will be used for all of them.

Mount the two mount points in the cluster nodes. Execute the following command
on both cluster nodes as root:

[subs="attributes"]
----
# mkdir -p /sapmnt /usr/sap/HA1/SYS
----
Add the following two lines to the file /etc/fstab on the two instances which
will run the SAP ASCS and the ERS service.

[subs="attributes"]
----
efs-name:SYS     /usr/sap/HA1/SYS    nfs4	rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2	0 0
efs-name:sapmnt  /sapmnt             nfs4	rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2	0 0
----

Replace _efs-name_ with the appropriate DNS name.

Mount the file system as root with the command

[subs="attributes"]
----
# mount /usr/sap/HA1/SYS
# mount /sapmnt
----

==== Enable Cluster Instances to use the Overlay IP Address

The two cluster instances need the Overlay IP address to be configured as
secondary IP address on their standard interface _eth0_. This can be achieved
by the command:

[subs="attributes"]
----
# ip address add OVERLAY-IP dev eth0
----

Execute this command with root privileges on both instances.
Add the ASCS IP address on the ASCS node {myNode1}. Add the Enque Replication
address on the ERS node {myNode2}.

=== Differences to previous Cluster Architecture
The concept is different to the old stack with the master-slave architecture.
With the new certification we switch to a more simple model with primitives.
This means we have on one machine the ASCS with its own resources and on
the other machine the ERS with its own resources.


=== Five Systems for ASCS, ERS, Database and additional SAP Instances

This guide describes the installation of a distributed {sap} system on the five
systems. In this setup only two systems are in the cluster. The database and
{sap} dialog instances could also be added to the cluster by either adding the
three nodes to the cluster or by installing the database on either of the
nodes. However we recommend to install the database on a separate
cluster.

NOTE: The cluster in this guide only manages the {sap} instances ASCS and ERS,
because of the focus of the {sapCert} certification.

If your database is {sapHana}, we recommend to set up the performance optimized
system replication scenario using our automation solution {sapHanaSR}. The
{sapHanaSR} automation should be set up in an own two node cluster. The setup is
described in a separate best practice available at our best practice page.
https://www.suse.com/products/sles-for-sap/resource-library/sap-best-practices/

.Five systems for the certification setup
image::sles4sap_nw740_5nodes.svg[SVG]

.Clustered machines

*    one machine ({myNode1}) for ASCS; Hostname: {myVipNAscs}
*    one machine ({myNode2}) for ERS; Hostname:   {myVipNErs}

.Non-Clustered machine

*    one machine for DB; Hostname:   {myVipNDb}
*    one machine for PAS; Hostname:   {myVipNPas}
*    one machine for AAS; Hostname:   {myVipNDSec}

=== High Availability for the Database

Depending on your needs you could also increase the availability of the database,
if your database is not already high available by design.

==== {SapHana} System Replication

A perfect enhancement of the five node scenario described in this document is
to implement a {saphana} system replication (SR) automation.

.One cluster for central services, one for {saphana} SR
image::sles4sap_awsnw740_cs+hanasr.svg[SVG]

The following Databases are supported in combination with this scenario:

- SAP HANA DATABASE 1.0
- SAP HANA DATABASE 2.0

=== Integration of {SapNW} into the Cluster using the Cluster Connector

The integration of the HA cluster through the SAP control framework using the
{s4sClConnector} is of special interest. The {SAPSTARTSRV} controls {sap} instances since
{sap} Kernel versions 6.40. One of the classical problems running
{sap} instances in an highly available environment is that if a {sap}
administrator changes the status (start/stop) of a {sap} instance without using
the interfaces provided by the cluster software then the cluster framework will
detect that as an error status and will bring the {sap} instance into the old
status by either starting or stopping the {sap} instance. This can result in
very dangerous situations, if the cluster changes the status of a {sap} instance
during some {sap} maintenance tasks. This new updated solution enables the central component
{SAPSTARTSRV} to report state changes to the cluster software, and therefore avoids the
previously  described dangerous situations.
(See also our blog "Using sap_vendor_cluster_connector for interaction between cluster
framework and sapstartsrv")
(https://blogs.sap.com/2014/05/08/using-sapvendorclusterconnector-for-interaction-between-cluster-framework-and-sapstartsrv/comment-page-1/).

.Cluster connector to integrate the cluster with the {sap} start framework
image::sles4sap_clusterconnector.svg[SVG]

NOTE: For this scenario we are using an updated version of the {s4sClConnector}
which implements the API version 3 for the communication between the cluster
framework and the {sapstartsrv}.

The new version of the {s4sClConnector} now allows to start, stop and 'migrate'
a {sap} instance. The integration between the cluster software and the
{sapstartsrv} also implements to run checks of the HA setup using either the
command line tool sapcontrol or even the {SAP} management consoles ({SAP} MMC or
{sap} MC).

=== Disks and Partitions

For all {sap} file systems beside the EFS file systems
we are using XFS.

==== EFS File Systems for Cluster ASCS and ERS

Create the following sub directories on both cluster nodes as root:

[subs="attributes"]
----
# mkdir -p /usr/sap/HA1/ASCS00 /usr/sap/HA1/ERS10
----

The file systems for the ASCS and ERS instances need to be shared and assigned
to the cluster nodes {myNode1} and {myNode2}. Create an EFS file system.

During the SAP installation we need the filesytems
_/usr/sap/HA1/ASCS00_ to be mounted on
{myNode1} and _/usr/sap/HA1/ERS10_ to be mounted on {myNode2}.

[subs="attributes"]
----
{myNode1}: efs-name:/ASCS00 /usr/sap/HA1/ASCS00
{myNode2}: efs-name:/ERS10 /usr/sap/HA1/ERS10
----

Replace the variable _efs-name_ with the appropriate DNS name of the EFS
filesystem.

NOTE: {myNode1} and {myNode2} operate in different availability zones. They need
mount points named "efs-name" which are individual to the availability
zone. Use the DNS name provided by AWS. The DNS name will point to the
files system mount point local to a given Availability Zone.
During the SAP installation we need {myMpAscs} to be mounted on {myNode1}
and {myMpErs} to be mounted on {myNode2}.

////

==== Disk for DB and Dialog Instances (MaxDB Example)

The disk for the database and primary application server is assigned to
{myNode3}. In an advanced setup this disk should be shared between {myNode3}
and an optional additional node building an own cluster.

* partition one ({myDevPartSbd}) for SBD (7M) - not used here but a reservation
for an optional second cluster
* partition two ({myDevPartDb}) for the Database (60GB) formatted with XFS
* partition three ({myDevPartPas}) for the second file system (10GB) formatted
with XFS
* partition four ({myDevPartSec}) for the third file system (10GB)
formatted with XFS

You could either use YaST to create partitions or using available command line
tools. The following script could be used for non-interactive setups.

[subs="attributes"]
----
# parted -s {myDev} print
# # we are on the 'correct' drive, right?
# parted -s {myDev} mklabel gpt
# parted -s {myDev} mkpart primary 1049k 8388k
# parted -s {myDev} mkpart primary 8389k 60G
# parted -s {myDev} mkpart primary 60G 70G
# parted -s {myDev} mkpart primary 70G 80G
# mkfs.xfs {myDevPartDb}
# mkfs.xfs {myDevPartPas}
# mkfs.xfs {myDevPartSec}
----

.To be mounted either by OS or an optional cluster
- {myNode3}:   {myDevPartDb}   {myMpDb}

- {myNode3}:   {myDevPartPas}   {myMpPas}

- {myNode3}:   {myDevPartSec}   {myMpSec}

NOTE:  {myInstPas} => Since NetWeaver 7.5 the primary application server instance
directory has been renamed. (D<Instance_Number>)


.NFS server
- {myNfsSrv}:{myNFSExpPath}/{mySid}/sapmnt   /sapmnt

- {myNfsSrv}:{myNFSExpPath}/{mySid}/usrsapsys /usr/sap/{mySid}/SYS

.Media
- {myNfsSrv}:{myNFSExpPathSapMedia} /sapcd

or

- {myNfsSrv}:{bsNFSExpPathSapMedia} /sapcd

////

=== IP Addresses and Virtual Names

Check, if the file _/etc/hosts_ contains at least the following address
resolutions. Add those entries, if they are missing.
The 10.0.0.0 addresses in the example below are primary IP addresses within the
VPC CIDR block. The 192.168.201.0 addresses are the Overlay IP addresses for the
virtual services. The listing below lists a virtual IP address for the database
server. A SAP system installation against a virtual database server address
will allow to upgrade the database server to be a protected cluster service in
a later step.

[subs="attributes"]
----
{myIPNode1}  {myNode1}
{myIPNode2}  {myNode2}
{myIPNode3}  {myNode3}
{myVipAPas}  sap{mySidLc}ci
{myVipADSec}  sap{mySidLc}d2
{myVipAAscs}  sap{mySidLc}as
{myVipAErs}  sap{mySidLc}er
{myVipADb}  sap{mySidLc}db

----

=== Mount Points and NFS Shares

In our setup the directory _/usr/sap_ is part of the root file system. You could
of course also create a dedicated file system for that area and mount _/usr/sap_
during the system boot. As _/usr/sap_ also contains the {sap} control file
_sapservices_ and the {saphostagent} the directory should not be placed on a
shared file system between the cluster nodes.

We need to create the directory structure on all nodes which might be able to
run the SAP resource. The SYS directory will be on a NFS share for all nodes.

- Creating mount points and mounting NFS share at all nodes
- Replace _efs-name_ with the appropriate DNS name.

.{sap} {sapNW} 7.4
==============================================
[subs="attributes"]
----
# mkdir -p /sapmnt
# mkdir -p /usr/sap/{mySid}/{{myInstAscs},{myInstDSec},{myInstPas},{myInstErs},SYS}
# mount -t nfs efs-name:/sapmnt /sapmnt
# mount -t nfs efs-name:/SYS    /usr/sap/{mySid}/SYS
# mount -t nfs efs-name:/sapcd /sapcd
----
==============================================

.{sap} {sapNW} 7.5
==============================================
[subs="attributes"]
----
# mkdir -p /sapmnt
# mkdir -p /usr/sap/{mySid}/{{myInstAscs},{bsInstPas},{myInstDSec},{myInstErs},SYS}
# mount -t nfs efs-name:/sapmnt    /sapmnt
# mount -t nfs efs-name:/SYS /usr/sap/{mySid}/SYS
# mount -t nfs efs-name:/sapcd /sapcd
----
==============================================

////

- Only MaxDB:  creating mount points for the database at {myNode3}:

[subs="attributes"]
----
# mkdir -p /sapdb
----

////

- Only HANA: creating mount points for database at {myNode3}:

[subs="attributes"]
----
# mkdir -p /hana/{shared,data,log}
----

////

- Other databases: creating mount points based on there installation guide.

////

////
review Lee means this looks like a 4 node cluster, adding additional title???
////

.File system layout including NFS shares
image::sles4sap_nw740_5nodes.svg[SVG]

We prepare the three servers for the distributed {sap} installation.

** Server 1 ({myNode1}) will be used to install the ASCS {sap} instance.
** Server 2 ({myNode2}) will be used to install the ERS {sap} instance
** Server 3 ({myNode3}) will be used to install the database.
** Server 4 ({myVipNPas}) will be used to install the PAS {sap} instance.
** Server 5 ({myVipNDSec}) will be used to install the AAS {sap} instance.
** Mounting the instance and database file systems at one specific node

////

.{sapNW} 7.40 on x86_64 architecture with MaxDB
=============================================================

[subs="attributes"]
----
(ASCS   {myNode1}) # mount {myDevPartAscs} {myMpAscs}
(ERS    {myNode2}) # mount {myDevPartErs} /usr/sap/{mySid}/{myInstErs}
(DB     {myNode3}) # mount {myDevPartDb} /sapdb
(Dialog {myNode3}) # mount {myDevPartPas} /usr/sap/{mySid}/{myInstPas}
(Dialog {myNode3}) # mount {myDevPartSec} /usr/sap/{mySid}/{myInstDSec}
----
=============================================================


.{sapNW} 7.50 on PowerLE architecture with HANA
=============================================================

[subs="attributes"]
----
(ASCS   {myNode1}) # mount {myDevPartAscs} {myMpAscs}
(ERS    {myNode2}) # mount {myDevPartErs} /usr/sap/{mySid}/{myInstErs}
(DB     {myNode3}) # mount {bsDevPartDbS} /hana/shared
(DB     {myNode3}) # mount {bsDevPartDbL} /hana/log
(DB     {myNode3}) # mount {bsDevPartDbD} /hana/data
(Dialog {myVipNPas}) # mount {myDevPartPas} /usr/sap/{mySid}/{bsInstPas}
(Dialog {myVipNDSec}) # mount {myDevPartSec} /usr/sap/{mySid}/{myInstDSec}
----
=============================================================

////

- As a result the directory _/usr/sap/{mySid}/_ should now look like:

[subs="attributes"]
----
# ls -l /usr/sap/{mySid}/
total 0
drwxr-xr-x 1 {mySidLc}adm sapsys 70 28. Mär 17:26 ./
drwxr-xr-x 1 root   sapsys 58 28. Mär 16:49 ../
drwxr-xr-x 7 {mySidLc}adm sapsys 58 28. Mär 16:49 {myInstAscs}/
drwxr-xr-x 1 {mySidLc}adm sapsys  0 28. Mär 15:59 {myInstDSec}/
drwxr-xr-x 1 {mySidLc}adm sapsys  0 28. Mär 15:59 {myInstPas}/
drwxr-xr-x 1 {mySidLc}adm sapsys  0 28. Mär 15:59 {myInstErs}/
drwxr-xr-x 5 {mySidLc}adm sapsys 87 28. Mär 17:21 SYS/
----

NOTE: The owner of the folder and files are changed during the {sap} installation. By default all
of them are owned by root.

== Testing the AWS Agents
Test the AWS agents before you start up the cluster. The tests will show
whether the AWS role and the policies are being configured correctly. The
tests should excute without AWS CLI errors

=== Test of Overlay IP Agents

Replace the following variables in the commands

- _ip_address_: The service IP addresses of the ASCS and the ERS system
- _rtb-table_ : The name of AWS routing table of the Overlay IP address
- _cluster_ : replace the AWS CLI profile name if needed

The variables will have to match the variables in the OCF primitives
later on!

Run the following commands as root on both systems:

[subs="attributes"]
----
OCF_RESKEY_address=ip_address \
OCF_RESKEY_routing_table=rtb-table \
OCF_RESKEY_interface=eth0 OCF_ROOT=/usr/lib/ocf OCF_RESKEY_profile=cluster \
/usr/lib/ocf/resource.d/suse/aws-vpc-move-ip start

OCF_RESKEY_address=ip_address \
OCF_RESKEY_routing_table=rtb-table \
OCF_RESKEY_interface=eth0 OCF_ROOT=/usr/lib/ocf OCF_RESKEY_profile=cluster \
/usr/lib/ocf/resource.d/suse/aws-vpc-move-ip monitor

OCF_RESKEY_address=ip_address \
OCF_RESKEY_routing_table=rtb-table \
OCF_RESKEY_interface=eth0 OCF_ROOT=/usr/lib/ocf OCF_RESKEY_profile=cluster \
/usr/lib/ocf/resource.d/suse/aws-vpc-move-ip stop
----

Check for AWS CLI access issues and fix the AWS Policy.
Use the AWS console to check whether the IP address got added after _start_.
Use the AWS console to check whether the IP got removed after _stop_.
Use the other cluster node to execute some access commands (ping, ssh etc.)
Recheck and fix all network related settings if this doesn't work.

=== Test Agents mounting EFS File Sytems

Test the monitoring function first.
Replace the following variable in the commands:

- _efs-name_: The name of the EFS filesystem

Run the following commands as root on both cluster nodes:

[subs="attributes"]
----
OCF_RESKEY_device="efs-name:ASCS00" \
OCF_RESKEY_directory="/usr/sap/HA1/ASCS00" OCF_RESKEY_fstype=nfs4 \
OCF_RESKEY_options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
OCF_ROOT=/usr/lib/ocf /usr/lib/ocf/resource.d/heartbeat/Filesystem start

OCF_RESKEY_device="efs-name:ERS10" \
OCF_RESKEY_directory="/usr/sap/HA1/ERS10" OCF_RESKEY_fstype=nfs4 \
OCF_RESKEY_options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
OCF_ROOT=/usr/lib/ocf /usr/lib/ocf/resource.d/heartbeat/Filesystem start

df -k

OCF_RESKEY_device="efs-name:ASCS00" \
OCF_RESKEY_directory="/usr/sap/HA1/ASCS00" OCF_RESKEY_fstype=nfs4 \
OCF_RESKEY_options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
OCF_ROOT=/usr/lib/ocf /usr/lib/ocf/resource.d/heartbeat/Filesystem stop

OCF_RESKEY_device="efs-name:ERS10" \
OCF_RESKEY_directory="/usr/sap/HA1/ERS10" OCF_RESKEY_fstype=nfs4 \
OCF_RESKEY_options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
OCF_ROOT=/usr/lib/ocf /usr/lib/ocf/resource.d/heartbeat/Filesystem stop

df -k
----

Check with the command _df -k_ whether the filesystems got mounted and unmounted.
Potential problems arise with an incorrect _efs-name_ or with missing
subdirectories.

=== Optional: Test of Route 53 Agents

This test should be conducted if the Route 53 agent will be used.
Test the monitoring function first.
Replace the following variables in the commands:

- _hosted zone id_: The ID of the hosted private zones
- _fullname_ : The full name of the service name including sub domain and
a trailing dot.
- _cluster_ : replace the AWS CLI profile name if needed


The variables will have to match the variables in the OCF primitives
later on!

Run the following commands as root on both systems:

[subs="attributes"]
----
OCF_RESKEY_hostedzoneid=hosted zone id OCF_RESKEY_ttl=10 \
    OCF_RESKEY_fullname=fullname OCF_ROOT=/usr/lib/ocf \
    OCF_RESKEY_profile=cluster \
    /usr/lib/ocf/resource.d/heartbeat/aws-vpc-route53 monitor

OCF_RESKEY_hostedzoneid=hosted zone id OCF_RESKEY_ttl=10 \
    OCF_RESKEY_fullname=fullname OCF_ROOT=/usr/lib/ocf \
    OCF_RESKEY_profile=cluster \
    /usr/lib/ocf/resource.d/heartbeat/aws-vpc-route53 start

OCF_RESKEY_hostedzoneid=hosted zone id OCF_RESKEY_ttl=10 \
  OCF_RESKEY_fullname=fullname OCF_ROOT=/usr/lib/ocf \
  OCF_RESKEY_profile=cluster \
  /usr/lib/ocf/resource.d/heartbeat/aws-vpc-route53 stop
----

Fix any problems in monitoring first. Try a start as second test and a stop
as last test.

== SAP Installation

The overall procedure to install the distributed SAP is:

- Installing the ASCS instance for the central services
- Installing the ERS to get a replicated enqueue scenario
- Prepare the ASCS and ERS installations for the cluster take-over
- Installing the Database
- Installing the primary application server instance (PAS)
- Installing additional application server instances (AAS)

The result will be a distributed {sap} installation as illustrated here:

.Distributed installation of the {sap} system
image::sles4sap_awsnw740_distInstall.svg[SVG]

=== Linux User and Group Number Scheme

Whenever asked by the SAP software provisioning manager (SWPM) which Linux User
IDs or Group IDs to use, refer to the following table which is, of course, only
an example.

[subs="attributes"]
----
Group sapinst      1000
Group sapsys       1001
Group sapadm       3000
Group sdba         3002

User  {mysapadm}       3000
User  sdb          3002
User  sqd{mySidLc}       3003
User  sapadm       3004
User  {bsDBadm}       4001
----

=== Install ASCS on {myNode1}

Temporarily we have to set the service IP address we will have later in the
cluster, as local IP because the installer would like to resolve or use it.
Please make sure to use the right virtual hostname for each installation step.
Take care for the file systems like {myDevPartAscs} and /sapcd/ which might also need
to be mounted.

[subs="attributes"]
----
# ip address add {myVipAAscs}{myVipNM} dev eth0
# mount {myDevPartAscs} {myMpAscs}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNAscs}
----

* SWPM option depends on {sapNW} version and architecture
** Installing {sapNW} 7.40 SR2 -> MaxDB -> SAP-Systems ->
Application Server ABAP -> High-Availability System -> ASCS Instance
** Installing {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> ASCS Instance
* SID id {mySid}
* Use instance number {myAscsIno}
* Deselect using FQDN
* All passwords: please use {mySapPwd}
* Double-check during the parameter review, if virtual name *{myVipNAscs}* is
used

=== Install ERS on {myNode2}

Temporarily we have to set the service IP address we will have later in the
cluster, as local IP because the installer would like to resolve or use it.
Please make sure to use the right virtual hostname for each installation step.

[subs="attributes"]
----
# ip address add {myVipAErs}{myVipNM} dev eth0
# mount {myDevPartErs} {myMpErs}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNErs}
----

* SWPM option depends on {sapNW} version and architecture

** Installing {sapNW} 7.40 SR2 -> MaxDB -> SAP-Systems ->
Application Server ABAP -> High-Availability System -> Enqueue Replication
Server Instance

** Installing {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> Enqueue Replication
Server Instance
* Use instance number {myErsIno}
* Deselect using FQDN
* Double-check during the parameter review, if virtual name *{myVipNErs}* is
used
* If you get an error during the installation about permissions, change the
ownership of the ERS directory

[subs="attributes"]
----
# chown -R {mysapadm}:sapsys /usr/sap/{mySid}/{myInstErs}
----

* If you get a prompt to manually stop/start the ASCS instance, please login at
{mynode1} as user {mysapadm} and call sapcontrol.

[subs="attributes"]
----
# sapcontrol -nr {myAscsIno} -function Stop    # to stop the ASCS
# sapcontrol -nr {myAscsIno} -function Start   # to start the ASCS
----

=== Poststeps for ASCS and ERS

==== Stopping ASCS and ERS

_On {myNode1}_

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myAscsIno} -function Stop
# sapcontrol -nr {myAscsIno} -function StopService
----

_On {myNode2}_

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myErsIno} -function Stop
# sapcontrol -nr {myErsIno} -function StopService
----

==== Maintaining _sapservices_

Ensure that the file _/usr/sap/sapservices_ holds both entries (ASCS+ERS) on
both cluster nodes. Modify the file by copying the missing entry from both hosts across. Alternativly add the missing command string with _sapstartsrv_.

Example steps for ASCS profile on the ERS host:

[subs="attributes"]
----
# cd /usr/sap/hostctrl/exe
# export LD_LIBRARY_PATH=.
# ./sapstartsrv pf=/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs} -reg
----

This allows the {sapstartsrv} clients to start the service
like:

_As user {mySapAdm}_

[subs="attributes"]
----
# sapcontrol -nr {myErsIno} -function StartService {mySid}
----

The _/usr/sap/sapservices_ looks like (typically one line per instance):

[subs="attributes"]
----
#!/bin/sh
LD_LIBRARY_PATH=/usr/sap/{mySid}/{myInstAscs}/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/{mySid}/{myInstAscs}/exe/sapstartsrv pf=/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs} -D -u {mySapAdm}
LD_LIBRARY_PATH=/usr/sap/{mySid}/{myInstErs}/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/{mySid}/{myInstErs}/exe/sapstartsrv pf=/usr/sap/{mySid}/{myInstErs}/profile/{mySid}_{myInstErs}_{myVipNErs} -D -u {mySapAdm}
----

==== Integrating the Cluster Framework using the {s4sClConnector} Package

Install the package *sap-suse-cluster-connector* version >3.0.0 from our
repositories:

[subs="attributes"]
----
# zypper install sap-suse-cluster-connector
----

////
NOTE: In future there might be two packages. The package {s4sClConnector}
might contain the old version 1.1.0 (SAP API 1). The package
sap-suse-cluster-connector might contain the new version 3.0.x (SAP API 3).
////

NOTE: The package {s4sClConnector} with version 3.0.x implements the SUSE SAP
API version 3. New features like SAP Rolling Kernel Switch (RKS) and migration of
ASCS are only supported with this new version.

For the ERS and ASCS instance edit the instance profile files
{mySid}_{myInstAscs}_{myVipNAscs} and {mySid}_{myInstErs}_{myVipNErs} in the
profile directory _/usr/sap/{mySid}/SYS/profile/_.

You need to tell the {sapStartSrv} service to load the HA script connector
library and to use the {s4sClConnector}.

[subs="attributes"]
----
service/halib = $(DIR_CT_RUN)/saphascriptco.so
service/halib_cluster_connector = /usr/bin/sap_suse_cluster_connector
----

Add the user {mySapAdm} to the unix user group haclient.

[subs="attributes"]
----
# usermod -a -G haclient {mySapAdm}
----

==== Adapting {sap} Profiles to match the {sapCert} Certification

For the ASCS, change the start command from __Restart_Programm_xx__ to
__Start_Programm_xx__ for the enqueue server (enserver). This change tells the
{sap} start framework *not* to self-restart the enqueue process. Such a restart
would lead in loss of the locks.

.File /usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs}

[subs="attributes"]
----
Start_Program_01 = local $(_EN) pf=$(_PF)
----

Optionally you could limit the number of restarts of services (in the case of
ASCS this limits the restart of the message server).

For the ERS change instance the start command from __Restart_Programm_xx__ to
__Start_Programm_xx__ for the enqueue replication server (enrepserver).

.File /usr/sap/{mySid}/SYS/profile/{mySid}_{myInstErs}_{myVipNErs}

[subs="attributes"]
----
Start_Program_00 = local $(_ER) pf=$(_PFL) NR=$(SCSID)
----

==== Starting ASCS and ERS

_On {myNode1}_

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myAscsIno} -function StartService {mySid}
# sapcontrol -nr {myAscsIno} -function Start
----

_On {myNode2}_

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myErsIno} -function StartService {mySid}
# sapcontrol -nr {myErsIno} -function Start
----

////
=== Install DB on {myNode3} (Example MaxDB)

The MaxDB needs min.40 GB. We use {myDevPartDb} and mount the partition to
_/sapdb_.

[subs="attributes"]
----
# ip address add {myVipADb}{myVipNM} dev eth0
# mount {myDevPartDb} {myMPDb}
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNDb}
----

* We are installing SAP NetWeaver 7.40 SR2 -> MaxDB -> SAP-Systems ->
Application Server ABAP -> High Availability System -> DB
* Profile directory /sapmnt/{mySid}/profile
* DB ID is {mySid}
* Volume Media Type *keep* File (not raw)
* Deselect using FQDN
* Double-check during the parameter review, if virtual name *{myVipNDb}* is
used

////

=== Install DB on {myNode3} (Example SAP HANA)

The HANA DB has very strict HW requirements. The storage sizing depends on many
indicators. Please check the supported configurations on
https://www.sap.com/documents/2015/03/74cdb554-5a7c-0010-82c7-eda71af511fa.html[SAP HANA Hardware Directory]
and https://www.sap.com/documents/2015/03/74cdb554-5a7c-0010-82c7-eda71af511fa.html[SAP HANA TDI].

Install the HANA file systems as being described in the section
http://docs.aws.amazon.com/quickstart/latest/sap-hana/planning.html["Planning the Deployment" of the AWS
SAP HANA on the AWS Cloud: Quick Start Reference Deployment]

Consider to install the database against an Overlay IP address which acts like
a service IP address. This will allow to upgrade the database to run in a SLES
for SAP HAE cluster:


[subs="attributes"]
----
# ip address add {myVipADb}{myVipNM} dev eth0
# mount {bsDevPartDbS} {bsMPDb}/shared
# mount {bsDevPartDbL} {bsMPDb}/log
# mount {bsDevPartDbD} {bsMPDb}/data
# cd /sapcd/SWPM/
# ./sapinst SAPINST_USE_HOSTNAME={myVipNDb}
----

* We are installing {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> Database Instance
* Profile directory /sapmnt/{mySid}/profile
* Deselect using FQDN
* Database parameters enter DBSID is {bsSidDB}; Database Host is {myVipNDb};
Instance Number is {bsDBIno}
* Database System ID enter Instance Number is {bsDBIno}; SAP Mount Directory is
{bsMPDb}/shared
* Account parameters change them in case of custom values needed
* Cleanup select *Yes*, remove operating system users from group'sapinst'....
* Double-check during the parameter review, if virtual name *{myVipNDb}* is
used

=== Install the Primary Application Server (PAS) on {myVipNPas}

Add the following mount points to the _/etc/fstab_ file on host {myVipNPas}. Replace
the string "efs_fs_local_az" with the IP address of your EFS service in your
availability zone.

[subs="attributes"]
----
efs-name:sapcd     /sapcd    nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
efs-name:sapmnt    /sapmnt   nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
efs-name:DVEBMGS01 /usr/sap/HA1/DVEBMGS01  nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
efs-name:SYS       /usr/sap/HA1/SYS        nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
----

Replace the variable _efs-name_ with the appropriate DNS name.

Create mount directories and mount the file systems

[subs="attributes"]
----
# mkdir -p /sapcd /sapmnt /usr/sap/HA1/SYS /usr/sap/HA1/DVEBMGS01
# mount -a
----
Install the PAS server with the _sapinst_ tool.

* SWPM option depends on {sapNW} version and architecture
** Installing SAP NetWeaver 7.40 SR2 -> MaxDB -> SAP-Systems ->
Application Server ABAP -> High-Availability System -> Primary Application Server Instance
(PAS)
** Installing {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> Primary Application Server Instance
(PAS)
* Use instance number {myPasIno}
* Deselect using FQDN
* For our hands-on setup use a default secure store key
* Do not install Diagnostic Agent
* No SLD
* Double-check during the parameter review, if virtual name *{myVipNPas}* is
used

=== Install an Additional Application Server (AAS) on {myVipNDSec}

Add the following mount points to the _/etc/fstab_ file on host {myVipNDSec}. Replace
the string "efs_fs_local_az" with the IP address of your EFS service in your
availability zone.

[subs="attributes"]
----
efs-name:sapcd     /sapcd            nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
efs-name:sapmnt    /sapmnt           nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
efs-name:D02       /usr/sap/HA1/D02  nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
efs-name:SYS       /usr/sap/HA1/SYS  nfs4    rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 0 0
----

Replace the variable _efs-name_ with the appropriate DNS name.
Create mount directories and mount the file systems

[subs="attributes"]
----
# mkdir -p /sapcd /sapmnt /usr/sap/HA1/SYS /usr/sap/HA1/D02
# mount -a
----

Install the AAS server with the _sapinst_ tool.

* SWPM option depends on {sapNW} version and architecture
** Installing SAP NetWeaver 7.40 SR2 -> MaxDB -> SAP-Systems ->
Application Server ABAP -> High-Availability System -> Additional Application Server
Instance (AAS)
** Installing {sapNW} 7.5 -> SAP HANA Database -> Installation ->
Application Server ABAP -> High-Availability System -> Additional Application Server
Instance (AAS)
* Use instance number {myDSecIno}
* Deselect using FQDN
* Do not install Diagnostic Agent
* Double-check during the parameter review, if name *{myVipNDSec}* is
used

== Implement the Cluster

The main procedure to implement the cluster is:

* Install the cluster software, if not already done during the installation of
the operating system
* Configure the cluster communication framework corosync
* Configure the cluster resource manager
* Configure the cluster resources

// * Tune the cluster timing in special for the SBD.

///////////////////////////////
TODO: Do we really need to stop, unconfigure and unmount?
Maybe we find a way to configure the resources that the cluster just
accepts the already started resource groups - lets see ;-)
///////////////////////////////

NOTE: Before we continue to set up the cluster we first stop all SAP instances, remove
the (manual added) IP addresses on the cluster nodes and unmount the file systems
which will be controlled by the cluster later.

////
NOTE: The SBD device/partition need to be created in beforehand. In this setup
guide we already have reserved partition {myDevPartSbd} for SBD usage.
////

.Tasks

. Setup NTP (best with yast2). Use AWS time service at 169.254.169.123 which
is accessible from all EC2 instances. Enable ongoing synchronization.

. Install pattern _ha_sles_ on both cluster nodes

[subs="attributes"]
----
# zypper install -t pattern ha_sles
----

Activate the public cloud module to get updates for the AWS CLI
(Command Line Interface):

[subs="attributes"]
----
# SUSEConnect --list-extensions
# SUSEConnect -p sle-module-public-cloud/12/x86_64
----

Update your packages with the command:

[subs="attributes"]
----
# zypper update
----

=== Configure the Cluster Base

.Tasks

- Install and configure the cluster stack at the first machine

////

You could use either YaST to configure the cluster base or use the interactive
command line tool ha-cluster-init. The following script could be used for
automated setups.

[subs="attributes"]
----
# modprobe softdog
# echo "softdog" > /etc/modules-load.d/softdog.conf
# systemctl enable sbd
# ha-cluster-init -y csync2
# ha-cluster-init -y -i {myHaNetIf} corosync
# ha-cluster-init -y -s {myDevPartSbd} sbd
# ha-cluster-init -y cluster
----

////

==== Configuration of System Logging

SUSE recommends to use rsyslogd for logging with the SUSE cluster.
This is a default configuration. Some AWS AMIs however use syslogd logging.
Please perform the following commands as root on all cluster nodes:

[subs="attributes"]
----
# zypper install rsyslog
----
Use option 1 (deinstallation of competing software, syslogd).
Reboot both nodes.

==== Corosync Configuration

===== Configuration of the _corosync.conf_ File

The configuration will have an IP address
for node node-1 which is supposed to be ip-node-1. Node node-2 has an ip address to
which we refer as ip-node-2.

All cluster nodes are required to have a local configuration
file _/etc/corosync/corosync.conf_ which will be structured as follows.

The relevant information is being located in the two sections describing
interface and nodelist. The other entries can be configured as needed for a
specific implementation.

NOTE: AWS requires a specific manual corosync configuration.

Use the following configuration in the _/etc/corosync/corosync.conf_ file
on both cluster nodes:

[subs="attributes"]
----
# Please read the corosync.conf.5 manual page
totem {
  version: 2
  token: 5000
  consensus: 7500
  token_retransmits_before_loss_const: 6
  crypto_cipher: none
  crypto_hash: none
  clear_node_high_bit: yes
  interface {
    ringnumber: 0
    bindnetaddr: &lt;ip-local-node&gt;
    mcastport: 5405
    ttl: 1
  }
  transport: udpu
}
 logging {
      fileline: off
      to_logfile: yes
      to_syslog: yes
      logfile: /var/log/cluster/corosync.log
      debug: off
      timestamp: on
      logger_subsys {
         subsys: QUORUM
         debug: off
     }
}
nodelist {
  node {
  ring0_addr: &lt;ip-node-1&gt;
  nodeid: 1
  }
  node {
  ring0_addr: &lt;ip-node-2&gt;
  nodeid: 2
  }
}

quorum {
  # Enable and configure quorum subsystem (default: off)
  # see also corosync.conf.5 and votequorum.5
  provider: corosync_votequorum
  expected_votes: 2
  two_node: 1
}

----

Replace the variables _ip-node-1_ and _ip-node-2_ with the IP addresses of your
two cluster instances. Replace _ip-local-node_ with the IP address of the server
the file is being created.

The chosen settings for __crypto_cipher__ and __crypto_hash__ are suitable for
clusters in AWS. They may be modified according to SUSE's documentation
if strong encryption of cluster communication is desired.

==== Starting the Cluster

The next step is to start the cluster with the command on both nodes:

[subs="attributes"]
----
# systemctl start pacemaker
----

==== Checking the Configuration

The configuration can be checked with the command:

[subs="attributes"]
----
# corosync-cfgtool -s
----

It'll create a result like the following one for a cluster node with the
IP address 10.0.0.111:

[subs="attributes"]
----
Printing ring status.
Local node ID 1
RING ID 0
id = 10.0.0.111
status = ring 0 active with no faults
----

The cluster in question has been using ring 0, the node had the ID 1.

- The _crm_mon -1_ output should look like this:

[subs="attributes"]
----
Stack: corosync
Current DC: hacert01 (version 1.1.15-19.15-e174ec8) - partition with quorum
Last updated: Wed Dec  6 16:02:42 2017
Last change: Wed Dec  6 15:44:45 2017 by hacluster via crmd on hacert01

2 nodes configured
0 resources configured

Online: [ hacert01 hacert02 ]

Full list of resources:
----

=== Configure Cluster Resources

We need a changed SAPInstance resource agent for {sapNw} in order *not* to use
the Master-Slave construct anymore and move to a more cluster like construct to
start and stop the ASCS and the ERS itself and *not* only the complete
master-slave.

////////////////////////////////////
TODO: We need to adapt this section, if the new enqueue2 is published.
////////////////////////////////////

For this there is a new functionality for the ASCS needed to follow the ERS.
The ASCS needs to mount the shared memory table of the ERS to avoid the loss of
locks.

.Resources and constraints
image::sles4sap_nw740_resources.svg[SVG]

The implementation is done with the help of a new flag "runs_ers_$SID" within
the RA, enabled with help of the resource parameter "IS_ERS=TRUE".

There is the option to add a Route 53 agent. The architecture will then look
as follows:

.Resources and constraints
image::sles4sap_nw740_awsr53_resources.svg[SVG]

==== Preparing the Cluster for adding the Resources

To avoid that the cluster starts partially defined resources we set the cluster
to the maintenance mode. This deactivates all monitor actions.

_As user root_

[subs="attributes"]
----
# crm configure property maintenance-mode="true"
----

==== Configure AWS specific Settings

Execute the following commands on one of the two cluster nodes:

[subs="attributes"]
----
# vi crm-bs.txt
----

Enter the following information to the file _crm-bs.txt_:

[subs="attributes"]
----
property cib-bootstrap-options: \
    stonith-enabled="true" \
    stonith-action="poweroff" \
    stonith-timeout="600s"
rsc_defaults rsc-options: \
	resource-stickiness=1 \
	migration-threshold=3
op_defaults op-options: \
	timeout=600 \
	record-pending=true
----

The setting _poweroff_ forces the agents to shutdown the instance.
This is desirable in order to avoid split brain scenarios on AWS.

Add the configuration to the cluster:

[subs="attributes"]
----
# crm configure load update crm-bs.txt
----

==== Configuration of AWS specific Stonith Resource

Create a file with the following content:

[subs="attributes"]
----
primitive res_AWS_STONITH stonith:external/ec2 \
op start interval=0 timeout=180 \
op stop interval=0 timeout=180 \
op monitor interval=120 timeout=60 \
params tag=pacemaker profile=cluster
----

The EC2 tag _pacemaker_ entry needs to match the tag chosen for
the EC2 instances. The value for this tag will contain the host name.
The name of the profile (_cluster_ in this example) will have to match the
previously configured AWS profile.

Name this file for example _aws-stonith.txt_ and add this file to the
configuration. The following command has to be issued as root.
It uses the file name _aws-stonith.txt_:

[subs="attributes"]
----
# crm configure load update aws-stonith.txt
----

==== Configure the Resources for the ASCS

First we configure the resources for the file system, IP address and the {sap}
instance. Of course you need to adapt the parameters to your environment.

Create a file with your editor of choice with a name _aws-ascs.txt_. Add the
ASCS primitive and the ASCS group to it. Don't forget to save your changes.

.ASCS primitive
================================================
[subs="attributes"]
----
primitive rsc_fs_{mySID}_{myInstAscs} Filesystem \
    params  device="{myDevPartAscs}" directory="/usr/sap/{mySID}/{myInstAscs}" \
            fstype="nfs4" \
            options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
    op start timeout=60s interval=0 \
    op stop timeout=60s interval=0 \
    op monitor interval=200s timeout=40s
primitive rsc_ip_{mySID}_{myInstAscs} ocf:suse:aws-vpc-move-ip \
    params  address={myVipAAscs} routing_table=rtb-table \
            interface=eth0 profile=cluster \
    op start interval=0 timeout=180 \
    op stop interval=0 timeout=180 \
    op monitor interval=60 timeout=60
primitive rsc_sap_{mySID}_{myInstAscs} SAPInstance \
    operations $id=rsc_sap_{mySID}_{myInstAscs}-operations \
    op monitor interval=120 timeout=60 on_fail=restart \
    params InstanceName={mySID}_{myInstAscs}_{myVipNAscs} \
        START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstAscs}_{myVipNAscs}" \
        AUTOMATIC_RECOVER=false \
    meta resource-stickiness=5000 failure-timeout=60 \
        migration-threshold=1 priority=10
----
================================================

Replace the variable _efs-name_ with the name of your EFS server.

Replace the variable _rtb-table_ with the identifier of the appropriate AWS
routing table for the subnets. The name of the AWS CLI profile (_cluster_ in this
example) will have to match the previously configured  AWS profile.

.ASCS group
================================================
[subs="attributes"]
----
group grp_{mySID}_{myInstAscs} \
  rsc_ip_{mySID}_{myInstAscs} rsc_fs_{mySID}_{myInstAscs} rsc_sap_{mySID}_{myInstAscs} \
	meta resource-stickiness=3000
----
================================================

Create a txt file _aws_ascs.txt_ with your preferred text editor, enter
both examples (primitives and group) to that file and load the configuration to
the cluster manager configuration.

[subs="attributes"]
----
# crm configure load update aws_ascs.txt
----

==== Optional: including Route  53

Name this file for example _aws-route53.txt_ and add this file to the
configuration. The following command has to be issued as root.
It uses the file name _aws-route53.txt_:

Enter the following primitive before or after the existing primitives in the
editor:

.ROUTE53 primitive
================================================
[subs="attributes"]
----
primitive rsc_r53_HA1_ASCS00 ocf:heartbeat:aws-vpc-route53 \
   params hostedzoneid=route-53-name ttl=10 fullname=name-full. profile=cluster \
   op start interval=0 timeout=180 \
   op stop interval=0 timeout=180 \
   op monitor interval=300 timeout=180
----
================================================

Replace the variable _route-53-name_ with the name of the associated private
hosted Route 53 zone.

Replace the variable _name-full._ will the fully qualified host name
with matches the private hosted Route 53 zone.

The agent uses a time-to-live (ttl) of 10 seconds in this example.
Change this parameter if needed.

Insert the __rsc_r53_{mySID}_{myInstAscs}__ after the
__rsc_ip_{mySID}_{myInstAscs}__. This will force the group to update
then Route 53 as second item after the Overlay IP address.

.ROUTE53 group
================================================
[subs="attributes"]
----
group grp_{mySID}_{myInstAscs} \
  rsc_ip_{mySID}_{myInstAscs} rsc_r53_{mySID}_{myInstAscs} \
  rsc_fs_{mySID}_{myInstAscs} rsc_sap_{mySID}_{myInstAscs} \
	meta resource-stickiness=3000
----

Create a txt file _aws-route53.txt_ with your preferred text editor, enter
both examples (primitives and group) to that file and load the configuration to
the cluster manager configuration.
Use the following command as root and modify ASCS group in the editor.

[subs="attributes"]
----
# crm configure load update aws-route53.txt
----

================================================

NOTE: Version 1.0.2 of the Route 53 agent
will not work if the EC2 metadata contains a
string like "local-ipv4" in the userdata section!

==== Configure the Resources for the ERS

Second we configure the resources for the file system, IP address and the {sap}
instance. Of course you need to adapt the parameters to your environment.

Replace _efs-name_ with the name of your EFS server.

The specific parameter __IS_ERS=true__ should only be set for the ERS instance.

.ERS primitive
================================================
[subs="attributes"]
----
primitive rsc_fs_{mySID}_{myInstErs} Filesystem \
  params device="{myDevPartErs}" directory="/usr/sap/{mySID}/{myInstErs}" fstype=nfs4 \
  options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
  op start timeout=60s interval=0 \
  op stop timeout=60s interval=0 \
  op monitor interval=200s timeout=40s
primitive rsc_ip_{mySID}_{myInstErs} ocf:suse:aws-vpc-move-ip \
  params address={myVipAErs} routing_table=rtb-table \
  interface=eth0 profile=cluster \
  op start interval=0 timeout=180 \
  op stop interval=0 timeout=180 \
  op monitor interval=60 timeout=60
primitive rsc_sap_{mySID}_{myInstErs} SAPInstance \
  operations $id=rsc_sap_{mySID}_{myInstErs}-operations \
  op monitor interval=120 timeout=60 on_fail=restart \
  params InstanceName={mySID}_{myInstErs}_{myVipNErs} \
         START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstErs}_{myVipNErs}" \
         AUTOMATIC_RECOVER=false IS_ERS=true \
  meta priority=1000
----
================================================
Replace the variable _rtb-table_ with the identifier of the appropriate AWS
routing table for the subnets. The name of the AWS CLI profile (_cluster_
in this example) will have to match the previously configured  AWS profile.

.ERS group
================================================
[subs="attributes"]
----
group grp_{mySID}_{myInstErs} \
  rsc_ip_{mySID}_{myInstErs} rsc_fs_{mySID}_{myInstErs} rsc_sap_{mySID}_{myInstErs}
----
================================================

Create a txt file (like __aws_crm_ers.txt__) with your preferred text editor, enter
both examples (primitives and group) to that file and load the configuration to
the cluster manager configuration.

_As user root_

[subs="attributes"]
----
# crm configure load update aws_crm_ers.txt
----

==== Configure the Colocation Constraints between ASCS and ERS

The constraints between the ASCS and ERS instance are needed to define that the
ASCS instance should start-up exactly on the cluster node running the ERS
instance after a failure (loc_sap_{mysid}_failover_to_ers). This constraint is
needed to ensure that the locks are not lost after a ASCS instance (or node)
failure.

If the ASCS instance has been started by the cluster the ERS instance should
be moved to an "other" cluster node (col_sap_{mysid}_no_both). This constraint
is needed to ensure that the ERS will sync the locks again and the cluster is
ready for an additional take-over.

.Location constraint
================================================
[subs="attributes"]
----
colocation col_sap_{mysid}_no_both -5000: grp_{mySID}_{myInstErs} grp_{mySID}_{myInstAscs}
location loc_sap_{mysid}_failover_to_ers rsc_sap_{mySID}_{myInstAscs} \
         rule 2000: runs_ers_{mySID} eq 1
order ord_sap_{mysid}_first_start_ascs Optional: rsc_sap_{mySID}_{myInstAscs}:start \
      rsc_sap_{mySID}_{myInstErs}:stop symmetrical=false
----
================================================

Create a text file (like _crm_col.txt_) with your preferred text editor, enter
all three constraints to that file and load the configuration to the
cluster manager configuration.

Issue the following command as root:

[subs="attributes"]
----
# crm configure load update crm_col.txt
----

==== Activating the Cluster

Now the last step is to end the cluster maintenance mode and to allow the
cluster to detect already running resources.

Issue the following command as root:

[subs="attributes"]
----
# crm configure property maintenance-mode="false"
----

The cluster will now start the ASCS and the ERS system. This can can a few
minutes. Check progress with the command:

[subs="attributes"]
----
# crm status
----

////
############## REMOVING THIS FROM THE OUTPUT FOR NOW ###########

=== Installing SAP Licenses

Most likely you have your own established method to maintain your {sap} licenses.
This section is only a reminder. You should have installed two license keys as
we need to failover the ASCS {sap} instance.

- Get the HWKEY of both cluster nodes
- Get the license from the SAP launchpad
- Install the two licenses per HWKEY using transaction SLICENSE

###################
////

////
############## REMOVING THIS FROM THE OUTPUT FOR NOW ###########

=== Optional - Installing the HA Test Tool HATool

This is an optional task. If you like to check your cluster using {sap}s HATool
you might find these notes helpful. Please always follow {SAP}s documentation
how to install and use the HATool.

We install the test Client at {myNode3}. The server part is imported
as a SAP Transport. In our environment the {sap} dialog instance is also running
at {myNode3}, so we also store the transport files there.

- Installing the HATool Client (as root)

[subs="attributes"]
----
# cd /root/herschel
# unzip HATool_v213.zip
(creates directories HATool/{bin,check,config,doc,event,server)
----

- Installing other sw parts of the HATool Client (as root).

See also sections 3.2 and 4.2 of the "High Availability Test Tool - Installation
& Operations Guide"

[subs="attributes"]
----
# cd /root/herschel
##
## SAP JVM8 - we skipped that part, because we have Oracle VM8
##
#######
##
## SAP JCO 3.0
##
# mkdir SAPJCO; cd SAPJCO
# Download from Marketplace
# unzip mkdir SAPJCOmkdir SAPJCO
# tar -xvzf sapjco3-linuxx86_64-3.0.16.tgz
# unzip sapjco30P16HF_1-10009485.zipsapjco30P16HF_1-10009485.zip
# ln -s $PWD/sapjco3.jar /root/herschel/HATool/bin
# ln -s $PWD/libsapjco3.so /root/herschel/HATool/bin
##
## d3.min.js - graphic library
##
# mkdir d3.min.js
# wget https://d3js.org/d3.v3.min.js
# mv d3.v3.min.js d3.min.js/d3.min.js
# ln -s /root/herschel/d3.min.js/d3.v3.min.js \
      /root/herschel/HATool/bin/d3.min.js
----

- Importing the HATool server part into the SAP system.
This is described in section 4.3 in the "High Availability Test Tool -
Installation & Operations Guide".

- Create and configure user hatool.
This is described in section 4.3 in the "High Availability Test Tool -
Installation & Operations Guide".

- Set up the first property file for a smoke test.
In directory /root/herschel/HATool/config change the file
haQuickStartTemplate.properties like the following patch does. So setting
systemclient, user, password, messagehost and SID.

[subs="attributes"]
----
 --- haQuickStartTemplate.properties     2015-10-14 14:50:48.000000000 +0200
 +++ FHhaQuickStartTemplate.properties   2017-04-24 20:43:41.117512210 +0200
 @@ -53,20 +53,20 @@ resettestdata = 1
  ###################################
  ########## login information for server user ###########
 -systemclient = nnn
 -user = xxxxxxxxxx
 -password = xxxxxxxxx
 +systemclient = 001
 +user = hatool
 +password = SuSE1234
  language = en
  ################# RFC ###################
  # login via message server
  -msgserverhost = xxxxxxxxxx
  +msgserverhost = {myVipNAscs}
  # the port information is necessary, if there is no entry for sapms systemid in /etc/services resp. C:\Windows\System32\drivers\etc\services
  #msgserverport = 3600
  # for the logon group, PUBLIC is used by default; if no explicit group was created, use SPACE
  logongroup = SPACE
  # if msgserverport is not set, systemid is mandatory
 -systemid = SID
 +systemid = {mySid}
 ####################
 #   One HA Event   #
----

- Testrun
[subs="attributes"]
 java -cp haTestTool.jar:sapjco3.jar com.sap.test.haload.ClientDriver \
     file=FHhaQuickStartTemplate.properties

- Adapting the test properties
This is described in section 3.2 of the "SAP Application Server HA Interface
Certification" guide.

** Login.properties - set systemclient, user, password, messagehost and SID
** TEC05Event.properties - set eventcall to TEC05Event.sh
** TEC14Event.properties - set eventcall to TEC14Event.sh
** TEC05Event.sh - edit the _sapcontrol_ command

[subs="attributes"]
----
 /usr/sap/{mySid}/SYS/exe/uc/linuxx86_64/sapcontrol -prot NI_HTTP \
   -user {mySapAdm} SuSE1234 \
   -host {myVipNAscs} -nr {myAscsIno} -function HAFailoverToNode ""
----
** TEC14Event.sh - edit the sapctrl command

[subs="attributes"]
----
 /usr/sap/{mySid}/SYS/exe/uc/linuxx86_64/sapcontrol -prot NI_HTTP \
   -user {mySapAdm} SuSE1234 \
   -host {myVipNAscs} -nr {myAscsIno} -function UpdateSystem 120 300 1
----

NOTE: Maybe we need to change the rks-host (here {myVipNAscs}) after we get
feedback from SAP which instance should be referenced,

##############
////

////
:leveloffset: 2
include::MaxdbStudio.txt[]

include::MaxdbResizeDB.txt[]

:leveloffset: 0
////

////
############################
#
# ADMINISTRATION
#
############################
////

== Administration

=== Do and Don't Do

==== Never stop the ASCS Instance

For normal operation *do not stop* the ASCS {sap} instance with any tool such
as cluster tools or {sap} tools. The stop of the ASCS instance might lead to a loss of enqueue
locks. Because following the new {sapCert} certification the cluster must allow local restarts
of the ASCS. This feature is needed to allow rolling kernel switch (RKS) updates without
reconfiguring the cluster.

WARNING: Stopping the ASCS instance might lead into the loss of {sap} enqueue
  locks during the start of the ASCS on the same node.

==== How to migrate ASCS

To *migrate* the ASCS {sap} instance you should use the {sap} tools such as
  the {sap} management console. This will trigger {sapStartSrv} to use the
  {s4sClConnector} to migrate the ASCS instance. As user _{mysapadm}_ you might call
  the following command to migrate-away the ASCS. The migrate-away will always
  migrate the ASCS to the ERS side which will keep the {sap} enqueue locks.

_As {mysapadm}_

[subs="attributes"]
----
# sapcontrol -nr 00 -function HAFailoverToNode ""
----

==== Never Block Resources

With {sapCert} it is *not longer allowed to block resources* from beeing
  controlled manually. This using the variable __BLOCK_RESOURCES__ in
  __/etc/sysconfig/sap_suse_cluster_connector__ is not allowed any more.

==== Always use Unique Instance Numbers

Currently all {sap} *instance numbers controlled by the cluster must be unique*.
  If you need to have multiple dialog instances such as D00 running on different
  systems they should be not controlled by the cluster.

==== How to set Cluster in Maintenance Mode

Procedure to set the cluster into maintenance mode can be done as _root_ or _sidadm_.

_As user root_

[subs="attributes"]
----
# crm configure properties maintenance-mode="true"
----

_As user {mysapadm} (the full path is needed)_

[subs="attributes"]
----
# /usr/sbin/crm configure properties maintenance-mode="true"
----

==== Procedure to End the Cluster Maintenance

_As user root_

[subs="attributes"]
----
# crm configure properties maintenance-mode="false"
----

==== Cleanup Resources

How to *cleanup resource failures*? Failures of the ASCS will be automatically
  deleted to allow a failback after the configured period of time. For all other
  resources you can cleanup the status including the failures:

_As user root_

[subs="attributes"]
----
# crm resource cleanup RESOURCE-NAME
----

WARNING: You should not cleanup the complete group of the ASCS resource as this
   might lead into an unwanted cluster action to take-over the complete group to
   the node where ERS instance is running.

=== Testing the Cluster

We strongly recommend that you at least process the following tests before you
plan going into production with your cluster.

==== Check Product Names with HAGetFailoverConfig

Check if the name of the SUSE cluster solution is shown in the output of
  sapcontrol or {sap} management console. This test checks the status of the
  {sapNW} cluster integration.


_As user {mysapadm}_

[subs="attributes"]
----
# sapcontrol -nr 00 -function HAGetFailoverConfig
----

==== Start SAP Checks using HACheckConfig and HACheckFailoverConfig

Check if the HA configuration tests are showing no errors.

_As user {mysapadm}_

[subs="attributes"]
----
# sapcontrol -nr 00 -function HACheckConfig
# sapcontrol -nr 00 -function HACheckFailoverConfig
----

==== Manually migrate ASCS

Check if manually migrating the ASCS using HA tools works properly.

_As user root_

[subs="attributes"]
----
# crm resource migrate rsc_sap_{mySid}_{myInstAscs} force
## wait till the ASCS is been migrated to the ERS host
# crm resource unmigrate rsc_sap_{mySid}_{myInstAscs}
----

==== Migrate ASCS using HAFailoverToNode

Check if moving the ASCS instance using {sap} tools like {sapCtrl} does work
properly.

_As user {mysapadm}_

[subs="attributes"]
----
# sapcontrol -nr 00 -function HAFailoverToNode ""
----

==== Test ASCS Migration after Failure

Check if the ASCS instance moves correctly after a node failure.

_As user root_

[subs="attributes"]
----
## on the ASCS host
# echo b >/proc/sysrq-trigger
----

==== Inplace Restart of ASCS using Stop and Start

Check if the inplace re-start of the {sap} resources have been processed
  correctly. The {sap} instance should not failover to an other node, it
  must start on the same node where it has been stopped.

WARNING: This test will force the SAP system to *lose* the enqueue locks.
   *This test should not be processed during production.*

_As user {mysapadm}_

[subs="attributes"]
----
## example for ASCS
# sapcontrol -nr 00 -function Stop
## wait till the ASCS is completly down
# sapcontrol -nr 00 -function Start
----

==== Additionally you should test

* Automated restart of the ASCS (simulating RKS)

* Check the recoverable and non-recoverable outage of the message server process

* Check the non-recoverable outage of the {sap} enqueue server process

* Check the outage of the {sapERS}

* Check the outage and restart of {sapStartSrv}

* Check the rolling kernel switch procedure (RKS), if possible

* Check the simulation of an upgrade

* Check the simulation of cluster resource failures

:leveloffset: 0

== AWS specific Post Installation Tasks

The optional installation of the Route 53 agent will update the DNS name of
the central instance as needed.
The Route 53 naming tables may have to be made visible for on premises users
like SAP GUI users. This happens through updating the on-premises name
servers to delegate name resolution to Route 53.
This forwarding of name resolution requests acquiresan extra configuration in
the AWS VPC.

Active directory users will have to configure an Active Directory Connector as
described in [https://aws.amazon.com/de/blogs/security/how-to-set-up-dns-resolution-between-on-premises-networks-and-aws-using-aws-directory-service-and-amazon-route-53/] .

DNS server users will have to implement bind forwarding EC2 instances as
described in [http://www.scalingbits.com/aws/dnsfailover/backpropagation].


== Additional Implementation Scenarios

=== Adaptive Server Enterprise Replication Failover Automation Integration

==== FM Integration with SUSE HAE Cluster

Standard SAP on AWS for an HA setup is Multi-AZ deployment with ASCS, Primary DB running in
one AZ and their counterpart ERS and Secondary DB running in the second AZ of the same region.
The Primary Application Server & Additional Application servers based on the load can be distributed
in both AZ’s as well to provide resiliency.
Considering a scenario where SAP NetWeaver or Business Suite system is running on SAP Sybase ASE.
The completely automated HA for the ABAP Stack (ASCS) is provided by the SUSE HAE Cluster. For
the Sybase ASE DB the HA feature is provided with the Always On configuration and the failover
orchestration is done by the Fault Manager (FM) utility which traditionally was installed on a third Host
(other than the Primary & Secondary DB). In an SAP world the FM utility comes along with SAP DB
dependent Kernel and gets installed in the ASCS Work directory _/usr/sap/<SID>/ASCS<instnr>/exe/_. The
failover of the ASCS instance along with the associated directories (provided they are installed on a
shared file system using either Amazon EFS or NFS) is taken care by the SUSE HAE Cluster add-on.

==== Sybase ASE Always On

SAP Sybase ASE comes with an Always On feature which provides native HA & DR capability. The
always-on option is a high-availability and disaster recovery (HADR) system that consists of two
SAP ASE servers: one designated as the primary, on which all transaction processing takes place; the
other acts as a warm standby (referred to as a "standby server" in DR mode, and as a "companion" in
HA mode) for the primary server, and contains copies of designated databases from the primary server.
The failover orchestration is carried out by ASE provided utility called Fault Manager. The Fault
Manager monitors the various components of the HADR environment – Replication Management Agent
(RMA), ASE, Replication Server, applications, databases, and the operating system. Its primary job is
to ensure the high-availability (zero data loss during failover) of the ASE cluster by initiating automatic
failover with minimal manual intervention. In a SAP Stack, the fault manager utility (sybdbfm) comes
as part of the DB (Sybase ASE) dependent SAP Kernel.
Refer to the SAP Standard ASE HA-DR guide (https://help.sap.com/viewer/efe56ad3cad0467d837c8ff1ac6ba75c/16.0.3.6/en-US/a6645e28bc2b1014b54b8815a64b87ba.html)
for setting up the Sybase ASE DB in HA mode.

==== Database Host Preparation

This guide will not duplicate the official HADR documentation. The following procedure will describe
key points which have to take care for.

.Installation 32-bit Environment

[subs="specialchars,attributes,quotes"]
----
# zypper install glibc-32bit libgcc_s1-32bit
----

For the example this software stack is used:

* SL TOOLSET 1.0 -- SWPM -> 1.0 SP25 for NW higher than 7.0x
* saphostagent -> 7.21 patch 41
* SAP Kernel -> 7.53 PL400
* SAP Installation Export ->  (51051806_1)
* Sybase RDBMS->  ASE 16.0.03.06 RDBMS (51053561_1)

NOTE: Very useful is that short table of installation information which helps to be prepared for the next steps.
SAP Adaptive Server Enterprise - Installation Worksheet
https://help.sap.com/viewer/efe56ad3cad0467d837c8ff1ac6ba75c/16.0.3.6/en-US/3fe35550f3814b2bb411d5494976e25a.html

==== Database Installation for Replication Scenario

The installation can be done with the SWPM which is provided by SAP.

.Installing the primary database with SWPM:
* Software Provisioning Manager 1.0 SP 25
** SAP NetWeaver AS for ABAP 7.52
*** SAP ASE
**** Installation
***** Application Server ABAP
****** High-Availability System
******* Database Instance

The following information are requested from the wizard:

* Master Password <secure>
* SAP System Code Page: Unicode (default)
* uncheck: -> Set FQDN for SAP system
* sybase database Administrator UID: 2003
* in our test setup we uncheck --> Use separate devices for sybmgmtdb database

After the basis installation is finished the primary database must be prepared for the replication. First
the user *sa* must be unlocked.

[subs="specialchars,attributes,quotes"]
----
# su - syb<sid>
# isql -Usapsso -P <secure password> -S<SID> -X -w1900
# 1> go
# 1> exec sp_locklogin sa, 'unlock'
# 2> go
# Account unlocked.
# (return status = 0)
# 1> exit
----

Next step installing the SRS software with a response file:

[subs="specialchars,attributes,quotes"]
----
# /sapcd/ase-16.0.03.06/BD_SYBASE_ASE_16.0.03.06_RDBMS_for_BS_/SYBASE_LINUX_X86_64/setup.bin -f /sybase/WAS/srs-setup.txt -i silent
----

Activate HADR on primary node with a response file:

[subs="specialchars,attributes,quotes"]
----
# setuphadr /sybase/WAS/WAS_primary_lin.rs.txt
----

Creating a secure store key entry for the database:

[subs="specialchars,attributes,quotes"]
----
# /usr/sap/hostctrl/exe/saphostctrl -user sapadm <secure password> -function LiveDatabaseUpdate -dbname <SID> -dbtype syb -dbuser DR_admin -dbpass <Secure password> -updatemethod Execute -updateoption TASK=SET_USER_PASSWORD -updateoption USER=DR_ADMIN
----


.Installing the companion database with SWPM:
* Software Provisioning Manager 1.0 SP 25
** SAP NetWeaver AS for ABAP 7.52
*** SAP ASE
**** Database Replication
***** Setup of Replication Environment

The following information are requested from the wizard:

* Replication System Parameters --> SID, Master Passsword, check Set up a secondary database instance
* Primary Database server --> hostname or virt. name
* Primary Database server port --> default is 4901, depends on the setup of your primary server

After the basis installation is finished the companion database must be prepared for the replication. First
the user *sa* must be unlocked.

[subs="specialchars,attributes,quotes"]
----
# su - syb<sid>
# isql -Usapsso -P <secure password> -S<SID> -X -w1900
# 1> go
# 1> exec sp_locklogin sa, 'unlock'
# 2> go
# Account unlocked.
# (return status = 0)
# 1> exit
----

Next step installing the SRS software with a response file on the companion site:

[subs="specialchars,attributes,quotes"]
----
# /sapcd/ase-16.0.03.06/BD_SYBASE_ASE_16.0.03.06_RDBMS_for_BS_/SYBASE_LINUX_X86_64/setup.bin -f /sybase/WAS/srs-setup.txt -i silent
----

Activate HADR on companion node with a response file:

[subs="specialchars,attributes,quotes"]
----
# setuphadr /sybase/WAS/WAS_primary_lin.rs.txt
----

Creating a secure store key entry for the database:

[subs="specialchars,attributes,quotes"]
----
# /usr/sap/hostctrl/exe/saphostctrl -user sapadm <secure password> -function LiveDatabaseUpdate -dbname <SID> -dbtype syb -dbuser DR_admin -dbpass <Secure password> -updatemethod Execute -updateoption TASK=SET_USER_PASSWORD -updateoption USER=DR_ADMIN
----

==== Fault Manager Installation

The Fault Manager is configured on the ASCS host. The benefit from this setup is that the sybdbfm service
can be monitored and tracked with the existing pacemaker for the ASCS / ERS replication.

[subs="specialchars,attributes,quotes"]
----
# su - <sid>adm
# cd /usr/sap/<SID>/ASCS00/exe/
# sybdbfm install
----

.This look like this example:
====
[subs="specialchars,attributes,quotes"]
----
replication manager agent user DR_admin and password set in Secure Store.
Keep existing values (yes/no)? (yes)
SAPHostAgent connect user sapadm and password set in Secure Store.
Keep existing values (yes/no)? (yes)
Enter value for primary database host: (suse7cl1)
suse7db1
Enter value for primary database name: (HA7)
Enter value for primary database port: (4901)
Enter value for primary site name: (Site1)
Enter value for primary database heart beat port: (13777)
Enter value for standby database host: (suse7cl1)
suse7db2
Enter value for standby database name: (HA7)
Enter value for standby database port: (4901)
Enter value for standby site name : (Site2)
Enter value for standby database heart beat port: (13787)
Enter value for fault manager host: (s7as-service)
Enter value for heart beat to heart beat port: (13797)
Enter value for support for floating database ip: (no)
Enter value for use SAP ASE Cockpit if it is installed and running: (no)
----
====

Update the values as per your environment for the Primary DB & companion DB hostname, SID &
Site Name. Make sure to use the virtual hostname for the ASCS host. Once the Fault Manager is installed,
profile for it will be created in the _/sapmnt/<SID>/profile_ by the name _SYBHA.PFL_ and will have the
configuration details.
Restart the ASCS Instance which will also start the Fault Manager that has been added to the start profile as below:

[subs="specialchars,attributes,quotes"]
----
# cat /sapmnt/<SID>/profile/<SID>_ASCS00_<virt. ASCS hostname>
....
#-----------------------------------------------------------------------
# copy sybdbfm and dependent
#-----------------------------------------------------------------------
_CP_SYBDBFM_ARG1 = list:$(DIR_CT_RUN)/instancedb.lst
Execute_06 = immediate $(DIR_CT_RUN)/sapcpe$(FT_EXE) pf=$(_PF) $(_CP_SYBDBFM_ARG1)
_CP_SYBDBFM_ARG2 = list:$(DIR_GLOBAL)/syb/linuxx86_64/cpe_sybodbc.lst
_CP_SYBDBFM_ARG3 = source:$(DIR_GLOBAL)/syb/linuxx86_64/sybodbc
Execute_07 = immediate $(DIR_CT_RUN)/sapcpe$(FT_EXE) pf=$(_PF) $(_CP_SYBDBFM_ARG2) $(_CP_SYBDBFM_ARG3)
#-----------------------------------------------------------------------
# Start sybha
#-----------------------------------------------------------------------
_SYBHAD = sybdbfm.sap$(SAPSYSTEMNAME)_$(INSTANCE_NAME)
_SYBHA_PF = $(DIR_PROFILE)/SYBHA.PFL
Execute_08 = local rm -f $(_SYBHAD)
Execute_09 = local ln -s -f $(DIR_EXECUTABLE)/sybdbfm$(FT_EXE) $(_SYBHAD)
Restart_Program_02 = local $(_SYBHAD) hadm pf=$(_SYBHA_PF)
#-----------------------------------------------------------------------
....
----

The status of the FM can be checked as below. Navigate to the ASCS work directory and the run
_sybdbfm status_:

_As user {mysapadm}_

[subs="specialchars,attributes,quotes"]
----
# 
----

Below are few parameters that needs to be updated in the _SYBHA.PFL_ to make the failover working.

[subs="specialchars,attributes,quotes"]
----
ha/syb/support_cluster = 1
ha/syb/failover_if_unresponsive = 1
ha/syb/allow_restart_companion = 1
ha/syb/set_standby_available_after_failover = 1
ha/syb/chk_restart_repserver = 1
ha/syb/cluster_fmhost1 = Hostname for Node 1 of the ASCS HA Setup
ha/syb/cluster_fmhost2 = Hostname for Node 2 of the ASCS HA Setup
ha/syb/use_boot_file_always = 1
----

Details of the all the FM parameters can be found in the SAP ASE HA DR User Guide. The one highlighted 
in bold are of interest thought for the setup on AWS. Since the FM is installed with the ASCS which can 
failover from Node 1 to Node 2 in the event of Node 1 going down, the parameter _ha/syb/cluster_fmhost1_ and 
_ha/syb/cluster_fmhost2_ provides the physical hostname of both the Nodes where the FM can potentially run.
In a scenario where the complete AZ, having the ASCS & Primary DB goes down. The DB failover is not triggered 
till the ASCS failover is complete and the FM is up and running on AZ2. The FM then needs to read the boot file 
to get the state of the ASE DB prior to failover so that it can trigger the failover correctly. The parameter 
_ha/syb/use_boot_file_always=1_ makes sure that the FM always reads from the boot file which is part of the work 
directory and failover along with FM.


== Appendix

=== CRM Config

The complete crm config for {sap} system {mySid}

[subs="specialchars,attributes,quotes"]
----
#
# nodes
#
node 1084753931: {myNode1}
node 1084753932: {myNode2}
#
# primitives for ASCS and ERS
#
primitive res_AWS_STONITH stonith:external/ec2 \
        op start interval=0 timeout=180 \
        op stop interval=0 timeout=180 \
        op monitor interval=120 timeout=60 \
        params tag=pacemaker profile=cluster
primitive rsc_fs_{mySID}_{myInstAscs} Filesystem \
  params device="efs-name:/ASCS00" \
         directory="/usr/sap/HA1/ASCS00" fstype=nfs4 \
         options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=200s timeout=40s
primitive rsc_fs_{mySID}_{myInstErs} Filesystem \
  params device="efs-name:/ERS10" \
    directory="/usr/sap/HA1/ERS10" fstype=nfs4 \
    options="rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2" \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=40s
primitive rsc_ip_{mySID}_{myInstAscs} ocf:suse:aws-vpc-move-ip \
        params address={myVipAAscs} routing_table=rtb-table-name \
        interface=eth0 profile=cluster \
        op start interval=0 timeout=180 \
        op stop interval=0 timeout=180 \
        op monitor interval=120 timeout=60
primitive rsc_ip_{mySID}_{myInstErs} ocf:suse:aws-vpc-move-ip \
        params address={myVipAErs} routing_table=rtb-table-name \
        interface=eth0 profile=cluster \
        op start interval=0 timeout=180 \
        op stop interval=0 timeout=180 \
        op monitor interval=120 timeout=60
primitive rsc_r53_{mySID}_{myInstAscs} aws-vpc-route53 \
        params hostedzoneid=hosted-zone-id ttl=10 \
        fullname=full-name profile=cluster \
        op start interval=0 timeout=180 \
        op stop interval=0 timeout=180 \
        op monitor interval=300 timeout=180
primitive rsc_sap_{mySID}_{myInstAscs} SAPInstance \
	operations $id=rsc_sap_{mySID}_{myInstAscs}-operations \
	op monitor interval=120 timeout=60 on_fail=restart \
	params InstanceName={mySID}_{myInstAscs}_{myVipNAscs} \
     START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstAscs}_{myVipNAscs}" \
     AUTOMATIC_RECOVER=false \
	meta resource-stickiness=5000 failure-timeout=60 migration-threshold=1 \
       priority=10
primitive rsc_sap_{mySID}_{myInstErs} SAPInstance \
	operations $id=rsc_sap_{mySID}_{myInstErs}-operations \
	op monitor interval=120 timeout=60 on_fail=restart \
	params InstanceName={mySID}_{myInstErs}_{myVipNErs} \
    START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstErs}_{myVipNErs}" \
    AUTOMATIC_RECOVER=false IS_ERS=true \
	meta priority=1000
#
# group definitions for ASCS and ERS
#
group grp_{mySID}_{myInstAscs} rsc_ip_{mySID}_{myInstAscs} \
   rsc_r53_{mySID}_{myInstAscs} rsc_fs_{mySID}_{myInstAscs} \
   rsc_sap_{mySID}_{myInstAscs} \
	 meta resource-stickiness=3000
group grp_{mySID}_{myInstErs} rsc_ip_{mySID}_{myInstErs} \
   rsc_fs_{mySID}_{myInstErs} rsc_sap_{mySID}_{myInstErs}

#
# constraints between ASCS and ERS
#
colocation col_sap_{mySid}_not_both -5000: grp_{mySID}_{myInstErs} grp_{mySID}_{myInstAscs}
location loc_sap_{mySid}_failover_to_ers rsc_sap_{mySID}_{myInstAscs} \
	rule 2000: runs_ers_{mySID} eq 1
order ord_sap_{mySid}_first_ascs Optional: rsc_sap_{mySID}_{myInstAscs}:start rsc_sap_{mySID}_{myInstErs}:stop symmetrical=false
#
# crm properties and more
#
property cib-bootstrap-options: \
    have-watchdog=false \
    dc-version=1.1.15-21.1-e174ec8 \
    cluster-infrastructure=corosync \
    stonith-enabled=true \
    stonith-action=poweroff \
    stonith-timeout=600s \
    last-lrm-refresh=1513844735
rsc_defaults rsc-options: \
    resource-stickiness=1 \
    migration-threshold=3
op_defaults op-options: \
    timeout=600 \
	record-pending=true

----

=== Checklist AWS Installation

Check your AWS configuration upfront and gather the following AWS items
before you start the installation:

[width="85%",options="header"]
|=========================================================
2+^|Checklist AWS installation
^| *Item* ^|*Status/Value*
2+^|SLES subscription and update status
|All systems have a SLES for SAP subscription |
|All systems have a public cloud channel |
|All system have been updated to use the latest patch level |
2+^|AWS User Privileges for the installing person
|Creation of EC2 instances and EBS volumes |
|Creation security groups |
|Creation EFS file systems |
|Modification of AWS routing tables |
|Creation policies and attach them to IAM roles |
|Optional for Route53 agent installation |
|Create and modify A-records in a private hosted zone |
|Potentially needed :Creation of subnets and routing tables |
2+^|VPC and Network
|VPC Id |
|CIDR range of VPC |
|Subnet id A for systems in first AZ |
﻿|Subnet id B for systems in second AZ |
|Routing table id for subnet A and B |
|Is this routing table associated with both subnets? |
| Alternative: Is it associated to VPC? Subnets do not have their own ones |
2+^|Optional: Route 53 configuration
|Name of private hosted Route 53 zone |
|Name of DHCP option set (Verify options!) |
|Is option set associated to VPC? |
2+^| AWS Policies Creation
|Name of data provider policy |
|Name of STONITH policy |
|Name of Move IP (Overlay IP) policy |
|Optionally: Name of Route53 policy |
2+^|First cluster node (ASCS and ERS)
|instance id |
|ENI id |
|IP address |
|hostname |
|instance is associated to subnet A? |
|instance has all 3 or 4 policies attached? |
|EC2 tag _pacemaker_ set with hostname? |
|AWS CLI profile _cluster_ created and set to _text_? |
|source/destination check disabled? |
2+^|Second cluster node (ASCS and ERS)
|instance id |
|ENI id |
|IP address |
|hostname |
|instance is associated to subnet B? |
|instance has all 3 or 4 policies attached? |
|EC2 tag _pacemaker_ set with hostname? |
|AWS CLI profile _cluster_ created and set to _text_?|
|source/destination check disabled? |
2+^|PAS system
|IP address |
|hostname |
|instance is associated to subnet A or B? |
|instance has data provider policy attached? |
2+^|AAS system
|IP address |
|hostname |
|instance is associated to subnet A or B |
|instance has data provider﻿ policy attached? |
2+^|DB system (is potentially node 1 of a database failover cluster)
|instance id |
|ENI id |
|IP address |
|hostname |
|instance is associated to subnet A? |
|instance has data provider﻿ policy attached?
A cluster node has 2 to 3 more policies attached |
2+^| Overlay IP address: service ASCS
|IP address |
|Has it been added to routing table? |
|Does it point to the ENI of first node? |
2+^|Overlay IP address: service ERS
|IP address |
|Has it been added to routing table? |
|Does it point to the ENI of the second node? |
2+^| Optional: Overlay IP address DB server
|IP address |
|Has it been added to routing table? |
|Does it point to the ENI of the DB server? |
2+^|Optional: Route 53 configuration
|The Route 53 private hosted zone has an A record with
the name of the ASCS system
the IP address of the first cluster node |
2+^| Creation of EFS filesystem
| DNS name of EFS filesystem |
2+^| Internet access
|All instance have Internet access ? Check routing tables |
|Alternative: Add http proxies for data providers and cluster software |
|=========================================================

////
//changelog since August 2018
- adding picture
- adding chapter regarding cloud-netconfig-ec2
-- May/June 2019
- adding ASE replication cluster integration

////
:leveloffset: 2
include::SAPNotes_AWS.adoc[]
