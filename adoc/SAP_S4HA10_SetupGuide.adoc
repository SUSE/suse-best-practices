:docinfo:
= SAP S/4 HANA - Enqueue Replication 2 High Availability Cluster - Setup Guide
// == SAP S/4 HANA 1809 High Availability Cluster

// Fabian Herschel Bernd Schubert
// 2019/01/31


// :Revision: 1.0

// Revision {Revision} from {docdate}

// Standard SUSE includes
// include::common_copyright_gfdl.adoc[]

// :toc:

include::Variables_s4_1809.adoc[]
//

////
TODO
////

== About this Guide

=== Introduction

{sles4sapReg} is the optimal platform to
run {sapReg} applications with high availability. Together with a redundant layout
of the technical infrastructure, single points of failure can be eliminated.

{sapBSReg} is a sophisticated application platform for large enterprises
and mid-size companies. Many critical business environments require the highest
possible {sapReg} application availability.

The described cluster solution can be used for {sapReg} {sapS4pl}.

{sapS4pl} is a common stack of middleware functionality used to support the SAP
business applications. The {sapERS} constitutes application
level redundancy for one of the most crucial components of the {sapS4pl} stack,
the enqueue service. An optimal effect of the enqueue replication mechanism can
be achieved when combining the application level redundancy with a high
availability cluster solution, as provided for example by {sles4sap}. Over several
years of productive operations, the components mentioned have proven their maturity
for customers of different sizes and branches.


=== Additional Documentation and Resources

Several chapters in this document contain links to additional documentation resources that
are either available on the system or on the Internet.

For the latest product documentation updates, see http://www.suse.com/documentation.

More whitepapers, guides and best practices documents referring to SUSE Linux Enterprise Server and SAP can be
found and downloaded at the SUSE Best Practices web page:

https://www.suse.com/documentation/suse-best-practices/#sap

Here you can access guides for {SAPHANA} system replication
automation and High Availability (HA) scenarios for {SAPNw} and {s4hana}.

Additional resources, such as customer references, brochures or flyers, can be found at the SUSE Linux
Enterprise Server for SAP Applications resource library:

https://www.suse.com/products/sles-for-sap/resource-library/.


=== Errata

To deliver urgent smaller fixes and important information in a timely manner, the Technical Information Document (TID) for this document
will be updated, maintained and published at a higher frequency:

- SAP S/4 HANA - Enqueue Replication 2 High Availability Cluster - Setup Guide - Errata
(https://www.suse.com/support/kb/doc/?id=7023714)

In addition to this guide, check the SUSE SAP Best Practice Guide Errata for other solutions
(https://www.suse.com/support/kb/doc/?id=7023713).

// Standard SUSE includes
=== Feedback
include::common_intro_feedback.adoc[]

//=== Documentation Conventions
//TODO work on SUSE doc standard conventions file
//include::common_intro_typografie.adoc[]

== Scope of this Document

The document at hand explains how to:

- Plan a {sleHA} platform for {sapS4pl}, including {sapERS}.
- Set up a {linux} high availability platform and perform a basic {sapS4pl}
  installation including {sapERS} on {sle}.
- Integrate the high availability cluster with the {sap} control framework via
  *{s4sClConnector3}* version 3, as certified by {sap}.

This document focuses on the high availability of the central services. For {saphana} system replication,
consult the guides for the performance-optimized or cost-optimized scenario (see section "Additional
documentation and resources").

== Overview

The document at hand describes how to set up a pacemaker cluster using {sles4sap}
{slesProdVersion} for the Enqueue Replication scenario. The focus is on matching the {sapCert}
certification specifications and goals. For the set up described in this document, two nodes are used for the ASCS
central services instance and ERS replicated enqueue instance. These two nodes are controlled by
the SUSE Linux Enterprise High Availability cluster. A third node is used for running the database and PAS and AAS application
server instances. An additional forth node is used as NFS server.

The goals for the setup include:

- Integration of the cluster with the {SAP} start framework _sapstartsrv_ to
  ensure that maintenance procedures do not break the cluster stability.
- Rolling Kernel Switch (RKS) awareness.
- Standard {sap} installation to improve support processes.
- Support of automated HA maintenance mode for SAP resources by implementing support of
  SAP HACheckMaintenanceMode and HASetMaintenanceMode.
- Support of more than two cluster nodes for ASCS and ERS instances allowed.

The updated certification {sapcert} redefines some of the test procedures
and describes new expectations how the cluster should behave in special
conditions. These changes allowed to improve the cluster architecture and to
design it for easier usage and setup.

Shared SAP resources are located on a central NFS server.

The {sap} instances themselves are installed on a shared disk to allow for switching over the file
systems for proper functionality. The second need for a shared disk derives from using the SBD
for the cluster fencing mechanism STONITH.

=== Differences to Previous Cluster Architectures

- The described new concept differs from the old stack with the master-slave architecture.
  With the new certification there is a switch to a more simple model with primitives.
  This means: on one machine there is the ASCS instance with its own resources and on
  the other machine there is the ERS instance with its own resources.
- For {s4Hana} the new concept implies that, after a resource failure, the ASCS does not need to be started at the ERS
  side. The new enqueue architecture is also named ENSA2.

=== Three Systems or More for ASCS, ERS, Database and Additional SAP Instances

The document at hand describes the installation of a distributed {sap} system on three and more
systems. In this setup only two systems reside inside the cluster. The database and
{sap} dialog instances could also be added to the cluster by either adding more nodes to the
cluster or by installing the database on either of the existing nodes. However it is recommended to install the
database on a separate cluster. The cluster configuration for three and more nodes is described at the
end of this document. The number of nodes within one cluster should be either two or an odd number.
////
TODO: check text content
////
NOTE: Because the setup at hand focuses on the {sapCert} certification, the cluster detailed
in this guide only manages the {sap} instances ASCS and ERS.

If your database is {sapHana}, it is recommended to set up the performance optimized
system replication scenario using the automation solution {sapHanaSR}. The
{sapHanaSR} automation should be set up in an own two-node cluster. The setup is
described in a separate best practices document available from the SUSE Best Practices web page.

.Three systems for the certification setup
image::sles4sap_nw740_3nodes.svg[SVG]

.Clustered machines Two Node Scenario

*    one machine ({my2nd1}) for ASCS
**    Hostname:    {myVipNAscs}

*    one machine ({my2nd2}) for ERS
**    Hostname:   {myVipNErs}

////
TODO: add the end if needed
.Clustered machines Four Node Scenario

*    one machine ({my4nd1}) for ASCS
**    Hostname:   {myVipNAscs}

*    one machine ({my4nd2}) for ERS
**    Hostname:   {myVipNErs}

*    one machine ({my4nd3}) for ASCS / ERS
**    Hostname:  not set during standby mode

*    one machine ({my4nd4}) for ASCS / ERS
**    Hostname:   not set during standby mode
////

.Non-Clustered machine

*    one machine ({myNode1}) for DB and as application server
**    Hostname:   {myVipNDb}
**    Hostname:   {myVipNPas}
**    Hostname:   {myVipNDSec}



=== High Availability for the Database

Depending on your needs you can increase the availability of the database, if your
database is not already highly available by design.

==== {SapHana} System Replication

A perfect enhancement of the three node scenario described in this document is
to implement an {saphana} system replication (SR) automation.

.One cluster for central services, one for {saphana} SR
image::sles4sap_nw740_cs+hanasr.svg[SVG]

////
TODO: this could be skipped
The following Databases are supported in combination with this scenario:

- SAP HANA DATABASE 2.0

==== Simple Stack

Another option is to implement a second cluster for a database without SR, also known as
"ANYDB". The cluster resource agent SAPDatabase uses the SAPHOSTAGENT to control
and monitor the database.

.One cluster for the central services and one cluster for the ANY database
image::sles4sap_nw740_cs+anydb.svg[SVG]
////

.The following OS / Databases combination are examples for this scenario
[width="85%",options="header"]
|=========================================================
2+^|{sles4sap} 15
^| *Intel X86_64*
|SAP HANA DATABASE 2.0
|=========================================================

NOTE: Version for {sapS4pl} on Linux on AMD64/Intel 64. More information about
the supported combinations of OS and Databases for {sapS4in} can be found at the
SAP Product Availability Matrix at https://apps.support.sap.com/sap/support/pam[SAP PAM].

=== Integration of {sapS4} into the Cluster Using the Cluster Connector

The integration of the HA cluster through the SAP control framework using the
*{s4sClConnector}* is of special interest. The service *{SAPSTARTSRV}* controls {sap} instances since
{sap} Kernel versions 6.40. One of the classic problems running
{sap} instances in a highly available environment is the following: If an {sap}
administrator changes the status (start/stop) of an {sap} instance without using
the interfaces provided by the cluster software, the cluster framework will
detect that as an error status and will bring the {sap} instance into the old
status by either starting or stopping the {sap} instance. This can result in
very dangerous situations, if the cluster changes the status of an {sap} instance
during some {sap} maintenance tasks. The new updated solution enables the central component
*{SAPSTARTSRV}* to report state changes to the cluster software. This avoids dangerous situations
as previously described.
More details can be found in the blog article "Using sap_vendor_cluster_connector for interaction between cluster
framework and sapstartsrv" at https://blogs.sap.com/2014/05/08/using-sapvendorclusterconnector-for-interaction-between-cluster-framework-and-sapstartsrv/comment-page-1/.

.Cluster connector to integrate the cluster with the {sap} start framework
image::sles4sap_clusterconnector.svg[SVG]

NOTE: For this scenario, an updated version of the *{s4sClConnector3}* is used.
It implements the API version 3 for the communication between the cluster
framework and the *{sapstartsrv}* service.

The new version of the *{s4sClConnector3}* allows to start, stop and 'migrate'
an {sap} instance. The integration between the cluster software and the
*{sapstartsrv}* also implements the option to run checks of the HA setup using either the
command line tool *sapcontrol* or even the {SAP} management consoles ({SAP} MMC or
{sap} MC). Since version 3.1.0 and later the maintenance mode of cluster resources triggered with SAP
*sapcontrol* commands is supported.


=== Disks and Partitions

XFS is used for all {sap} file systems besides the file systems on NFS.

==== Shared Disk for Cluster ASCS and ERS

The disk for the ASCS and ERS instances must be shared and assigned to the
cluster nodes {my2nd1} and {my2nd2} in the two node cluster example. In addition to the partitions for the file systems
for the SAP instances, the disk also provides the partition to be used as SBD.

On {my2nd1} prepare the file systems for the shared disk. Create three partitions on
the shared drive {myDevA}:

* Partition one ({myDevPartSbd}) for SBD (7MB)
* Partition two ({myDevPartAscs}) for the first file system (10GB) formatted
with XFS
* Partition three ({myDevPartErs}) for the second file system (10GB) formatted
with XFS

To create the partitions, you can use either YaST or available command line tools.
The following script can be used for non-interactive setups.

[subs="attributes"]
----
# parted -s {myDevA} print
# # we are on the 'correct' drive, right?
# parted -s {myDevA} mklabel gpt
# parted -s {myDevA} mkpart primary 1049k 8388k
# parted -s {myDevA} mkpart primary 8389k 10.7G
# parted -s {myDevA} mkpart primary 10.7G 21.5G
# mkfs.xfs {myDevPartAscs}
# mkfs.xfs {myDevPartErs}
----

For these file systems it is recommended to use plain partitions to keep the cluster
configuration as simple as possible. However you can also place these file
systems in separate volume groups. In that case you need to add further cluster
resources to control the logical volume groups, and occasionally
MD-Raid devices. The description of such a setup however is out-of-scope of the guide at hand.

After having partitioned the shared disk on {my2nd1}, you need to request a
partition table rescan on {my2nd2}.

[subs="attributes"]
----
# partprobe; fdisk -l {myDevA}
----

During the SAP installation you need to mount {myMpAscs} on {my2nd1} and
{myMpErs} on {my2nd2}.
////
TODO: non line break with in command or variables
////
- {my2nd1}:   {myDevPartAscs}   {myMpAscs}
- {my2nd2}:   {myDevPartErs}   {myMpErs}

==== Disk for DB and Dialog Instances (HANA DB)

The disk for the database server is assigned to {myNode1}.
The disk for the primary and secondary application server is assigned to
{myNode1}.

.{myNode1}
* Partition one ({myDevPartDb}) for the Database (160GB) formatted with XFS
* Partition two ({myDevPartPas}) for the PAS instance (10GB) formatted with XFS
* Partition three ({myDevPartSec}) for the AAS instance (10GB) formatted
with XFS

////
TODO we used a non shared disk. Forgot to mount during installation?!?!?!
////

You can either use YaST or available command line tools to create the partitions.
The following script can be used for non-interactive setups.

.{myNode1}
==============================================
[subs="attributes"]
----
# parted -s {myDevB} print
# # we are on the 'correct' drive, right?
# parted -s {myDevB} mklabel gpt
# parted -s {myDevB} mkpart primary 1049k 160G
# parted -s {myDevB} mkpart primary 160G 170G
# parted -s {myDevB} mkpart primary 170G 180G
# mkfs.xfs {myDevPartDb}
# mkfs.xfs {myDevPartPas}
# mkfs.xfs {myDevPartSec}
----
==============================================


.To be mounted either by OS or an optional cluster
- {myNode1}:   {myDevPartDb}   {bsMpDb}

- {myNode1}:   {myDevPartPas}   {myMpPas}

- {myNode1}:   {myDevPartSec}   {myMpSec}

NOTE:  {myInstPas}: Since NetWeaver 7.5 the primary application server instance
directory has been renamed to 'D<Instance_Number>'.


.NFS server
- {myNfsSrv}:{myNFSSapmnt}   /sapmnt

- {myNfsSrv}:{myNFSSys} /usr/sap/{mySid}/SYS

.Media
- {myNfsSrv}:{myNFSSapmedia} /var/lib/Landscape

=== IP Addresses and Virtual Names

Check if the file _/etc/hosts_ contains at least the following address resolutions.
Add those entries if they are missing.

[subs="attributes"]
----
{myIPNode1}  {myNode1}
{myIP2nd1}  {my2nd1}
{myIP2nd2}  {my2nd2}

{myVipAAscs}  sap{mySidLc}as
{myVipAErs}  sap{mySidLc}er
{myVipADb}  sap{mySidLc}db
{myVipAPas}  sap{mySidLc}d1
{myVipAAas}  sap{mySidLc}d2
----

////
TODO: multinode cluster extension
# 4node cluster and divided application server
{myIPNode2}  {myNode2}
{myIPNode3}  {myNode3}
{myIP4nd1}  {my4nd1}
{myIP4nd2}  {my4nd2}
{myIP4nd3}  {my4nd3}
{myIP4nd4}  {my4nd4}
----
////

=== Mount Points and NFS Shares

In the present setup the directory _/usr/sap_ is part of the root file system. You can
also create a dedicated file system for that area and mount _/usr/sap_
during the system boot. As _/usr/sap_ contains the {sap} control file
_sapservices_ and the *{saphostagent}*, the directory should not be placed on a
shared file system between the cluster nodes.

You need to create the directory structure on all nodes that should run the SAP resource.
The SYS directory will be located on an NFS share for all nodes.

- Creating mount points and mounting NFS share at all nodes

.{sapS4}
==============================================
[subs="attributes"]
----
# mkdir -p /sapmnt /var/lib/Landscape
# mkdir -p /usr/sap/{mySid}/{{myInstAscs},{myInstPas},{myInstDSec},{myInstErs},SYS}
# mount -t nfs {myNfsSrv}:{myNFSSapmnt}    /sapmnt
# mount -t nfs {myNfsSrv}:{myNFSSys} /usr/sap/{mySid}/SYS
# mount -t nfs {myNfsSrv}:{myNFSSapmedia} /var/lib/Landscape
----
==============================================

- For HANA: creating mount points for database at {myNode1}:

[subs="attributes"]
----
# mkdir /hana
# mount {myDevPartDb} /hana/
# mkdir -p /hana/{shared,data,log}
# mount {myDevPartPas} {myMPPas}
# mount {myDevPartSec} {myMPSec}
----
////
TODO: one partition will be mounted mkdir must be run again after mount
////
You do not control the NFS shares via the cluster in the setup at hand. Thus you should
add these file systems to _/etc/fstab_ to get the file systems mounted during
the next system boot.

////
review Lee means this looks like a 4 node cluster, adding additional title???
////
.File system layout including NFS shares
image::sles4sap_nw740_fs.svg[SVG]

Prepare the three servers for the distributed {sap} installation. Server 1
({myNode1}) is used to install the {sap} HANA database. Server 1
({myNode1}) is used to install the PAS {sap} instance. Server 1
({myNode1}) is used to install the AAS {sap} instances. Server 2
({my2nd1}) is used to install the ASCS {sap} instances. Server 3
({my2nd2}) is used to install the ERS {sap} instances.

- Mounting the instance and database file systems at one specific node:

.{sapS4}
=============================================================

[subs="attributes"]
----
(ASCS   {my2nd1}) # mount {myDevPartAscs} {myMpAscs}
(ERS    {my2nd2}) # mount {myDevPartErs} /usr/sap/{mySid}/{myInstErs}
(DB     {myNode1}) # mount {myDevPartDb} /hana/
(Dialog {myNode1}) # mount {myDevPartPas} /usr/sap/{mySid}/{myInstPas}
(Dialog {myNode1}) # mount {myDevPartPas} /usr/sap/{mySid}/{myInstDSec}
----
=============================================================

- As a result the directory _/usr/sap/{mySid}/_ should now look as follows:

[subs="attributes"]
----
# ls -l /usr/sap/{mySid}/
total 0
drwxr-xr-x 1 {mySidLc}adm sapsys 70 28. Mar 17:26 ./
drwxr-xr-x 1 root   sapsys 58 28. Mar 16:49 ../
drwxr-xr-x 7 {mySidLc}adm sapsys 58 28. Mar 16:49 {myInstAscs}/
drwxr-xr-x 1 {mySidLc}adm sapsys  0 28. Mar 15:59 {myInstDSec}/
drwxr-xr-x 1 {mySidLc}adm sapsys  0 28. Mar 15:59 {myInstPas}/
drwxr-xr-x 1 {mySidLc}adm sapsys  0 28. Mar 15:59 {myInstErs}/
drwxr-xr-x 5 {mySidLc}adm sapsys 87 28. Mar 17:21 SYS/
----

NOTE: The owner of the folder and files is changed during the {sap} installation. By default all
of them are owned by _root_.

== SAP Installation

The overall procedure to install the distributed SAP system is as follows:

- Install the ASCS instance for the central services
- Install the ERS to get a replicated enqueue scenario
- Prepare the ASCS and ERS installations for the cluster take-over
- Install the Database
- Install the primary application server instance (PAS)
- Install additional application server instances (AAS)

The result will be a distributed {sap} installation as illustrated here:

.Distributed installation of the {sap} system
image::sles4sap_ap1809_fs.svg[SVG]

TODO:PRIO1 new picture

=== Linux User and Group Number Scheme

Whenever asked by the SAP software provisioning manager (SWPM) which Linux User
IDs or Group IDs to use, refer to the following table as example:

[subs="attributes"]
----
Group sapinst      1001
Group sapsys      1002
Group {bsSidDBLc}shm    1003

User  {mysapadm}       2001
User  sapadm       2002
User  {bsDBadm}       2003
----


=== Install ASCS on {my2nd1}

Temporarily, as local IP address, you need to set the service IP address which you will later use in the
cluster, because the installer wants to resolve or use it.
Make sure to use the correct virtual host name for each installation step.
Take care for the file systems like _{myDevPartAscs}_ and _/var/lib/Landscape/_ which might also need
to be mounted.

[subs="attributes"]
----
# ip a a {myVipAAscs}{myVipNM} dev eth0
# mount {myDevPartAscs} {myMpAscs}
# cd {mySAPinst}
# ./sapinst SAPINST_USE_HOSTNAME={myVipNAscs}
----

* SWPM product installation path:
** Installing {sapS4in}  -> SAP HANA DATABASE -> Installation -> Application Server ABAP
-> High-Availability System -> ASCS Instance
* Use SID {mySid}
* Use instance number {myAscsIno}
* Deselect using FQDN
* All passwords: use {mySapPwd}
* Double-check during the parameter review, if virtual name *{myVipNAscs}* is used

=== Install ERS on {my2nd2}

Temporarily, as local IP address, you need to set the service IP address which you will later use in the
cluster, because the installer want to resolve or use it.
Make sure to use the correct virtual host name for each installation step.

[subs="attributes"]
----
# ip a a {myVipAErs}{myVipNM} dev eth0
# mount {myDevPartErs} {myMpErs}
# cd {mySAPinst}
# ./sapinst SAPINST_USE_HOSTNAME={myVipNErs}
----

* SWPM product installation path:
** Installing {sapS4in}  -> SAP HANA DATABASE -> Installation -> Application Server ABAP ->
High-Availability System -> ERS Instance
* Use instance number {myErsIno}
* Deselect using FQDN
* Double-check during the parameter review, if virtual name *{myVipNErs}* is used
* If you get an error during the installation about permissions, change the
  ownership of the ERS directory

[subs="attributes"]
----
# chown -R {mysapadm}:sapsys /usr/sap/{mySid}/{myInstErs}
----

* If you get a prompt to manually stop/start the ASCS instance, log in at
{my2nd1} as user {mysapadm} and call 'sapcontrol'.

[subs="attributes"]
----
# sapcontrol -nr {myAscsIno} -function Stop    # to stop the ASCS
# sapcontrol -nr {myAscsIno} -function Start   # to start the ASCS
----

=== Subsequent Steps for ASCS and ERS

After installation, you can perform several subsequent steps on the ASCS and ERS instances.

==== Stopping ASCS and ERS

To stop the ASCS and ERS instances, use the commands below.

On _{my2nd1}_, do the following:

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myAscsIno} -function Stop
# sapcontrol -nr {myAscsIno} -function StopService
----

On _{my2nd2}_, do the following:

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myErsIno} -function Stop
# sapcontrol -nr {myErsIno} -function StopService
----

==== Maintaining _sapservices_

Ensure _/usr/sap/sapservices_ holds both entries (ASCS and ERS) on both cluster
nodes. This allows the *{sapstartsrv}* clients to start the service as follows:

As user _{mySapAdm}_, type the following command:

[subs="attributes"]
----
# sapcontrol -nr {myErsIno} -function StartService {mySid}
----
The _/usr/sap/sapservices_ file looks as follows - typically one line per instance is displayed:

[subs="attributes"]
----
#!/bin/sh
LD_LIBRARY_PATH=/usr/sap/{mySid}/{myInstAscs}/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/{mySid}/{myInstAscs}/exe/sapstartsrv pf=/usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs} -D -u {mySapAdm}
LD_LIBRARY_PATH=/usr/sap/{mySid}/{myInstErs}/exe:$LD_LIBRARY_PATH; export LD_LIBRARY_PATH; /usr/sap/{mySid}/{myInstErs}/exe/sapstartsrv pf=/usr/sap/{mySid}/{myInstErs}/profile/{mySid}_{myInstErs}_{myVipNErs} -D -u {mySapAdm}
----

==== Integrating the Cluster Framework Using _{s4sClConnector3}_

Install the package *{s4sClConnector3}* version 3.1.0 from the SUSE
repositories:

[subs="attributes"]
----
# zypper in {s4sClConnector3}
----

NOTE: The package *{s4sClConnector3}* contains the version 3.x.x (SAP API 3).
The package *{s4sClConnector3}* with version 3.0.x implements the SUSE SAP API
version 3. New features like SAP Rolling Kernel Switch (RKS) and migration of ASCS are
only supported with this new version.
The package *{s4sClConnector3}* with version 3.1.x supports in addition the maintenance mode of
cluster resources triggered from SAP tools.

For the ERS and ASCS instances, edit the instance profiles
{mySid}_{myInstAscs}_{myVipNAscs} and {mySid}_{myInstErs}_{myVipNErs} in the
profile directory _/usr/sap/{mySid}/SYS/profile/_.

Tell the *{sapStartSrv}* service to load the HA script connector library and
to use the connector *{s4sClConnector3}*.

[subs="attributes"]
----
service/halib = $(DIR_CT_RUN)/saphascriptco.so
service/halib_cluster_connector = /usr/bin/sap_suse_cluster_connector
----

Add the user _{mySapAdm}_ to the unix user group _haclient_.

[subs="attributes"]
----
# usermod -a -G haclient {mySapAdm}
----

==== Adapting {sap} Profiles to match the {sapCert} Certification

For the ASCS instance, change the start command from _Restart_Programm_xx_ to
_Start_Programm_xx_ for the enqueue server (Enqueue Server 2). This change tells the
{sap} start framework *not* to self-restart the enqueue process. Such a restart
would result in a loss of the locks.

.File /usr/sap/{mySid}/SYS/profile/{mySid}_{myInstAscs}_{myVipNAscs}

[subs="attributes"]
----
Start_Program_01 = local $(_ENQ) pf=$(_PF)
----

Optionally you can limit the number of restarts of services (in the case of
ASCS this limits the restart of the message server).

For the ERS instance, change the start command from _Restart_Programm_xx_ to
_Start_Programm_xx_ for the enqueue replication server (Enqueue Replicator 2).

.File /usr/sap/{mySid}/SYS/profile/{mySid}_{myInstErs}_{myVipNErs}

[subs="attributes"]
----
Start_Program_00 = local $(_ENQR) pf=$(_PF) NR=$(SCSID)
----

==== Starting ASCS and ERS

To start the ASCS and ERS instances, use the commands below.

On _{my2nd1}_, do the following:

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myAscsIno} -function StartService {mySid}
# sapcontrol -nr {myAscsIno} -function Start
----

On _{my2nd2}_, do the following

[subs="attributes"]
----
# su - {mySapAdm}
# sapcontrol -nr {myErsIno} -function StartService {mySid}
# sapcontrol -nr {myErsIno} -function Start
----

=== Install DB on {myNode1} (Example SAP HANA)

The HANA DB has very strict HW requirements. The storage sizing depends on many
indicators. Check the supported configurations at
https://www.sap.com/documents/2015/03/74cdb554-5a7c-0010-82c7-eda71af511fa.html[SAP HANA Hardware Directory]
and https://www.sap.com/documents/2015/03/74cdb554-5a7c-0010-82c7-eda71af511fa.html[SAP HANA TDI].

[subs="attributes"]
----
# ip a a {myVipADb}{myVipNM} dev eth0
# mount {myDevPartDb} {bsMPDb}
# mount {myDevPartPas} {myMPPas}
# mount {myDevPartSec} {myMPSec}
# cd {mySAPinst}
# ./sapinst SAPINST_USE_HOSTNAME={myVipNDb}
----

* SWPM product installation path:
** Installing {sapS4in}  -> SAP HANA DATABASE -> Installation -> Application Server ABAP ->
High-Availability System -> Database Instance
* Profile directory /sapmnt/{mySid}/profile
* Deselect using FQDN
* Database parameters : Database ID (DBSID) is {bsSidDB};
Database Host is {myVipNDb}; Instance Number is {bsDBIno}
* Database System ID enter Instance Number is {bsDBIno}; SAP Mount Directory is
sapmnt/{mySID}/profile
* Account parameters: change them in case of custom values needed
* Cleanup: select *Yes*, remove operating system users from group'sapinst'....
* Double-check during the parameter review, if virtual name *{myVipNDb}* is
  used

=== Install the Primary Application Server (PAS) on {myNode1}

[subs="attributes"]
----
# ip a a {myVipAPas}{myVipNM} dev eth0
# mount {myDevPartPas} {myMPPas}
# cd {mySAPinst}
# ./sapinst SAPINST_USE_HOSTNAME={myVipNPas}
----

* SWPM product installation path:
** Installing {sapS4in}  -> SAP HANA DATABASE -> Installation -> Application Server ABAP ->
High-Availability System -> Primary Application Server Instance (PAS)
* Use instance number {myPasIno}
* Deselect using FQDN
* For this example setup we have used a default secure store key
* Do not install Diagnostic Agent
* No SLD
* Double-check during the parameter review, if virtual name *{myVipNPas}* is used

=== Install an Additional Application Server (AAS) on {myNode1}

[subs="attributes"]
----
# ip a a {myVipADSec}{myVipNM} dev eth0
# mount {myDevPartSec} {myMPSec}
# cd {mySAPinst}
# ./sapinst SAPINST_USE_HOSTNAME={myVipNDSec}
----

* SWPM product installation path:
** Installing {sapS4in}  -> SAP HANA DATABASE -> Installation -> Application Server ABAP ->
High-Availability System -> Additional Application Server Instance (AAS)
* Use instance number {myDSecIno}
* Deselect using FQDN
* Do not install Diagnostic Agent
* Double-check during the parameter review, if virtual name *{myVipNDSec}* is used

== Implement the Cluster

The main procedure to implement the cluster is as follows:

* Install the cluster software, if not already done during the installation of
the operating system.
* Configure the cluster communication framework *corosync*.
* Configure the cluster resource manager.
* Configure the cluster resources.
* Tune the cluster timing in special for the SBD.

///////////////////////////////
TODO: Do we really need to stop, unconfigure and unmount?
Maybe we find a way to configure the resources that the cluster just
accepts the already started resource groups - lets see ;-)
///////////////////////////////

NOTE: Before you continue to set up the cluster, perform the following actions:
First stop all SAP instances. Then remove the (manually added) IP addresses on the cluster nodes.
Finally unmount the file systems which will be controlled by the cluster later.

NOTE: The SBD device/partition needs to be created beforehand. Double-check which
device/partition to use! In this setup guide a partition _{myDevPartSbd}_
is already reserved for SBD usage.

.Tasks

- Setup *chrony* (this is best done with *yast2*) and enable it.

- Install *pattern ha_sles* on both cluster nodes.

[subs="attributes"]
----
# zypper in -t pattern ha_sles
----

=== Configure the Cluster Base

.Tasks

- To configure the cluster base, you can use either YaST or the interactive
command line tool *ha-cluster-init*. The example below uses the command line wizard.

- Install and configure the watchdog device on the first machine.

Instead of deploying the software based solution, a hardware-based watchdog device should preferably be used.
The following example uses the software device but can be easily adapted to the hardware device.

[subs="attributes"]
----
# modprobe softdog
# echo softdog > /etc/modules-load.d/watchdog.conf
# systemctl restart systemd-modules-load
# lsmod | egrep "(wd|dog|i6|iT|ibm)"
----

- Install and configure the cluster stack on the first machine

[subs="attributes"]
----
# ha-cluster-init -u -s /dev/sda1
----

- Join the second node

On the second node, perform some preparation steps.

[subs="attributes"]
----
# modprobe softdog
# echo softdog > /etc/modules-load.d/watchdog.conf
# systemctl restart systemd-modules-load
# lsmod | egrep "(wd|dog|i6|iT|ibm)"
----

To configure the cluster base, you can use either YaST or the interactive
command line tool *ha-cluster-join*. The example below uses the command line wizard.

[subs="attributes"]
----
# ha-cluster-join -c {my2nd1}
----

- The _crm_mon -1r_ output should look as follows:

[subs="attributes"]
----
Stack: corosync
Current DC: {my2nd1} (version 1.1.18+20180430.b12c320f5-1.14-b12c320f5) - partition with quorum
Last updated: Mon Jan 28 13:10:37 2019
Last change: Wed Jan 23 09:52:57 2019 by root via cibadmin on {my2nd1}

2 nodes configured
1 resource configured

Online: [ {my2nd1} {my2nd2} ]

stonith-sbd	(stonith:external/sbd):	Started {my2nd1}
----

=== Configure Cluster Resources

A changed SAPInstance resource configuration is needed for {sapS4} to *not* use
the master-slave construct anymore. Move to a more cluster-like construct to
start and stop the ASCS and the ERS instances themselves, but *not* the complete
master-slave.

////////////////////////////////////
TODO: We need to adap this section, if the new enqueue2 is published.
////////////////////////////////////

With the new version of ENSA2, the ASCS instance can be started on the same host. There is no longer a need
to follow the ERS instance. The ASCS instance receives the enqueue lock table over the network from the ERS instance. If no
other node is available, the ASCS instance will be started on the same host where the ERS instance is running.

.Resources and constraints
image::sles4sap_s4hana1809_resources.svg[SVG]

////
The implementation is done using a new flag "runs_ers_$SID" within
the resource agent (RA), enabled using the resource parameter "IS_ERS=TRUE".
////

Another benefit of this concept is that you can work with native (mountable)
file systems instead of a shared (NFS) file system for the {sap} instance
directories.

==== Preparing the Cluster for Adding the Resources

To prevent the cluster from starting partially defined resources, set the cluster
to the maintenance mode. This deactivates all monitor actions.

As user _root_, type the following command:

[subs="attributes"]
----
# crm configure property maintenance-mode="true"
----
////
TODO: for 2+ node cluster different SBD settings
==== Configure the SBD Resource
Verify the SBD cluster configuration and if needed, modify them as described.

.SBD stonith
================================================
[subs="attributes"]
----
# crm resource param stonith-sbd set pcmk_action_limit -1
----
================================================
////
==== Configuring the Resources for the ASCS Instance

First, configure the resources for the file system, IP address and the {sap}
instance. You need to adapt the parameters for your specific environment.

.ASCS primitive
================================================
[subs="attributes"]
----
primitive rsc_fs_{mySID}_{myInstAscs} Filesystem \
  params device="{myDevPartAscs}" directory="/usr/sap/{mySID}/{myInstAscs}" \
         fstype=xfs \
  op start timeout=60s interval=0 \
  op stop timeout=60s interval=0 \
  op monitor interval=20s timeout=40s
primitive rsc_ip_{mySID}_{myInstAscs} IPaddr2 \
  params ip={myVipAAscs} \
  op monitor interval=10s timeout=20s
primitive rsc_sap_{mySID}_{myInstAscs} SAPInstance \
  operations $id=rsc_sap_{mySID}_{myInstAscs}-operations \
  op monitor interval=11 timeout=60 on_fail=restart \
  params InstanceName={mySID}_{myInstAscs}_{myVipNAscs} \
     START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstAscs}_{myVipNAscs}" \
     AUTOMATIC_RECOVER=false \
  meta resource-stickiness=5000
----
================================================

.ASCS group
================================================
[subs="attributes"]
----
group grp_{mySID}_{myInstAscs} \
  rsc_ip_{mySID}_{myInstAscs} rsc_fs_{mySID}_{myInstAscs} rsc_sap_{mySID}_{myInstAscs} \
	meta resource-stickiness=3000
----
================================================

Create a _txt_ file (like _crm_ascs.txt_) with your preferred text editor. Add
both examples (primitives and group) to that file and load the configuration to
the cluster manager configuration.

As user _root_, type the following command:

[subs="attributes"]
----
# crm configure load update crm_ascs.txt
----

==== Configuring the Resources for the ERS Instance

Second, configure the resources for the file system, IP address and the {sap}
instance. You need to adapt the parameters for your specific environment.

The specific parameter _IS_ERS=true_ must only be set for the ERS instance.

.ERS primitive
================================================
[subs="attributes"]
----
primitive rsc_fs_{mySID}_{myInstErs} Filesystem \
  params device="{myDevPartErs}" directory="/usr/sap/{mySID}/{myInstErs}" fstype=xfs \
  op start timeout=60s interval=0 \
  op stop timeout=60s interval=0 \
  op monitor interval=20s timeout=40s
primitive rsc_ip_{mySID}_{myInstErs} IPaddr2 \
  params ip={myVipAErs} \
  op monitor interval=10s timeout=20s
primitive rsc_sap_{mySID}_{myInstErs} SAPInstance \
  operations $id=rsc_sap_{mySID}_{myInstErs}-operations \
  op monitor interval=11 timeout=60 on_fail=restart \
  params InstanceName={mySID}_{myInstErs}_{myVipNErs} \
         START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstErs}_{myVipNErs}" \
         AUTOMATIC_RECOVER=false IS_ERS=true
----
================================================

.ERS group
================================================
[subs="attributes"]
----
group grp_{mySID}_{myInstErs} \
  rsc_ip_{mySID}_{myInstErs} rsc_fs_{mySID}_{myInstErs} rsc_sap_{mySID}_{myInstErs}
----
================================================

Create a _txt_ file (like _crm_ers.txt_) with your preferred text editor. Add
both examples (primitives and group) to that file and load the configuration to
the cluster manager configuration.

_As user root_

[subs="attributes"]
----
# crm configure load update crm_ers.txt
----

==== Configuring the Colocation Constraints between ASCS and ERS

Compared to the ENSA1 configuration, the constraints between the ASCS and ERS instances are changed.
An ASCS instance should avoid to start up on the cluster node running the ERS instance if any other
node is available.
Today the ENSA2 setup can resynchronize the lock table over the network.

If the ASCS instance has been started by the cluster on the ERS node, the ERS instance should
be moved to "another" cluster node (col_sap_{mysid}_no_both). This constraint
is needed to ensure that the ERS instance will synchronize the locks again and the cluster is
ready for an additional take-over.

.Location constraint
================================================
[subs="attributes"]
----
colocation col_sap_{mysid}_no_both -5000: grp_{mySID}_{myInstErs} grp_{mySID}_{myInstAscs}
order ord_sap_{mysid}_first_start_ascs Optional: rsc_sap_{mySID}_{myInstAscs}:start \
      rsc_sap_{mySID}_{myInstErs}:stop symmetrical=false
----
================================================

Create a _txt_ file (like _crm_col.txt_) with your preferred text editor. Add
both constraints to that file and load the configuration to the
cluster manager configuration.

As user _root_, type the following command:

[subs="attributes"]
----
# crm configure load update crm_col.txt
----

==== Activating the Cluster

The last step is to end the cluster maintenance mode and to allow the
cluster to detect already running resources.

As user _root_, type the following command:

[subs="attributes"]
----
# crm configure property maintenance-mode="false"
----

////
############################
#
# ADMINISTRATION
#TODO: renaming Do and Don't to Operational Tasks
############################
////

== Administration

=== Dos and Don'ts

NOTE: Before each test verify that the cluster is in idle state, no migration constraints are active,
and no resource failure messages are visible. Start each procedure with a clean setup.

==== How to migrate the ASCS Instance

To *migrate* the ASCS {sap} instance, you should use {sap} tools such as
the {sap} management console. This will trigger *{sapStartSrv}* to use the
{s4sClConnector3} to migrate the ASCS instance. As user _{mysapadm}_ you can run
the command below to migrate the ASCS. This will always
migrate the ASCS to the ERS side which will keep the {sap} enqueue locks.

As user _{mysapadm}_, type the command:

[subs="attributes"]
----
# sapcontrol -nr 00 -function HAFailoverToNode ""
----

==== Never Block Resources

With {sapCert} it is *not longer allowed to block resources* from being
  controlled manually. This means using the variable _BLOCK_RESOURCES_ in
  _/etc/sap_suse_cluster_connector_ is not allowed anymore.

==== Always Use Unique Instance Numbers

Currently all {sap} *instance numbers controlled by the cluster must be unique*.
  If you need multiple dialog instances, such as D00, running on different
  systems, they should not be controlled by the cluster.

==== How to Set the Cluster in Maintenance Mode

The procedure to set the cluster into maintenance mode can be executed as user _root_ or _sidadm_.

As user _root_, type the following command:

[subs="attributes"]
----
# crm configure properties maintenance-mode="true"
----

As user _{mysapadm}_, type the following command (the full path is needed):

[subs="attributes"]
----
# /usr/sbin/crm configure properties maintenance-mode="true"
----

==== Procedure to End the Cluster Maintenance

The procedure to end the maintenance mode for the cluster can be executed as user _root_.
Type the following command:

[subs="attributes"]
----
# crm configure properties maintenance-mode="false"
----

==== Procedure to start the Resource Maintenance Mode

The procedure to start the resource maintenance mode can be executed as user _{mysapadm}_.
This sets the ASCS and ERS cluster resource to 'unmanaged'.

As user _{mysapadm}_, type the command:

[subs="attributes"]
----
# sapcontrol -nr 00 -function HASetMaintenanceMode 1
----

==== Procedure to Stop the Resource Maintenance Mode

The procedure to start the resource maintenance mode can be executed as user _{mysapadm}_.
This sets the ASCS and ERS cluster resource to 'managed'.

As user _{mysapadm}_, type the command:

[subs="attributes"]
----
# sapcontrol -nr 00 -function HASetMaintenanceMode 0
----

==== Cleanup Resources

You can also *cleanup resource failures*. Failures of the ASCS will be automatically
  deleted to allow a failback after the configured period of time. For all other
  resources, you can cleanup the status including the failures.

As user _root_, type the following command:

[subs="attributes"]
----
# crm resource cleanup RESOURCE-NAME
----

WARNING: Do *not* clean up the complete group of the ASCS resources. This might lead into an unwanted cluster action
and move the complete group to the node where the ERS instance is running.

=== Testing the Cluster

It is strongly recommended to perform at least the following tests before you
go into production with your cluster:

==== Check Product Names with HAGetFailoverConfig

Check if the name of the SUSE cluster solution is shown in the output of
  *sapcontrol* or the {sap} management console. This test checks the status of the
  {sapNW} cluster integration.

As user _{mysapadm}_, type the following command:

[subs="attributes"]
----
# sapcontrol -nr 00 -function HAGetFailoverConfig
----

==== Start SAP Checks Using HACheckConfig and HACheckFailoverConfig

Check if the HA configuration tests are passed successfully and do not produce error messages.

As user _{mysapadm}_, type the following commands:

[subs="attributes"]
----
# sapcontrol -nr 00 -function HACheckConfig
# sapcontrol -nr 00 -function HACheckFailoverConfig
----

==== Manually Migrate ASCS

Check if manually migrating the ASCS instance using HA tools works properly.

As user _root_, type the following commands:

[subs="attributes"]
----
# crm resource migrate rsc_sap_{mySid}_{myInstAscs} force
## wait till the ASCS is been migrated to the ERS host
# crm resource unmigrate rsc_sap_{mySid}_{myInstAscs}
----

==== Migrate ASCS Using HAFailoverToNode

Check if moving the ASCS instance using {sap} tools like {sapCtrl} works properly.

As user _{mysapadm}_, type the following command:

[subs="attributes"]
----
# sapcontrol -nr 00 -function HAFailoverToNode ""
----

==== Test ASCS Migration after OS Failure

Check if the ASCS instance moves correctly after a node failure.
This test will immediately trigger a hard reboot of the node.

As user _root_, type the following command:

[subs="attributes"]
----
## on the ASCS host
# echo b >/proc/sysrq-trigger
----

==== In-place Restart of ASCS Using Stop and Start

Check if the in-place restart of the {sap} resources have been processed
  correctly. The {sap} instance should not failover to an other node, it
  must start on the same node where it has been stopped.

////
TODO: Is the statement still valid? I guess no.
WARNING: This test will force the SAP system to *lose* the enqueue locks.
   *This test should not be processed during production.*
////
As user _{mysapadm}_, do the following:

[subs="attributes"]
----
## example for ASCS
# sapcontrol -nr 00 -function Stop
## wait till the ASCS is completly down
# sapcontrol -nr 00 -function Start
----

==== Automated Restart of the ASCS Instance (Simulating Rolling Kernel Switch)

The next test should proof that the cluster solution did nor interact neither try to restart the ASCS instance
during a maintenance procedure. In addition, it should verify that no locks are lost during the restart of
an ASCS instance during an RKS procedure. The cluster solution should recognize that the restart of
the ASCS instance was expected. No failure or error should be reported or counted.

Optionally you can set some locks and verify if they still exist after the maintenance procedure. There are multiple
ways to do that. One example test can be performed as follows:
- Log in to your SAP system and open the transaction SU01.
- Create a new user.
- With the SAP MC / MMC, check if there are locks available.
- Open the ASCS instance entry and go to _Enqueue Locks_.
- With the transaction SM12 you can also see the locks.
Do this test multiple times in a short time frame. The restart of the ASCS instance in the example below happens five times.

As user _{mysapadm}_, do the following:

[subs="attributes"]
----
# bash -c 'for lo in 1 2 3 4 5; do echo LOOP "$lo - Restart {myInstAscs}"; sapcontrol -host {myVipNAscs} -nr {myAscsIno} -function StopWait 120 1; sleep 1; sapcontrol -host {myVipNAscs} -nr {myAscsIno} -function StartWait 120 1; sleep 1; done'
----

==== Do a Rolling Kernel Switch Procedure

The rolling kernel switch (RKS) is an automated procedure that enables the kernel in an ABAP system
to be exchanged without any system downtime. During an RKS, all instances of the system, and
generally all SAP start services (sapstartsrv), are restarted.

- Check in SAP note 953653 whether the new kernel patch is RKS compatible to your currently running kernel.
- Check SAP note 2077934 - Rolling kernel switch in HA environments.
- Download the new kernel from the SAP service market place.
- Make a backup of your current central kernel directory.
- Extract the new kernel archive to the central kernel directory.
- Start the RKS via SAP MMC, system overview (transaction SM51) or via command line.
- Monitor and check the version of your SAP instances with the SAP MC / MMC or with *sapcontrol*.

As user _{mysapadm}_, type the following commands:

[subs="specialchars,attributes"]
----
## sapcontrol [-user <sidadm psw>] -host <host> -nr <INSTANCE_NR> -function UpdateSystem 120 300 1
# sapcontrol -user {mySapAdm} {mySapPwd} -host {myVipNAscs} -nr {myAscsIno} -function UpdateSystem 120 300 1
# sapcontrol -nr {myAscsIno} -function GetSystemUpdateList -host {myVipNAscs} -user {mySapAdm} {mySapPwd}
# sapcontrol -nr {myAscsIno} -function GetVersionInfo -host {myVipNAscs} -user {mySapAdm} {mySapPwd}
# sapcontrol -nr {myErsIno} -function GetVersionInfo -host {myVipNErs} -user {mySapAdm} {mySapPwd}
# sapcontrol -nr {myPasIno} -function GetVersionInfo -host {myVipNPas} -user {mySapAdm} {mySapPwd}
# sapcontrol -nr {myDSecIno} -function GetVersionInfo -host {myVipNDSec} -user {mySapAdm} {mySapPwd}
----


==== Additional Test

In addition to the already performed tests, you should do the following:

* Check the recoverable and non-recoverable outage of the message server process.

* Check the non-recoverable outage of the {sap} enqueue server process.

* Check the outage of the {sapERS}.

* Check the outage and restart of {sapStartSrv}.

* Check the simulation of an upgrade.

* Check the simulation of cluster resource failures.

//:leveloffset: 0

////
############################
#
# APENDIX
#
############################
////



== Appendix

=== CRM Configuration

Find below the complete crm configuration for {sap} system {mySid}.

[subs="attributes"]
----
node 1084752231: {my2nd1}
node 1084752232: {my2nd2}
primitive rsc_fs_{mySID}_{myInstAscs} Filesystem \
	params device="{myDevPartAscs}" directory="/usr/sap/{mySID}/{myInstAscs}" \
     fstype=xfs \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=40s
primitive rsc_fs_{mySID}_{myInstErs} Filesystem \
	params device="{myDevPartErs}" directory="/usr/sap/{mySID}/{myInstErs}" \
     fstype=xfs \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=40s
primitive rsc_ip_{mySID}_{myInstAscs} IPaddr2 \
	params ip={myVipAAscs} \
	op monitor interval=10s timeout=20s
primitive rsc_ip_{mySID}_{myInstErs} IPaddr2 \
	params ip={myVipAErs} \
	op monitor interval=10s timeout=20s
primitive rsc_sap_{mySID}_{myInstAscs} SAPInstance \
	operations $id=rsc_sap_{mySID}_{myInstAscs}-operations \
	op monitor interval=11 timeout=60 on_fail=restart \
	params InstanceName={mySID}_{myInstAscs}_{myVipNAscs} \
    START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstAscs}_{myVipNAscs}" \
    AUTOMATIC_RECOVER=false \
	meta resource-stickiness=5000
primitive rsc_sap_{mySID}_{myInstErs} SAPInstance \
    operations $id=rsc_sap_{mySID}_{myInstErs}-operations \
    op monitor interval=11 timeout=60 on_fail=restart \
    params InstanceName={mySID}_{myInstErs}_{myVipNErs} \
    START_PROFILE="/sapmnt/{mySID}/profile/{mySID}_{myInstErs}_{myVipNErs}" \
    AUTOMATIC_RECOVER=false IS_ERS=true \
    meta priority=1000
primitive stonith-sbd stonith:external/sbd \
    params pcmk_delay_max=30s
group grp_{mySID}_{myInstAscs} rsc_ip_{mySID}_{myInstAscs} rsc_fs_{mySID}_{myInstAscs} \
    rsc_sap_{mySID}_{myInstAscs} \
    meta resource-stickiness=3000
group grp_{mySID}_{myInstErs} rsc_ip_{mySID}_{myInstErs} rsc_fs_{mySID}_{myInstErs} \
    rsc_sap_{mySID}_{myInstErs}
colocation col_sap_{mySid}_not_both -5000: grp_{mySID}_{myInstErs} grp_{mySID}_{myInstAscs}
order ord_sap_{mySid}_first_ascs Optional: rsc_sap_{mySID}_{myInstAscs}:start \
    rsc_sap_{mySID}_{myInstErs}:stop symmetrical=false
property cib-bootstrap-options: \
    have-watchdog=true \
    dc-version="1.1.18+20180430.b12c320f5-1.14-b12c320f5" \
    cluster-infrastructure=corosync \
    cluster-name=hacluster \
    stonith-enabled=true \
    placement-strategy=balanced \
    last-lrm-refresh=1548155187
rsc_defaults rsc-options: \
    resource-stickiness=1 \
    migration-threshold=3
op_defaults op-options: \
    timeout=600 \
    record-pending=true
----

=== Corosync Configuration

Find below the Corosync configuration.

[subs="attributes"]
----
# Read the corosync.conf.5 manual page
totem {
	version: 2
	secauth: on
	crypto_hash: sha1
	crypto_cipher: aes256
	cluster_name: hacluster
	clear_node_high_bit: yes
	token: 5000
	token_retransmits_before_loss_const: 10
	join: 60
	consensus: 6000
	max_messages: 20
	interface {
		ringnumber: 0
		mcastport: 5405
		ttl: 1
	}

	transport: udpu
}

logging {
	fileline: off
	to_stderr: no
	to_logfile: no
	logfile: /var/log/cluster/corosync.log
	to_syslog: yes
	debug: off
	timestamp: on
	logger_subsys {
		subsys: QUORUM
		debug: off
	}

}

nodelist {
	node {
		ring0_addr: {myIP2nd1}
		nodeid: 1
	}

	node {
		ring0_addr: {myIP2nd2}
		nodeid: 2
	}
}

quorum {

	# Enable and configure quorum subsystem (default: off)
	# see also corosync.conf.5 and votequorum.5
	provider: corosync_votequorum
	expected_votes: 2
	two_node: 1
}
----

////
TODO: 4node example

valuga21:~ # cat /etc/corosync/corosync.conf
# Read the corosync.conf.5 manual page
totem {
	version: 2
	secauth: on
	crypto_hash: sha1
	crypto_cipher: aes256
	cluster_name: hacluster
	clear_node_high_bit: yes
	token: 5000
	token_retransmits_before_loss_const: 10
	join: 60
	consensus: 6000
	max_messages: 20
	interface {
		ringnumber: 0
		mcastport: 5405
		ttl: 1
	}

	transport: udpu
}

logging {
	fileline: off
	to_stderr: no
	to_logfile: no
	logfile: /var/log/cluster/corosync.log
	to_syslog: yes
	debug: off
	timestamp: on
	logger_subsys {
		subsys: QUORUM
		debug: off
	}

}

nodelist {
	node {
		ring0_addr: 192.168.1.105
		nodeid: 1
	}

	node {
		ring0_addr: 192.168.1.106
		nodeid: 2
	}

	node {
		ring0_addr: 192.168.1.107
		nodeid: 3
	}

	node {
		ring0_addr: 192.168.1.108
		nodeid: 4
	}

}

quorum {

	# Enable and configure quorum subsystem (default: off)
	# see also corosync.conf.5 and votequorum.5
	provider: corosync_votequorum
	expected_votes: 4
	two_node: 0
}

valuga21:~ # cat 4node.txt
node 1: valuga21
node 2: valuga22
node 3: valuga23
node 4: valuga24
primitive rsc_fs_EN2_ASCS00 Filesystem \
	params device="/dev/sda2" directory="/usr/sap/EN2/ASCS00" fstype=xfs \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=40s
primitive rsc_fs_EN2_ERS10 Filesystem \
	params device="/dev/sda3" directory="/usr/sap/EN2/ERS10" fstype=xfs \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=40s
primitive rsc_ip_EN2_ASCS00 IPaddr2 \
	params ip=192.168.1.112 \
	op monitor interval=10s timeout=20s
primitive rsc_ip_EN2_ERS10 IPaddr2 \
	params ip=192.168.1.113 \
	op monitor interval=10s timeout=20s
primitive rsc_sap_EN2_ASCS00 SAPInstance \
	operations $id=rsc_sap_EN2_ASCS00-operations \
	op monitor interval=11 timeout=60 \
	op_params on_fail=restart \
	params InstanceName=EN2_ASCS00_sapen2as START_PROFILE="/sapmnt/EN2/profile/EN2_ASCS00_sapen2as" AUTOMATIC_RECOVER=false \
	meta resource-stickiness=5000 target-role=Started maintenance=false
primitive rsc_sap_EN2_ERS10 SAPInstance \
	operations $id=rsc_sap_EN2_ERS10-operations \
	op monitor interval=11 timeout=60 \
	op_params on_fail=restart \
	params InstanceName=EN2_ERS10_sapen2er START_PROFILE="/sapmnt/EN2/profile/EN2_ERS10_sapen2er" AUTOMATIC_RECOVER=false IS_ERS=true \
	meta target-role=Started maintenance=false
primitive stonith-sbd stonith:external/sbd \
	params pcmk_delay_max=30s pcmk_action_limit=-1
group grp_EN2_ASCS00 rsc_ip_EN2_ASCS00 rsc_fs_EN2_ASCS00 rsc_sap_EN2_ASCS00 \
	meta resource-stickiness=3000 target-role=Started
group grp_EN2_ERS10 rsc_ip_EN2_ERS10 rsc_fs_EN2_ERS10 rsc_sap_EN2_ERS10 \
	meta target-role=Started
colocation ASCS00_ERS10_separated_EN2 -50000: grp_EN2_ERS10 grp_EN2_ASCS00
#
# currently not helpful on 2 node clusters
#
#colocation col_sap_EN2_no_both -5000: grp_EN2_ERS10 grp_EN2_ASCS00
#
# not needed for ENSA2
#location loc_sap_EN2_failover_to_ers rsc_sap_EN2_ASCS00 #         rule 2000: runs_ers_EN2 eq 1
order ord_sap_EN2_first_start_ascs Optional: rsc_sap_EN2_ASCS00:start rsc_sap_EN2_ERS10:stop symmetrical=false
property cib-bootstrap-options: \
	have-watchdog=true \
	dc-version="1.1.18+20180430.b12c320f5-1.14-b12c320f5" \
	cluster-infrastructure=corosync \
	cluster-name=hacluster \
	stonith-enabled=true \
	placement-strategy=balanced \
	maintenance-mode=false \
	last-lrm-refresh=1548243394
rsc_defaults rsc-options: \
	resource-stickiness=1 \
	migration-threshold=3
op_defaults op-options: \
	timeout=600 \
	record-pending=true
////

== Reference

For more detailled information, have a look at the documents listed below.

=== Pacemaker
- Pacemaker 1.1 Configuration Explained
https://clusterlabs.org/pacemaker/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/.

:leveloffset: 2
include::SAPNotes_s4_1809.adoc[]

:leveloffset: 0
// Standard SUSE Best Practices includes
== Legal Notice
include::common_sbp_legal_notice.adoc[]


// Standard SUSE Best Practices includes
include::common_gfdl1.2_i.adoc[]

//
// REVISION 1.0 2019/01
//   - ENSA2 cluster setup 2 nodes
