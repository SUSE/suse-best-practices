:docinfo:

:localdate:

// Document Variables
:slesProdVersion: 15
//

= SAP Convergent Mediation ControlZone High Availability Cluster: Setup Guide

// Revision {Revision} from {docdate}
// Standard SUSE includes
// include::common_copyright_gfdl.adoc[]

// :toc:

include::Variables_s4_2021.adoc[]
//

////
TODO PRIOx: example
////

== About this guide

The following sections focus on background information and the purpose of the document at hand.

=== Introduction

{sles4sapReg} is the optimal platform to
run {sapReg} applications with high availability. Together with a redundant layout
of the technical infrastructure, single points of failure can be eliminated.

TODO


=== Abstract

This guide describes planning, setup, and basic testing of {sles4sap} 15 (TODO variable)
as an high availability cluster for an {sap} Convergent Mediation ControlZone platform.

TODO

From the application perspective the following variants are covered:

- Convergent Mediation platform service running alone

- Convergent Mediation platform and UI services running together

- Convergent Meditation binaries stored and started on central NFS (not recommended)

- Convergent Meditation binaries copied to and started from local disks

- TODO

From the infrastructure perspective the following variants are covered:

- 2-site cluster with disk-based SBD fencing

- 3-site cluster with disk-based or diskless SBD fencing, not explaind in detail here

- Other fencing is possible, but not explained here

- On-premises deployment on physical and virtual machines

- Public cloud deployment (usually needs additional documentation on cloud specific details)

Deployment automation simplifies roll-out. There are several options available, particularly on public cloud platfoms. Ask your public cloud provider or your SUSE contact for details.


[id="sec.resources"]
=== Additional documentation and resources

Several chapters in this document contain links to additional documentation resources that
are either available on the system or on the Internet.

For the latest product documentation updates, see https://documentation.suse.com/.

More whitepapers, guides and best practices documents referring to SUSE Linux Enterprise Server and SAP can be
found and downloaded at the SUSE Best Practices Web page:
https://documentation.suse.com/sbp/sap/

Here you can access guides for {SAPHANA} system replication
automation and High Availability (HA) scenarios for {SAPNw} and {s4hana}.

Additional resources, such as customer references, brochures or flyers, can be found at
the {sles4sap} resource library:
https://www.suse.com/products/sles-for-sap/resource-library/.

Supported high availability solutions by {sles4sap} overview:
https://documentation.suse.com/sles-sap/sap-ha-support/html/sap-ha-support/article-sap-ha-support.html

Lastly, there are manual pages shipped with the product.


// Standard SUSE includes
=== Feedback
include::common_intro_feedback.adoc[]



== Overview

TODO
Convergent Mediation (CM)

The CM ControlZone platform is responsible for providing services to other instances.
Several platform containers may exist in a CM system, for high availability,
but only one is active at a time. The CM central ControlZone UI is used to query, edit,
import, and export data.

NFS shares with work directories can be mounted statically on all nodes. The HA cluster
does not need to control that filesystems.
TODO

=== High availabilty for the Convergent Mediation ControlZone platform

TODO
The ControlZone services platform, and optinally UI, are handled as active/passive
resources. The related virtual IP adress is managed by the HA cluster as well.
The HA cluster does not control filesystems used by the ControlZone services. However,
optionally this filesystem could be monitored.

TODO picture

=== Handling of NFS failures

A shared filesystem migth be statically mounted by OS on both cluster nodes. This
filesystem holds work directories. It must not be confused with the ControlZone
application itself. Client-side write caching has to be disabled.

A Filesystem resource is configured for a bind-mount of the real NFS share. This resource
is grouped with the ControlZone platform and IP address. In case of filesystem failures,
the whole group gets restarted. No mount or umount on the real NFS share is done.

TODO this filesyetm resource is optional


=== Scope of this document

The document at hand explains how to:

TODO

[id="sec.prerequisites"]
=== Prerequisites

TODO Requirements of Convergent Mediation ControlZone 

TODO Requirements of the SUSE high availability solution for CM ControlZone are:

- Convergent Mediation ControlZone version 9.0.0.0  or higher is installed and 
configured  on both cluster nodes. If the software is installed into a shared NFS
filesystem, the binaries are copied into both cluster nodes´ local filesystems.

- Only one ControlZone instance per Linux cluster.

- Technical  users and groups are defined locally in the Linux system. If users are
resolved by remote service, local caching  is  neccessary. Substitute  user (su) to
the mz-user (e.g. "mzadmin") needs to work reliable and without customized actions or
messages.

- Strict time synchronization between the cluster nodes, e.g. NTP.  All nodes of a
cluster have configured the same timezone.

- Needed NFS shares (e.g. /mnt/platform/) mounted statically or by automounter.
No client-side write caching.

- The RA monitoring operations have to be active.

- RA runtime almost completely depends on call-outs to controlled resources, OS and
Linux cluster. The infrastructure needs to allow these call-outs to return in time.

- The ControlZone application is not started/stopped by OS. Thus there is no SystemV,
systemd or cron job.

- As long as the ControlZone application is managed by the Linux cluster, the application
is not started/stopped/moved from outside. Thus no manual actions are done.

- Interface for the RA to the ControlZone platform is the command mzsh.
The mzsh is accessed on the cluster nodes´ local filesystems. The mzsh is called
with the arguments startup, shutdown, status and kill. Its output is parsed by the RA.
Thus the command and its output needs to be stable.

- The mzsh is called on the active node with a defined interval for regular resource
monitor operations. It also is called on the active or passive node in certain situations. Those calls might run in parallel.

- TODO


=== The setup procedure at a glance
 
TODO

- Checking the operating system 
// [id="sec.os-basic-check"]

- Checking the HA cluster basic setup
// [id="sec.ha-basic-check"]

- Preparing the OS for NFS monitoring
// [id="sec.ha-filsystem-monitor"]

- Adapting the cluster basic configuration
// [id="sec.basic-ha-cib"]

- Configuring the ControlZone cluster resources
// [id="sec.cm-ha-cib"]

- Testing the HA cluster
//  [id="sec.testing"]


== Checking the operating system and the HA cluster basic setup

TODO

[id="sec.os-basic-check"]
=== Checking the operating system

TODO on both nodes

==== IP addresses and virtual names

Check if the file _/etc/hosts_ contains at least the following address resolutions.
Add those entries if they are missing.

[subs="attributes"]
----
{myIPNode1}  {myNode1}
{myIPNode2}  {myNode2}

{myVipAAscs}  sap{mySidLc}as
----

==== Mount points and NFS shares

TODO

==== Linux user and group number scheme

TODO

==== Password-free ssh login

TODO

==== Time synchronisation

TODO

[id="sec.ha-basic-check"]
=== Checking the HA cluster basic setup

TODO on both nodes

==== Watchdog

TODO

==== SBD device

TODO

==== Corosync cluster communication

TODO 

==== systemd cluster services

TODO

==== Basic Linux cluster configuration

TODO on one node


== Integrating Convergent Mediation ControlZone with the Linux cluster

TODO

[id="sec.ha-filsystem-monitor"]
=== Preparing the OS for NFS monitoring

TODO on both nodes

[id="sec.basic-ha-cib"]
=== Adapting the cluster basic configuration

TODO

==== Adapting cluster bootstrap options and resource defaults

TODO

[subs="specialchars,attributes"]
----
property cib-bootstrap-options: \
    have-watchdog=true \
    cluster-infrastructure=corosync \
    cluster-name=hacluster \
    dc-deadtime=20 \
    stonith-enabled=true \
    stonith-timeout=150 \
    priority-fencing-delay=30 \
    stonith-action=reboot
rsc_defaults rsc-options: \
    resource-stickiness=1 \
    migration-threshold=3 \
    failure-timeout=86400
op_defaults op-options: \
    timeout=120
----

==== Adapting SBD STONITH resource

TODO for priority fencing

[subs="specialchars,attributes"]
----
primitive rsc_stonith_sbd stonith:external/sbd \
    params pcmk_delay_max=15
----

[id="sec.cm-ha-cib"]
=== Configuring ControlZone cluster resources

TODO

==== Virtual IP address resource

TODO

[subs="specialchars,attributes"]
----
primitive rsc_ip_{mySid} ocf:heartbeat:IPaddr2 \
    op monitor interval=60 timeout=20 on-fail=restart \
    params ip={myVipAAscs}
----

See manual page ocf_heartbeat_IPAddr2(7) for more details.

==== Filesystem resource (only monitoring)

TODO 

[subs="specialchars,attributes"]
----
primitive rsc_fs_{mySid} ocf:heartbeat:Filesystem \
    params device=/mnt/platform/check/ directory=/mnt/check/ \
    fstype=nfs4 options=bind,rw,noac,sync,defaults \
    op monitor interval=120 timeout=120 on-fail=restart \
    op_params OCF_CHECK_LEVEL=20 \
    op start timeout=120 \
    op stop timeout=120 \
    meta target-role=stopped
----

See manual page ocf_heartbeat_Filesystem(7) for more details.

==== SAP Convergent Mediation ControlZone resource

TODO

[subs="specialchars,attributes"]
----
primitive rsc_cz_{mySid} ocf:suse:SAPCMControlZone \
    params SERVICE=platform MZSHELL=/opt/mz/bin/mzsh \
    op monitor interval=60 timeout=120 on-fail=restart \
    op start timeout=120 interval=0 \
    op stop timeout=120 interval=0 \
    meta priority=100 maintenance=true
----

// [cols="1,2", options="header"]
[width="100%",cols="30%,70%",options="header"]
.Table Description of important resource agent parameters
|===
|Name
|Description

|USER
|OS user who calls mzsh, owner of $MZ_HOME.
Optional. Unique, string. Default value: "mzadmin".

|SERVICE
|The ControlZone service to be managed by the resoure agent.
Optional. Unique, [ platform \| ui ]. Default value: "platform".

|MZSHELL
|Path to mzsh.
Optional. Unique, string. Default value: "/usr/bin/mzsh".

|CALL_TIMEOUT
|Define timeout how long calls to the ControlZone platform for checking the
status can take. If the timeout is reached, the return code will be 124. If you
increase this timeout for ControlZone calls, you should also adjust the monitor
operation timeout of your Linux cluster resources. (Not yet implemented.)
Optional. Unique, integer. Default value: 60.

|SHUTDOWN_RETRIES
|Number of retries to check for process shutdown. Passed to mzsh.
If you increase the number of shutdown retries, you should also adjust the stop
operation timeout of your Linux cluster resources. (Not yet implemented.)
Optional. Unique, integer. Default: mzsh builtin value.
|===

See manual page ocf_suse_SAPCNControlZone(7) for more details.

==== ControlZone resource group

TODO

[subs="specialchars,attributes"]
----
group grp_cz_{mySid} rsc_fs_{mySid} rsc_ip_{mySid} rsc_cz_{mySid} \
    meta maintenance=true
----

=== Activating the cluster resources

TODO

[subs="specialchars,attributes"]
----
# crm resource maintenance grp_cz_{mySid} off
----

=== Checking the cluster resource configuration

TODO

[subs="specialchars,attributes"]
----
# crm_mon -1r
----

TODO Congratulations!

[subs="specialchars,attributes"]
----
# crm configure show
----

[id="sec.testing"]
=== Testing the cluster

TODO

==== Manually restarting ControlZone resources in-place

TODO

==== Manually migrating ControlZone resources

TODO

==== Testing ControlZone restart by cluster on application failure

TODO

==== Testing ControlZone migration by cluster on operating system failure

TODO

==== Testing ControlZone migration by cluster on NFS failure

TODO

==== Testing cluster reaction on network split-brain

TODO

==== Additional tests

TODO basic cluster tests

TODO


== Administration

=== Dos and don'ts

TODO

==== Stopping an starting the ControlZone resources

TODO

==== Migrating the ControlZone resources

TODO

==== Setting ControlZone resources into maintenance mode

TODO

==== Ending ControlZone resources maintenance

TODO

==== Cleaning up resources

TODO



[id="sec.references"]
== References

For more information, see the documents listed below.

=== Pacemaker
- Pacemaker documentation online:
https://clusterlabs.org/pacemaker/doc/

:leveloffset: 2
include::SAPNotes-convergent-mediation.adoc[]

++++
<?pdfpagebreak?>
++++

////
############################
#
# APPENDIX
#
############################
////

:leveloffset: 0
[id="sec.appendix"]
== Appendix

=== CRM configuration for a minimal setup

Find below a minimal CRM configuration for an CM ControlZone platform instance,
with the platform service and its IP address.
Ideally a filesystem resource would be included in the group. Also an UI instance
could be included.

[subs="specialchars,attributes"]
----
{myNode1}:~ # crm configure show 
node 1: {myNode1}
node 2: {myNode2}
#
primitive rsc_cz_{mySid} ocf:suse:SAPCMControlZone \
        params SERVICE=platform MZSHELL="/opt/mz/bin/mzsh" \
        op monitor interval=60 timeout=120 on-fail=restart \
        op start timeout=120 interval=0 \
        op stop timeout=120 interval=0 \
        meta priority=100
#
primitive rsc_ip_{mySid} IPaddr2 \
        params ip={myVipAAscs} \
        op monitor interval=60 timeout=20 on-fail=restart
#
primitive rsc_stonith_sbd stonith:external/sbd \
        params pcmk_delay_max=15
#
group grp_cz_{mySid} rsc_ip_{mySid} rsc_cz_{mySid}
#
property cib-bootstrap-options: \
        have-watchdog=true \
        dc-version="2.1.2+20211124.ada5c3b36-150400.2.43-2.1.2+20211124.ada5c3b36" \
        cluster-infrastructure=corosync \
        cluster-name=hacluster \
        dc-deadtime=20 \
        stonith-enabled=true \
        stonith-timeout=150 \
        stonith-action=reboot \
        last-lrm-refresh=1704707877 \
        priority-fencing-delay=30
rsc_defaults rsc-options: \
        resource-stickiness=1 \
        migration-threshold=3 \
        failure-timeout=86400
op_defaults op-options: \
        timeout=120
----

=== Corosync configuration of the two-node cluster

Find below the corosync configuration for one corosync ring. Ideally two rings would be used.

[subs="specialchars,attributes"]
----
{myNode1}:~ # cat /etc/corosync/corosync.conf
# Read the corosync.conf.5 manual page
totem {
    version: 2
    secauth: on
    crypto_hash: sha1
    crypto_cipher: aes256
    cluster_name: hacluster
    clear_node_high_bit: yes
    token: 5000
    token_retransmits_before_loss_const: 10
    join: 60
    consensus: 6000
    max_messages: 20
    interface {
        ringnumber: 0
        mcastport: 5405
        ttl: 1
    }
    transport: udpu
}

logging {
    fileline: off
    to_stderr: no
    to_logfile: no
    logfile: /var/log/cluster/corosync.log
    to_syslog: yes
    debug: off
    timestamp: on
    logger_subsys {
        subsys: QUORUM
        debug: off
    }
}

nodelist {
    node {
        ring0_addr: {myIP2nd1}
        nodeid: 1
    }

    node {
        ring0_addr: {myIP2nd2}
        nodeid: 2
    }
}

quorum {

    # Enable and configure quorum subsystem (default: off)
    # see also corosync.conf.5 and votequorum.5
    provider: corosync_votequorum
    expected_votes: 2
    two_node: 1
}
----

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
== Legal notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
:leveloffset: 0
include::common_gfdl1.2_i.adoc[]

//
// REVISION 0.1 2024/01
//
