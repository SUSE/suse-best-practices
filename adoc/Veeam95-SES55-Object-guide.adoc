ifdef::env-daps[]
:imgpath:
endif::[]
ifndef::env-daps[]
:imgpath: ../images/src/png/
endif::[]
:SUSEProduct: SUSE Enterprise Storage
:SESversion: (v5.5)
:SLESversion: 12 SP3
:vendor: Veeam
:v: Veeam
:vplatform: Backup & Replication
:vplatform1: Object Storage Repository
:vplatformver: 9.5
:vvalidationlink1: https://www.veeam.com/ready.html
:vvalidationlink2: https://www.veeam.com/ready.html

:docinfo:

= SUSE Enterprise Storage with {vendor} {vplatform}: Implementation Guide

////
''''
== Solution Components
SUSE::

{SUSEProduct} {SESversion}

{vendor}::

{vendor} {vplatform} {vplatformver}
////

== Introduction
The objective of this guide is to present instructions on how to implement {SUSEProduct} {SESversion} with {vendor} {vplatform} 
as both a Linux repository and an S3 target as part of a Scale Out Backup Repository. It is suggested that the document be read in its entirety, 
along with the supplemental <<appendix>> information, before attempting the process.

The deployment presented in this guide aligns with architectural best practices of both SUSE and {vendor}.

Upon completion of the steps in this document, a working SUSE Enterprise Storage {SESversion} cluster will be operational as described in the 
https://www.suse.com/documentation/suse-enterprise-storage-5/book_storage_deployment/data/book_storage_deployment.html[SUSE Enterprise Storage Deployment Guide] 
and integrated with {vendor} {vplatform}.

== Solution Description
The solution outlined in this guide enables a customer to deploy a disk-to-disk target that is orchestrated through Veeam software. 
SUSE Enterprise Storage can be used as a backup target via a Veeam proxy over a common network. The result is a high-performing and flexible backup target with exabyte scalability. 


== Business Value
*SUSE Enterprise Storage*

SUSE Enterprise Storage provides a Veeam disk-to-disk backup solution with:

* commodity hardware for minimal hardware cost.
* open source software for minimal software cost and maximum flexibility.
* a self-managing, self-healing architecture for minimal management cost.
* a flexible, cluster-based design for graceful and inexpensive upgrade and innovative licensing model that avoids per-gigabyte storage charges, so you will not owe more for saving more data.
* the lowest-price solution for enterprise archive and backup implementations with minimal acquisition, management, and upgrade cost.

*{vendor} {vplatform}*

{vendor} {vplatform} delivers availability for all your virtual, physical and cloud-based workloads. 
Through a single management console, you can manage fast, flexible and reliable backup, recovery and replication of all your applications and 
data to eliminate legacy backup solutions. The solution includes native, certified SAP support for backups and recoveries. 

Together, {vendor} and SUSE deliver the flexibility and near-unlimited scalability you want for long-term data retention, 
plus a single storage architecture that delivers the various performance requirements a {vendor} backup solution needs. 
It is ideal for mission-critical applications and platforms such as SAP HANA, and allows you to recover vital data fast when failures occur.


*Solution Value Propositions*

* *Issue*: Customer needs to handle more simultaneous backup streams but not capacity. 
+
*Solution*: In large environments with many backup streams, adding more Veeam proxies and Linux repository servers may handle more simultaneous backups.

* *Issue*: Customer needs more storage, but not more simultaneous streams. 
+
*Solution*: Add more Object Storage Devices (OSDs) to the SUSE Enterprise Storage cluster.

* *Issue*: Customer wants to add S3 repository on-prem for long term archive. 
+
*Solution*: Deploy Rados Gateways (RGWs) for SUSE Enterprise Storage and implement the S3 repo.

* *Issue*: Customer has multiple sites with small local Veeam repositories, but wants to replicate to a central Veeam location. 
+
*Solution*: Deploy Veeam server and Linux repositories with SUSE Enterprise Server as central site OR deploy SUSE Enterprise Storage with S3 to act as central S3 repository.

One of the huge advantages is that every Veeam backup can benefit from the aggregated throughput of the cluster.  
This brings both performance and storage efficiency. Instead of being limited to the throughput capacity of a single server, 
the I/O is spread across ALL the storage nodes. It also means that there won’t be one storage enclosure that is maxed out on I/O capability while another is sitting idle.  
This is all done without using Veeam’s Scale Out Backup Repository.﻿


== Requirements

The solution has the following requirements:

* Simple to setup and deploy 
* Able to meet the documented guidelines for system hardware, networking and environmental prerequisites
* Adaptable to the physical and logical constraints needed by the business, both initially and as needed over time for performance, security, and scalability concerns
* Resilient to changes in physical infrastructure components, caused by failure or required maintenance
* Capable of providing optimized object and block services to client access nodes, either directly or through gateway services
* Data protection configurable to meet the customer's individual needs at a granular level 


== Architectural Overview
{SUSEProduct} provides everything that Veeam needs for storage, from the high-performance tier to the Cloud Tier repository.  

image::{imgpath}VeeamArchitecturewSES.png[Veeam and SES Architecture, scaledwidth=100%]

=== Solution Architecture - RBD
SUSE Enterprise Storage can be used as a storage location for Veeam via a Veeam Linux repository. 
The architecture and settings described below were used during testing to achieve the repository level of validation.

The architecture used to achieve compliance with the {vvalidationlink1}[Veeam Ready program] uses a RADOS Block Device on a Veeam Linux repository server. 
This paper will also briefly discuss CephFS as a potential storage mechanism.

=== Solution Architecture - S3
{vendor} {vplatform} enables the usage of storage targets that are compatible with specific S3 API calls.  
SUSE Enterprise Storage provides a target that is certified with Veeam software. 

=== Architectural Notes and Discussion
For both RBD and CephFS, having both the proxy and the Linux repository server can offer several benefits:

.	The Veeam Proxy server located on the ESX server is able to directly mount the VMware snapshot images, 
resulting in the highest possible streaming read performance for the backups. The figure below illustrates the traffic flow of a backup process.
.	In large deployments, network communication from the Veeam Proxy to the Linux Repository server flows across the ESX server without traversing the physical network infrastructure. 
This results in very high network performance between these two critical pieces of infrastructure for the Veeam Backup and Restore environment.

image::{imgpath}VeeamDataFlow.png[Veeam Data Flow, scaledwidth=70%]


== Pool Configuration
When configuring the SUSE Enterprise Storage cluster for use as a backup target, the data protection scheme is an important consideration. 
There are two main options for data protection, each with advantages and disadvantages. 

The first scheme is replication. It works by replicating each data chunk on each of the specified number of unique devices. The default is three. 
If the failure domain is assumed to be at the storage host level, this means the cluster could survive the loss of two storage servers without data loss. 
The downside of replication is the space overhead, which is 200 percent or two thirds of the total cluster capacity. 

The performance characteristics of replication are that it has lower latency than erasure coding.  
This is especially true where the I/O pattern is that of small random I/O. 

The second scheme is erasure coding (EC). It works by splitting the data into the specified number of chunks (k) and then performing a mathematical calculation to create the requested number of EC chunks (m). 
Again, assuming the failure domain is at the host level, a system using an EC scheme of k=6, m=3 has an overhead of only 50 percent, or one third of the total cluster capacity. 
Because EC actually writes less data, it is sometimes faster than replication for writes, but slower on the reads due to the requirement to reassemble the data from multiple nodes.

Another aspect to consider is the total cluster size. In general, it is not recommended to use EC with a cluster of fewer than seven storage nodes. 
When using EC with SUSE Enterprise Storage, it is recommended that the data chunks + (2x erasure coding chunks) are less than or equal to the cluster node count. 

Expressed in a formula this looks as follows: 


[source]
data chunks [k] + (coding chunks [m] * 2) <= cluster node count


A cluster size of seven would thus allow for three data chunks plus 2 erasure coding chunks plus 2 spare nodes to allow for device failures. 
In a larger cluster, EC profiles of 8+3, 6+4, 9+3 and the like are not uncommon and represent superior percentages of storage available for data.

An additional consideration is the availability of hardware accelerators for erasure coding. 
Intel CPUs provide such an accelerator, which is specified with the plug-in option when creating the erasure coding profile for the pool. 

[source]
ceph osd erasure-code-profile set veeam_ec plugin=isa k=8 m=3


=== Ceph Protocol – RBD
The RBD protocol is the native block protocol for Ceph. Clients leveraging RBD could be termed “intelligent” because they are able 
to leverage the CRUSH algorithm to determine where data will be placed. Thus they can communicate directly to each individual storage device. 
The result is performance that scales horizontally with the cluster. 

As a client protocol, RBD has numerous tuning options that can be controlled on each client, or for the cluster as a whole. 
These include things like caching type, size, etc. For this effort, some tuning was performed for the caching parameters to optimize performance for the I/O patterns being tested. 
These are outlined in the deployment section below.

The Veeam Linux repository maps the RBD device created as a block device, and then a file system is placed on it. 
This allows for tuning that can be applied to the particular filesystem you plan to use and to accelerate performance.
 
=== Ceph protocol – CephFS
CephFS is a distributed file system available for and integrated with Veeam. 
Our testing showed similar or better performance as RBD. An advantage of this particular protocol choice is that multiple 
repositories can be hosted on the same massively scalable distributed file system. This also means that if a backup server disappears or fails, 
it is quite simple to add the repository to another server.

=== Ceph Protocol - S3
The S3 protocol has become the de facto standard for use in developing web-scale friendly applications that store and retrieve data.  
The protocol uses either HTTP or HTTPS as the data transport protocol. This makes it capable of leveraging standard load-balancing and proxy technologies to ensure scalability and improved security. 

== Deployment Recommendations

This deployment section should be seen as a supplement to the available online https://www.suse.com/documentation/[documentation].  
This is specifically the case for the https://www.suse.com/documentation/suse-enterprise-storage-5/book_storage_deployment/data/book_storage_deployment.html[SUSE Enterprise Storage 5 Deployment Guide]
and the https://www.suse.com/documentation/sles-12/book_sle_admin/data/book_sle_admin.html[SUSE Linux Enterprise Server Administration Guide].

=== Network Deployment Overview
When working with a backup environment, there are multiple considerations when it comes to designing the network to support horizontally scaling storage.  
These include single stream throughput, aggregate write throughput, verification job requirements, and any replication traffic that may be needed.  
It is important to identify the maximum simultaneous throughput that is required to support the backup traffic and then account for a back-end operation like reconstruction of a failed node.  

If two physically separate networks are used, it is somewhat simple to calculate and allocate an appropriate amount of network bandwidth for back-end reconstruction for a replicated storage environment.
[.text-center, font-size:20em]
[source]
[back-end network throughput] = [front-end network] * 3

[.text-left]

Sizing the network in this way ensures that there is sufficient bandwidth for two operations writing from the primary OSD to the two replica OSDs while a reconstruction operation is taking place.


For an environment where the networks are all sharing the same physical paths, but segmented using VLANs, the calculation would be similar.
[.text-center]
[source]
[aggregate backup performance required] = [backup throughput required] * 4

[.text-left]

== RBD/CephFS Deployment
This section outlines the steps required to deploy an environment similar in architecture to the tested environment.

=== Deploying and Preparing the SUSE Enterprise Storage Environment

Build and deploy a SUSE Enterprise Storage Cluster as described in the SUSE Enterprise Storage Deployment Guide (https://www.suse.com/documentation/suse-enterprise-storage-5/book_storage_deployment/data/book_storage_deployment.html)

* Create an EC profile from the command line on the admin node.
[source]
ceph osd erasure-code-profile set veeam_ec plugin=isa k=4 m=2


==== Creating Pools

* Create one pool for each protocol being supported.
To create an EC pool, type the following:
[source]
ceph osd pool create ecpool 512 512 erasure veeam_ec

* Create the RBD.
[source]
rbd create reppool/veeam -size 5T -data-pool ecpool

=== Creating and Configuring Linux Repository Virtual Machines

Perform the following steps:

. Create virtual machines on ESX.
  * Configure resource reservations.
. Perform a base Linux install.
+
SLES 12 SPx::
.. Select the KVM Host install pattern.
.. Unselect the KVM Host from Software Selection on the summary screen.
.. Enable Multi-queue block IO (`blk-mq`).
+
* Do this on the Ceph OSD nodes and the Linux target VM(s).
* Information on enabling `blk-mq` can be found at https://www.suse.com/documentation/sles-12/book_sle_tuning/data/cha_tuning_io_scsimq.html
+
SLES 15 SPx::
//FIXME validate the following
.. Select the base server pattern.
.. Select to disable mitigations during installation.
+
All::
.. Set network tuning parameters in /etc/sysctl.conf for SUSE Enterprise Storage nodes and Linux target(s) as found in <<appendix>>.
+
. Add repositories and packages for Veeam.
  * Veeam requires perl modules be present for the Linux repository to function. 
  * These are detailed in: https://www.veeam.com/kb2216.

. Modify /etc/ssh/sshd_config to enable the Veeam services to work correctly.
.. See https://www.veeam.com/kb1512.
.. Find the PasswordAuthentication parameter and set the value to "yes".
.. Save and restart the SSHD daemon.
+
[source]
systemctl restart sshd.service

. To add the required perl-SOAP-Lite, the Software Development Kit (SDK) repositories need to be added.
+
For SLES12SP3::
+
[source]
SUSEConnect -p sle-sdk/12.3/x86_64
zypper in perl-SOAP-Lite

For SLES15::
+
[source]
SUSEConnect -p PackageHub/15/x86_64
zypper in perl-SOAP-Lite

+
// FIXME toms: not sure where this exactly belongs to
. The following script can be used to validate that all packages/perl modules are installed. If any are missing, they should be added.
[source]
#!/bin/bash
for i in constant Carp Cwd Data::Dumper Encode Encode::Alias \
   Encode::Config Encode::Encoding Encode::MIME::Name Exporter \
   Exporter::Heavy File::Path File::Spec File::Spec::Unix \
   File::Temp List::Util Scalar::Util SOAP::Lite Socket Storable threads
do
 echo "Checking for perl $i;..."
 perldoc -lm $i >/dev/null
 perlpkgfound=$?
 if [ ! $perlpkgfound -eq 1 ]
 then
   echo Installed
 fi
done

. Add the package `ceph-common` to the Linux target.
[source]
zypper in ceph-common 

. Add the client key and `ceph.conf` file to the directory `/etc/ceph`.
.. From the admin node, run:
[source]
scp /etc/* root@vtarget:/etc/ceph/

. Edit the directory `/etc/ceph/rbdmap` on the Linux repository nodes and add the RBD.
[source]
RbdDevice Parameters
poolname/imagename  id=client,keyring=/etc/ceph/ceph.client.keyring
reppool/veeam       id=admin,keyring=/etc/ceph/ceph.client.admin.keyring

. Enable and start the `systemd rbdmap` service.
[source]
systemctl enable rbdmap
systemctl start rbdmap

. Use the command `mkfs.xfs` to create an XFS file system on the target.
[source]
mkfs.xfs /dev/rbd0

. Add the mount point.
[source]
mkdir /veeam

. Add your entry to the file systems table configuration file `fstab` and include any tuning desired.
[source]
/dev/rbd0 /veeam xfs _netdev 1 1

. Mount the file system.
[source]
mount -a

. Verify it mounted.
[source]
mount

+
The output should be as follows:
+
[source]
/dev/rbd0 on /veeam type xfs (rw,relatime,attr2,inode64,sunit=8192,swidth=8192,noquota,_netdev)
[source]


== Adding Veeam Linux Repository 

1.	Within the Veeam Console, click "Backup Infrastructure" on the left-hand menu bar. Right-click "Backup Repositories" followed by "Add Backup Repository". 

2.	Provide a friendly name to distinguish the multiple repositories.

3.	Choose a repository type and click "Next". 

4.	Click "Add New", enter the details and click "Next".

5.	Click "Add" to add credentials that have Read, Write, and Execute permissions to the mounted storage location and the ability to execute Perl code. Then click "OK" and "Finish".

6.	Ensure the credentials are selected and click "Next".

7.	Click "Browse" and select the path to the mounted RBD with the XFS file system. Then click "Advanced" to select Use per-VM backup files.

8.	Finish the process by selecting a mount server (Veeam Backup Server or proxy) and enabling a vPower NFS service as desired. Then select "Finish".

For the most accurate steps, review the latest Veeam documentation.


=== Disabling Multiple Streams

Multiple streams are designed to enhance performance for higher latency environments. It may be desirable to disable this for the local deployment.
This can be done when defining the job, by setting it for the proxy, or globally. In all cases, it involves selecting the "Network Traffic Rules" and de-selecting "Multiple Streams".


=== Defining a Backup Job

Create a backup job. On the Storage setting tab, select the correct proxy and repository. On the Storage screen, select "Advanced". 
On the Storage tab, set the appropriate rules for your environment. 


== S3 Environment for Scale-Out Backup Repository

Object storage repositories augment your scale-out backup abilities. 
This simplifies offloading existing backup data directly to cloud-based object storage. 
In our case, Veeam can leverage {SUSEProduct} to offload to S3 compatible environments such as Amazon S3, Microsoft Azure Blob Storage, IBM Cloud Object Storage. 

Configuring {SUSEProduct} as an S3 target for a {Vendor} {Vplatform1} usually requires only a few steps.

=== SUSE Enterprise Storage Preparation

The following steps need to be completed to prepare {SUSEProduct}: 

. Install Rados Gateway 
+
* You can add a Rados Gateway role to an existing monitor node or dedicated node for larger environments (recommended). 
* In our case, we used a monitor named "example.ses5". This name will be specific to the name you set for your Rados Gateway. 
* For more information, see the https://www.suse.com/documentation/suse-enterprise-storage-5/book_storage_admin/data/salt_adding_services.html[Rados Gateway Installation Guide].
+
// insert screenshot of policy.cfg

. Navigate to `/srv/pillar/ceph/proposals/policy.cfg` and match the existing host with a new role.
[source]
root@master # role-rgw/cluster/example.ses5.sls

. Run stage 2 to update the pillar.
[source]
root@master # salt-run state.orch ceph.stage.2

. After making these custom changes, you should run stage 3 and 4 to apply the updates. 
For additional details, see the https://www.suse.com/documentation/suse-enterprise-storage-5/pdfdoc/book_storage_admin/book_storage_admin.pdf[{Suseproduct} Guide].
[source]
root@master # salt-run state.orch ceph.stage.3
root@master # salt-run state.orch ceph.stage.4

==== Installing and Configuring the RGW Daemons

* IMPORTANT: Ensure that HTTPS/SSL is enabled on the target pool to allow {v} {vplatform1} to connect. 
This allows for secure communication supported by {v}. For more details, see the following section about https://www.suse.com/documentation/suse-enterprise-storage-5/book_storage_admin/data/ceph_rgw_https.html[enabling HTTPS/SSL for Object Gateways].

* Modify the `rgw.conf` to allow port 443 (or 80 + 443). Navigate to the `srv/salt/ceph/configuration/files/ceph.conf.d` directory to edit the `rgw.conf` file. 
[source]
root@master # cd /srv/salt/ceph/configuration/files/ceph.conf.d
root@master # vi rgw.conf

* Edit the contents of this file with the appropriate information listed below. 
The following example represents what was used during our testing process (parameter values below will vary):
+
[source]
....
[client{{ client }}]
rgw frontends = "civetweb port=80+443s ssl_certificate=/etc/ceph/rgw.pem"
rgw dns name = {{ fqdn }}
rgw enable usage log = false
rgw thread pool size = 512
rgw ops log rados = false
rgw max chunk size = 4194304
rgw num rados handles = 4
rgw usage max user shards = 4
rgw cache lru size = 100000
....
+

* Validate the Rados Gateway is in "active (running)" state by running `systemctl`.
In our example, the Rados Gateway is called "example.ses5". Exchange that with the name of your Rados Gateway. 
[source]
systemctl status ceph-radosgw@example.ses5

// * You can verify this has worked properly by accessing the {SUSEproduct} (Open Attic) dashboard. You should see the Rados Gateway listed under the "Nodes" tab.  

==== Configuring the Storage Pools

* Storage pools need to be created to host the {vplatform1} data. Create a Ceph Object Pool for the Rados Gateway. 
You can do this via the openATTIC dashboard or command line. The dashboard can help with your Placement Groups (PG) calculation.
In our example, we use 2048 (depends on environment). Examples of both are as follows:

image::{imgpath}SES_pool.png[SES Pool, scaledwidth=100%] 

* Create an erasure code profile from the command line on the admin node.
[source]
ceph osd erasure-code-profile set veeam_ec plugin=isa k=4 m=2

* Create the required pools.
[source]
ceph osd pool create default.rgw.veeam.data 2048 2048 erasure veeam_ec
ceph osd pool create default.rgw.veeam.index 2048 2048 erasure veeam_ec
ceph osd pool create default.rgw.veeam.non-ec 2048 2048 replicated

===== Creating an S3 User

* When accessing the Object Gateway through the S3 interface, you need to create an S3 user by running the below command and adjusting the options in <> brackets. 
This can also be done using the openATTIC dashboard by going to 'Object Gateway > User' tab. 
[source] 
root@master # radosgw-admin user create --uid=<username> \
--display-name=<display-name> --email=<email>

* Configure a placement policy and set the user placement.
[source]
+
....
radosgw-admin zonegroup placement add --rgw-zonegroup default --placement-id veeam

radosgw-admin zone placement add --rgw-zone default --placement-id veeam --data-pool default.rgw.veeam.data --index-pool default.rgw.veeam.index --data-extra-pool default.rgw.veeam.non-ec

radosgw-admin metadata get user:veeam > user.json
....
+

* Edit the user.json and change `default_placement` to the placement ID created.
[source]
"default_placement":"veeam"

* Next, save the changes and commit them.
[source] 
radosgw-admin metadata put user:<user-id> <user.json

=== {v} {vplatform1} Configuration
After successfully completing the steps above for {SUSEproduct} preparation, you can proceed to properly configuring {v} {vplatform1}. 
{v} documents this process very well. Follow the step-by-step instructions at https://helpcenter.veeam.com/docs/backup/vsphere/new_object_storage.html?ver=95u4[the Veeam help center].

Tips and reminders::
* {v} will prompt you for a service point. Use the IP of the gateway node.
* Provide the access and secret keys, which can be found in 'Open Attic > Object > user' tab.
* The {v} software wizard may ask for a self-signed certificate. You will receive an error if the self-signed certificate is not properly imported to {v} Server.
// insert screenshot of failure of connection
* A bucket can be created with the openATTIC dashboard or any S3.
* Verify the connection to your bucket via S3 browser or any S3-compatible tools. 
//mention successful login


== {vplatform1} Tuning Parameters 

The following steps can be implemented to improve performance specific to your workload. 
Refer to the https://documentation.suse.com/ses/6/html/ses-all/book-storage-tuning.html[SUSE Enterprise Storage Tuning guide] for additional instructions. 
 
* Modify the `ceph.conf` file found in the directory `/srv/salt/ceph/configuration/files/ceph.conf.d/rgw.conf`.  
+
[source]
....
[client.{{ client }}]
rgw frontends = "civetweb port=80+443s ssl_certificate=/etc/ceph/rgw.pem error_log_file=/var/log/ceph/dl360-3.rgw.error.log"
#rgw frontends = "beast port=80 ssl_port=443 ssl_certificate=/etc/ceph/rgw.pem"
rgw dns name = {{ fqdn }}
rgw enable usage log = false
rgw thread pool size = 512
rgw max chunk size = 4194304
#abhi changes
rgw_obj_stripe_size = 4194304 # (default 4M for luminous)
rgw_list_bucket_min_readahead = 4000 #(default 1000)
rgw_max_listing_results = 4000
rgw_cache_expiry_interval = 1800 #(default 900s)
rgw_enable_usage_log = false
rgw_enable_ops_log = false
rgw dynamic resharding = false
rgw override bucket index max shards = 50 # alternatively we reshard the bucket manually after creation
rgw bucket index max aio = 16 # default 8
rgw cache lru size = 50000
# GC settings
rgw_gc_obj_min_wait = 21600 #(default 2_hr), decreasing will more actively purge objects
rgw gc processor period = 7200 #(default 1hr, decreasing more actively purges deletion)
rgw objexp gc interval = 3600 # default 10_min, we dont run swift objexp. so no need to run this 
objecter inflight op bytes = 1073741824 # default 100_M
objecter inflight ops = 24576 
....

* This configuration needs to be pushed out to all Rados Gateways that may be running in the {SUSEproduct} environment. 
+
[source]
salt 'salt_master_hostname' state.apply ceph.configuration.create
salt '*' state.apply ceph.configuration

== Special Note

When using SSDs directly on the ESX server as a restoration target, you may want to disable VAAI for VMware to perform optimally.  
For more information, see https://kb.vmware.com/s/article/1033665.

== Conclusion
{vendor} {vplatform} represents a strong option for data center backup when combined with {SUSEProduct}. 
The benefits to customers include increased efficiency and performance, while achieving industry leading cost efficiency.

== Resources
* Veeam KB – SUSE KB Articles
+
https://www.veeam.com/kb_search_results.html?product=Backup_Replication&kb-search-type=&search=suse
* Veeam Documentation
+
https://www.veeam.com/documentation-guides-datasheets.html
* SUSE Enterprise Storage Technical Overview
+
https://www.suse.com/docrep/documents/1mdg7eq2kz/suse_enterprise_storage_technical_overview_wp.pdf 
* SUSE Enterprise Storage 5.5 - Deployment Guide
+
https://documentation.suse.com/ses/5.5/pdf/book-storage-deployment_color_en.pdf 
* SUSE Enterprise Storage 5.5 - Administration Guide
+
https://documentation.suse.com/ses/5.5/pdf/book-storage-admin_color_en.pdf
* SUSE Enterprise Storage 6 - Tuning Guide 
+
https://documentation.suse.com/ses/6/pdf/book-storage-tuning_color_en.pdf 

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
== Legal Notice
include::common_sbp_legal_notice.adoc[]

////
++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
// :leveloffset: 1
include::common_gfdl1.2_i.adoc[]
////

++++
<?pdfpagebreak?>
++++

[appendix]
[[appendix]]
== OS Networking Configuration
[source] 
net.ipv4.ip_forward = 0 
net.ipv6.conf.all.forwarding = 0 
net.core.netdev_max_backlog = 10000 
net.core.netdev_budget = 300 
net.core.somaxconn = 128 
net.core.busy_poll = 50 
net.core.busy_read = 50 
net.core.rmem_max = 125829120
net.core.wmem_max = 125829120 
net.core.rmem_default = 125829120 
net.core.wmem_default = 125829120 
net.ipv4.tcp_fastopen = 1 
net.ipv4.tcp_low_latency = 1 
net.ipv4.tcp_sack = 1 
net.ipv4.tcp_rmem = 10240 87380 125829120 
net.ipv4.tcp_wmem = 10240 87380 125829120
net.ipv4.ip_local_port_range = 1024 64999 
net.ipv4.tcp_max_syn_backlog = 1024 
net.ipv4.tcp_tw_reuse = 0 
net.ipv4.tcp_tw_recycle = 0 
net.ipv4.tcp_timestamps = 0 
net.ipv4.tcp_syn_retries = 5 


++++
<?pdfpagebreak?>
++++

[appendix]
// Standard SUSE Best Practices includes
// :leveloffset: 1
include::common_gfdl1.2_i.adoc[]


////
++++
<?pdfpagebreak?>
++++

[appendix]
== Resources
•	Veeam KB – SUSE KB Articles
https://www.veeam.com/kb_search_results.html?product=Backup_Replication&kb-search-type=&search=suse
•	Veeam Documentation 
https://www.veeam.com/documentation-guides-datasheets.html
•	SUSE Enterprise Storage Technical Overview
https://www.suse.com/docrep/documents/1mdg7eq2kz/suse_enterprise_storage_technical_overview_wp.pdf 
•	SUSE Enterprise Storage 5.5 – Deployment Guide
https://documentation.suse.com/ses/5.5/pdf/book-storage-deployment_color_en.pdf 
•	SUSE Enterprise Storage 5.5 – Administration Guide
https://documentation.suse.com/ses/5.5/pdf/book-storage-admin_color_en.pdf 
////