
:docinfo:

= SAP Data Hub 2 on SUSE CaaS Platform 4: Installation Guide


== Introduction

Today, business and industries create more and more data. As the amount of data
grows, so does the need to manage and get the best out of the data. 
SAP Data Hub 2 is a tool to make it easier to deal with such amounts of data, 
and with SUSE CaaS Platform 4, SUSE delivers the foundation on top of which to
run SAP Data Hub 2.

== Requirements

To install SAP Data Hub 2 on SUSE CaaS Platform 4, we need to meet certain requirements. Consult the relevant documentation:

* SUSE

** link:https://documentation.suse.com/suse-caasp/4/[SUSE CaaS Platform 4]
** link:https://www.suse.com/documentation/suse-enterprise-storage-5[SUSE Enterprise Storage]

* SAP

** link:https://help.sap.com/viewer/product/SAP_DATA_HUB/2.7.latest/en-US[SAP Data Hub]

** link:https://launchpad.support.sap.com/#/notes/2686169[SAP Note 2686169]

** link:https://launchpad.support.sap.com/#/notes/2865345[SAP Note 2865345]

=== Hardware and software requirements

==== SUSE CaaS Platform 4 cluster

For the hardware requirements, see also SAP's installation guide and sizing recommendations:

* link:https://help.sap.com/viewer/product/SAP_DATA_HUB/[SAP Data Hub Install Guide]
* link:https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.7.latest/en-US/79724de552db4b2b81c4a893f2c7ed18.html[SAP Data Hub 2.7]


The minimum hardware requirements for installing SAP Data Hub 2 for on-premises production use are:

* 7 Kubernetes cluster nodes (three master nodes and four worker nodes)

** All machines should have at least four CPU cores

** The master nodes should have > 32 GiB RAM

** The worker nodes should have > 64 GiB RAM

* Loadbalancer

* Management host

// TODO check network requirements The faster the better 1 GiB/s or more
// TODO disk requirements, Master nodes and worker nodes need at least 100 GiB so called epheremal storage so 150 GiB for / /var/lib and /var/log are needed.

Environments for non-production use may run with one master, three workers, and a management node.

NOTE: Having only one master node is not recommended for production environments.
For production use, three master nodes are recommended.

* Access to a SUSE Enterprise Storage 5 or SUSE Enterprise Storage 6 system 
(see SAP Note link:https://launchpad.support.sap.com/#/notes/2686169[Pre-requisites for installing SAP Data Hub])

==== Management host

//It is recommended to do the installation of SAP Data Hub Foundation from an external "Jump" host and not from within the SUSE CaaS Platform Cluster.
//The hardware and operating system specifications for the jump host can be e.g.

//- SUSE Linux Enterprise Server 12 SP5 or SUSE Linux Enterprise Server 15 SP1 (or even openSUSE Leap 15.X)
* Two cores
* 16 GiB RAM
* Diskspace: 
** 80 GiB for `/`, including the space for the SAP Data Hub 2 software 
** At least 100 GiB for `/var/lib/docker` (needed for the SAP Data Hub 2 installation)
* Network connectivity to the SUSE CaaS Platform cluster ( 1 GBit/s)

// FIXME what about replacing docker with podman?

=== Software requirements

The following software is needed:

* SUSE CaaS Platform 4.0.3
+
NOTE: For SAP Data Hub, it is required to stay on SUSE CaaS Platform 4.0.0 - 4.0.3.,
because of the Kubernetes version delivered with our CaaSP product. 
SAP Data Hub 2 needs a Kubernetes version not higher than 1.15.

* SAP Data Hub 2.7.1 or higher

* SAP Hostagent

* Access to SAP Maintenance Planner

* Connection to a secure private Docker registry

* Optional Hadoop/Spark see Vora's Spark extensions


== Installing SUSE CaaS Platform 4

=== Getting the installation media

All installation media can be found at:
link:++https://download.suse.com++[SUSE CaaS Platform ISO images]

=== Getting a subscription for SUSE CaaS Platform 4

To be able to get all maintenance updates for your SUSE products,
you need a valid subscription for the particular product:
* https://www.suse.com/support/?id=SUSE_CaaS_Platform[Order subscription for SUSE CaaS Platform]


=== Reading the deployment guide for SUSE CaaS Platform 4

SUSE CaaS Platform is designed to make the installation of Kubernetes easy.
To get a deeper understanding, you should read the Deployment Guide for SUSE CaaS Platform 4, 
available at https://www.suse.com/documentation/.
For further reference, there are also a Quickstart Guide and an Administrator's Guide.

=== Installing SUSE CaaS Platform 4

In this guide, we describe the installation of SUSE CaaS Platform 4 on top of SUSE Linux Enterprise 15 SP1 on your premises. 
We will use the SUSE Linux Enterprise 15 SP1 installer ISO image for starting the installation.

Make sure that the host names you will use for the installation are resolvable via DNS.
It is preferred to have a static network setup.

You will need to the FQDN  or IP address of your time server. A reliable system time is required.
Connect the media to your hardware and boot from the media.
// Screenshot?

Select *Installation* from the GRUB menu.

==== Preparation

Install SUSE Linux Enterprise 15 SP1 or higher (as released for CaaS Platform 4.x) on all nodes.
The following Modules/Products are required on the respective hosts:

* Management host

** SUSE Linux Enterprise 15 SP1
** SUSE Linux Enterprise 15 SP1 Containers Modules
** SUSE Linux Enterprise 15 SP1 SP1 Public Cloud
** SUSE CaaSP 4


* Kubernetes master nodes

** SUSE Linux Enterprise 15 SP1
** SUSE Linux Enterprise 15 SP1 Public Cloud
** SUSE CaaSP 4

* Kubernetes worker nodes

** SUSE Linux Enterprise 15 SP1
** SUSE Linux Enterprise 15 SP1 Public Cloud
** SUSE CaaSP 4

* Load balancer host

** SUSE Linux Enterprise Server for SAP applications 15 SP1 
+
or
+
** SUSE Linux Enterprise 15 SP1 plus High Availability Extension


==== Installing the management node

* Install SUSE Linux Enterprise 15 SP1

* Add the following software repositories

** Install software pattern "CaaSP management"

** Install `python2` and `PyYaml-python2` (needed for installing DH2.7)

** Install Docker (needed for installing DH2.7)


==== Installing the load balancer

This is only necessary if no other load-balancing device is available.
In our setup, we describe the setup of a `haproxy` instance.
`haproxy` is available in the SUSE Linux Enterprise High Availability Extension or in SUSE Linux Enterprise Server for SAP Applications.

* Install SUSE Linux Enterprise 15 SP1

* Add the "SLE 15 SP1 HA" repository 
+
(in case of SUSE Linux Enterprise Server for SAP Applications, this is automatically added)

* Create the `haproxy` configuration

* Enable and start `haproxy`


==== Installing the master nodes

* Install SUSE Linux Enterprise 15 SP1

** Set up static IP addresses

** Use the "Expert Partitioner" to disable any swap partition

** Disable `firewalld`

** Enable `ssh`

* Add the necessary software repositories

** SUSEConnect -p caasp/4.0/x86_64

** SUSEConnect -p ...

* Create directory `/var/lib/docker/containers`


==== Installing the worker nodes

* Install SUSE Linux Enterprise 15 SP1

** Set up static IP addresses

** Use the "Expert Partitioner" to disable any swap partition

** Disable `firewalld`

** Enable `ssh`

* Add the necessary software repositories

** SUSEConnect -p caasp/4.0/x86_64

** SUSEConnect -p ...


==== Creating the Kubernetes Cluster on SUSE CaaS Platform 4

* Log in to the management host

** Ensure that `ssh-agent` is working as expected 
+
(for example, `ssh -A root@managementhost.example.com` )
// FIXME: ssh-agent

** Initialize the Kubernetes cluster
+
----
skuba cluster init --control-plane <LB_IP/FQDN> my-cluster
----

** create the first master node to the Kubernetes cluster
+
----
cd my-cluster
skuba node bootstrap --user sles --sudo --target <IP/FQDN> <NODE_NAME>
----

** Add additional master nodes
+
----
skuba node join --role master --user sles --sudo --target <IP/FQDN> <NODE_NAME>
----

** Add worker nodes to the Kubernetes cluster
+
----
skuba node join --role worker --user sles --sudo --target <IP/FQDN> <NODE_NAME>
----

** Verify your Kubernetes cluster
+
----
skuba cluster status
----

//FIXME command output????

* Modifactions needed to install SAP Data Haub 2 on CaaSP 4 cluster

** Change the `pid` parameter in `/etc/crio/crio.conf` to 8192 (was 1024)

** Edit `/etc/containers/registry.conf` to reflect your private registry

// e.g.:
// add example configuration



== Installing SAP Data Hub 2

The following sections describe the preparation and installation of SAP Data Hub 2 on a SUSE CaaS Platform 4 cluster.


=== Preparing the SAP Data Hub 2 installation

The steps that are needed to successfully install SAP Data Hub on SUSE CaaS Platform are as follows:


==== Downloading the SAP Data Hub 2 software archive:

To download and install SAP Data Hub 2:

1. Go to the SAP Software Download Center, log in with your SAP account and search for "SAP DATA HUB 2". 
// or access this link [LINK IS MISSING]

2. Download the SAP Data Hub Foundation file, for example:
* `DHFOUNDATION07_3-80004015.ZIP` (SAP DATA HUB - FOUNDATION 2.7)

3. Unzip the software archive on to your management host.
+
There are three ways to install the SAP Data Hub 2:
+
* Use the SL Plugin. There are two variants of it:

** SL Plugin with Maintenance Planner (`mpsl`)
** SL Plugin only (`mpfree`)

* Use the command line `install.sh` script.

+
This document will focus on the Maintenance Planner and SL Plugin installation method.


[id="prerequisites_caasp_cluster"]
==== Prerequisites on the SUSE CaaS Platform 4 cluster

The following steps are done on the jump host if not stated otherwise:

1. Create a new namespace in the Kubernetes cluster, into which to install SAP Data Hub 2:
+
----
$ kubectl create namespace datahub
----

2. Create the storage class to provide volumes for SAP Data Hub 2 on SUSE Enterprise Storage:
+
Make sure you have the connection data for your SUSE Enterprise Storage at hand:

* IP addresses and port number (defaults to 6789) of the monitor nodes of your SUSE Storage
* Create a data pool (datahub in this example) on your SUSE Enterprise Storage for the use with SAP Data Hub 2

3. Edit the example below to fit your environment.
+
----
$ cat > storageClass.yaml <<EOF
apiVersion: storage.kubernetes.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
  name: datahub
  namespace: default
parameters:
  adminId: admin
  adminSecretName: ceph-admin-secret
  adminSecretNamespace:  default
  imageFeatures: layering
  imageFormat: "2"
  monitors: <IP ADDRESS OF MONITOR 1>:6789, <IP ADDRESS OF MONITOR 2>:6789, <IP ADDRESS OF MONITOR 3 >:6789
  pool: datahub
  userId: admin
  userSecretName: ceph-user-secret
provisioner: kubernetes.io/rbd
reclaimPolicy: Delete
volumeBindingMode: Immediate
EOF

$ kubectl create -f storageClass.yaml
----

4. Create the secrets needed to access the storage:

+
Obtain the keys from your SUSE Enterprise Storage cluster. 
They are located in `ceph.admin.keyring` and `ceph.user.keyring`.

+
You must encode the keys using `base64`:
+
----
echo <YOUR KEY HERE> | base64
----
+
----
$ cat > ceph-admin-secret.yaml <<EOF
apiVersion: v1
kind: Secret
metadata:
    name: ceph-admin-secret
type: "kubernetes.io/rbd"
data:
   key: <YOUR BASE64 ENCODED KEY HERE>
EOF
image::002-SCT-CaaSP.png
$ cat > ceph-user-secret.yaml <<EOF
apiVersion: v1
kind: Secret
metadata:
    name: ceph-user-secret
type: "kubernetes.io/rbd"
data:
   key: <YOUR BASE64 ENCODED KEY HERE>
EOF

$ kubectl create -f ceph-admin-secret.yaml
$ kubectl create -f ceph-user-secret.yaml
----


=== Installing SAP Data Hub 2 using the Maintenance Planner with SL Plugin

SAP recommends the "SAP Maintenance Planner with SL Plugin" (`mpsl`) installation method as the best approach to install SAP Data Hub 2.

SAP recommends this Web-based installation method because it offers the option to send analytic data and feedback to SAP. 
All the necessary prerequisites have been satisfied by applying all the steps described above.

NOTE: You need to install the latest SAP Host Agent on the management host. 
Use the `rpm` package downloadable from the SAP Software Download center.

* Install SAP Host Agent on management host

** Download the `rpm` package from the SAP Software Download Center
** Install SAP Host Agent
+
----
zypper in path to rpm
----

** Start SAP Host Agent
+
----
/etc/init.d/sapinit start
----
//TODO Check systemd integration

** Configure SAP Host Agent to be used with SAP Maintenance Planner

*** Create certificates according to the documentation provided by SAP

*** Enable CORS webservice in SAP Host Agent

** Navigate to `apps.sap.com/mp` with your browser

*** Create new plan

*** Select "container based system"

*** Connect to your SAP Host Agent running on the management host

*** Follow the steps given by the wizard
//TODO add screenshots

*** Fill in values for the installer as needed. 
//See screenshots: TODO add screenshots

*** Wait for successful deployment


=== Installing SAP Data Hub 2 using the SL Plugin (`mpfree` method)

This is an alternative command line-based installation method. 
Refer to the SAP Data Hub documentation (2.7) for more information and the exact procedure.
//TODO insert URLs


=== Installing SAP Data Hub 2 using the command line (manual installation)

Unpack the SAP Data Hub software archive on the jump host — for example:

----
$ unzip DHFOUNDATION07_3-80004015.ZIP
----


Run the installation command as described in the SAP Data Hub 2 install guide at 
// TODO URL help.sap.com/

* https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.7.latest/en-US [Installation of SAP Data Hub 2]
// and link:
----
$ cd SAP-Datahub-2.7.155-Foundation
$ export DOCKER_REGISTRY=<URI of your registry>
$ export NAMESPACE=<YOUR NAME SPACE HERE>
// $ ./install.sh --enable-kaniko=yes  --docker-log-path='/var/log/containers'  -e vora-vsystem.vRep.nfsv4MinorVersion=1 -e vora-diagnostic.fluentd.logDriverFormat=regexp
$ ./install.sh --enable-kaniko=yes  --docker-log-path='/var/log/containers' -e vora-diagnostic.fluentd.logDriverFormat=regexp
----


This interactive script configures the installation of SAP Data Hub 2.
You should have the following information at hand:
//TODO namespace, login on registry, S-User

* Name and credentials of your SAP S-User 
* Login credentials to your secure registry

//TODO explain cmdline options


=== Post-installation actions

After successful installation, you can connect to the SAP Data Hub web UI.
You need to identify the service IP and port of the SAP Data Hub UI.

// Refer to SAP Documentation, create ingress, or edit service vsystem to be of type NodePort.
// Or create software loadbalancer, e.g. Metalllb, see nginx ingress docs.

----
kubectl -n $NAMESPACE get services
kubectl -n $NAMESPACE describe service vsystem
----

Point your browser to the IP and port you obtained from the steps above.
// TODO Example.

Use the login data you defined during the installation.

==== Post-installation work

Follow the documentation provided by SAP at 

* https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.7.latest/en-US/4c472c40595b450283a6ce039f71cfc6.html

to the post-installation work.

* Create the `vflow-secret` for the modeler app, as pointed out in the SAP documentation.

* Import any necessary certificate authority, for example the CA that signed the certificate of the secure registry.


== Upgrading SAP Data Hub 2

This section describes the update of an existing SAP Data Hub 2 installation to a higher version (for example 2.3 to 2.4).

Execute the SAP Data Hub upgrade as described in the official instructions. 
One can choose between different upgrade methods:

* Maintenance Planner:
+
link:https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.6.latest/en-US/31079833a65f4f379d5a76957ff8073c.html[Upgrade SAP Data Hub 2 using the Maintenance Planner / SL Plugin and SAP Host Agent]
* SL Plugin method:
+
link:https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.6.latest/en-US/ff37f3ccf6504bb38d7db53936fe8017.html[Upgrade SAP Data Hub 2 using the SL Plugin and SAP Host Agent]
* Command line method:
+
link:https://help.sap.com/viewer/e66c399612e84a83a8abe97c0eeb443a/2.6.latest/en-US/aec679bc0209443ba4ae03a9018d4bd8.html[Upgrade SAP Data Hub 2 using the install.sh script]


== Appendix

=== Secure Private Registry

To satisfy the requirements of SAP Data Hub requirements, you also need a Docker Registry.
The easiest way to build and manage one is with the link:http://port.us.org/[Portus project].

** link:http://port.us.org/[Portus]

First, you need to create a dedicated server for your Docker Registry and Portus stack.

----
# sudo virt-install --name portus-dr --ram 8192 --disk path=/var/lib/libvirt/VMS/portus-dr.qcow2,size=40 --vcpus 4 --os-type linux --os-variant generic --network bridge=common --graphics none --console pty,target_type=serial --location '/var/lib/libvirt/isos/SLE-12-SP4-Server-DVD-x86_64-GM-DVD1.iso' --extra-args 'console=ttyS0,115200n8 serial ifcfg=eth0=10.10.10.11/24,10.10.10.1,10.10.10.11,suse-sap.net hostname=portus-dr domain=suse-sap.net Textmode=1'
----

In our example, this server will be connected to another local bridge 
which provides common services (DNS, SMT, Docker-registry) for the Data Hub stack.

Our Portus deployment will be based on a container, and orchestrated locally with `docker-compose`.

NOTE: Portus deployments using `docker-compose` require an up-to-date release of
the `docker-compose` tool.

----
sudo curl -L "https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
----

Now you can simply clone the Portus repository. 
Adapt the `.env` and the `nginx` configuration to your naming convention.

----
# git clone https://github.com/SUSE/Portus.git /tmp/Portus-DR
# mv /tmp/Portus-DR/examples/compose ./portus
# cd portus
----

Now you can edit both `.env` and `nginx/nginx.conf`.
This is how our configuration looks:

----
# cat .env
MACHINE_FQDN=portus-dr.suse-sap.net
SECRET_KEY_BASE=b494a25faa8d22e430e843e220e424e10ac84d2ce0e64231f5b636d21251eb6d267adb042ad5884cbff0f3891bcf911bdf8abb3ce719849ccda9a4889249e5c2
PORTUS_PASSWORD=XXXXXXXX
DATABASE_PASSWORD=YYYYYYYY
----

In the nginx/nginx.conf file, you should adapt the following section :

----
server {
    listen 443 ssl http2;
    server_name portus-dr.suse-sap.net;
    root /srv/Portus/public;
----

Pull the latest `docker-compose.yml`:

----
# rm docker-compose.*
# wget https://gist.githubusercontent.com/Patazerty/d05652294d5874eddf192c9b633751ee/raw/6bf4ac6ba14192a1fe5c337494ab213200dd076e/docker-compose.yml
----

To avoid dealing with Docker's insecure registry configuration, add SSL to your setup.

----
echo "subjectAltName = DNS:portus-dr.suse-sap.net" > extfile.cnf
openssl genrsa -out secrets/rootca.key 2048
openssl req -x509 -new -nodes -key secrets/rootca.key -subj "/C=FR/ST=FR/O=SUSE"  -sha256 -days 1024 -out secrets/rootca.crt
openssl genrsa -out secrets/portus.key 2048
openssl req -new -key secrets/portus.key -out secrets/portus.csr -subj "/C=FR/ST=FR/O=SUSE/CN
openssl req -new -key secrets/portus.key -out secrets/portus.csr -subj "/C=FR/ST=FR/O=SUSE/CN=portus-dr.suse-sap.net"
openssl x509 -req -in secrets/portus.csr -CA secrets/rootca.crt -extfile extfile.cnf -CAkey secrets/rootca.key -CAcreateserial  -out secrets/portus.crt -days 500 -sha256
----

Next, all you need to do is to make the servers aware of this certificate:

----
cp -p secrets/rootca.crt /etc/pki/trust/anchors/.net-ca.crt
scp secrets/rootca.crt root@jumpbox.suse-sap.net:/etc/pki/trust/anchors/portus-dr.suse-sap.net-ca.crt
----

Then, on all servers that need to interact with the `docker-registry`, do the following:

----
sudo update-ca-certificates
sudo systemctl restart docker
----

Start your Portus setup:

----
docker-compose up -d
----

Finally, you can log in to Portus and set the registry:

image::portus-registry.png[portus-registry.png,scaledwidth=95%]


=== Installing and configuring a secure private registry using SUSE Linux Enterprise Server and the SLE-Container-Module

The needed componentes are Docker, a registry, and Portus.
Create SSL certificates as needed. Distribute the CA certificates to all your Kubernetes nodes.

Run:

----
# update-ca-certificates
# systemctl restart docker
----


Create the namespaces on your registry that are needed for SAP Data Hub 2:

* com.sap.hana.container

* com.sap.datahub.linuxx86_64

* com.sap.datahub.linuxx86_64.gcc6

* consul

* elasticsearch

* fabric8

* google_containers

* grafana

* kibana

* prom

* vora

* kaniko-project

* com.sap.bds.docker
//TODO Liste der Namespaces


=== SUSE Enterprise Storage

//SAP Data Hub 2 installation on premises require SUSE Enterprise Storage 5 or higher.
//Follow the installation documentation to create SUSE Enterprise Storage.
//Create a storage pool on your SES for the use with SAP Data Hub 2.
//You can also leverage the S3 API to create buckets on SES that can be used from applications from within SAP Data Hub 2.


An on-premises installation of SAP Data Hub 2 requires SUSE Enterprise Storage 5 or higher.
If you plan to use SUSE Enterprise Storage not only for your Kubernetes dynamic storage class, 
but also for your Kubernetes Control plan (virtualized or not), you should reserve enough resources to address the 
link:https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md.[`etcd` hardware requirements]

The following steps will deploy a minimalist, virtualized, test-oriented
instance of SUSE Enterprise Storage 5.5. In our example, we will build a four-nodes (1 Admin + 3 OSD) Ceph cluster.

Before you start:

* Obtain registration codes for SUSE Linux Enterprise Server 15 SP1 and SUSE Enterprise Storage from https://scc.suse.com,
or have a SMT/RMT properly set up and already mirroring theses products:

++++
<?pdfpagebreak?>
++++

** link:https://scc.suse.com[SCC]
+
image::scc-sle.png[scc-sle,scaledwidth=99%]

++++
<?pdfpagebreak?>
++++

** link:https://documentation.suse.com/sles/12-SP4/html/SLES-all/book-smt.html[SMT]
+
image::scc-ses.png[scc-ses,scaledwidth=99%]


* You should already have set up a DNS zone. In our example, where all Data Hub
components are in the same DNS zone and the same subnet, it should look like:
+
image::dns.png[dns,scaledwidth=95%]

* To be as efficient as possible when using interactive shell-scripted infrastructure deployment, 
we advise to use an advanced terminal client or multiplexer which permits to address multiple shells at once.
+
image::multi-s-virtinstall.png[multi-s-virtinstall,scaledwidth=95%]


Now you can create the virtual machines.

* Create

** first the Admin Node:
+
----
# sudo virt-install --name ses55-admin --ram 16384 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-admin.qcow2,size=40 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-admin-osd0.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-admin-osd1.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-admin-osd2.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-admin-osd3.qcow2,size=20 --vcpus 4 --os-type linux --os-variant generic --network bridge=caasp3 --graphics none --console pty,target_type=serial --location '/var/lib/libvirt/ISOS/SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso' --extra-args 'console=ttyS0,115200n8 serial autoyast-ses5=http://10.10.10.101/autoyast-ses5 ifcfg=eth0=10.18.10.200/24,10.18.10.1,10.10.10.11,suse-sap.net domain=suse-sap.net Textmode=1'
----

** then the OSD Nodes:
+
----
# sudo virt-install --name ses55-osd0 --ram 16384 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd0.qcow2,size=40 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd0-osd0.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd0-osd1.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd0-osd2.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd0-osd3.qcow2,size=20 --vcpus 4 --os-type linux --os-variant generic --network bridge=caasp3 --graphics none --console pty,target_type=serial --location '/var/lib/libvirt/ISOS/SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso' --extra-args 'console=ttyS0,115200n8 serial autoyast-ses5=http://10.10.10.101/autoyast-ses5 ifcfg=eth0=10.18.10.230/24,10.18.10.1,10.10.10.11,suse-sap.net domain=suse-sap.net Textmode=1'

# sudo virt-install --name ses55-osd1 --ram 16384 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd1.qcow2,size=40 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd1-osd0.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd1-osd1.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd1-osd2.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd1-osd3.qcow2,size=20 --vcpus 4 --os-type linux --os-variant generic --network bridge=caasp3 --graphics none --console pty,target_type=serial --location '/var/lib/libvirt/ISOS/SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso' --extra-args 'console=ttyS0,115200n8 serial autoyast-ses5=http://10.10.10.101/autoyast-ses5 ifcfg=eth0=10.18.10.231/24,10.18.10.1,10.10.10.11,suse-sap.net domain=suse-sap.net Textmode=1'

# sudo virt-install --name ses55-osd2 --ram 16384 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd2.qcow2,size=40 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd2-osd0.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd2-osd1.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd2-osd2.qcow2,size=20 --disk bus=virtio,path=/var/lib/libvirt/VMS/ses55-osd2-osd3.qcow2,size=20 --vcpus 4 --os-type linux --os-variant generic --network bridge=caasp3 --graphics none --console pty,target_type=serial --location '/var/lib/libvirt/ISOS/SLE-12-SP3-Server-DVD-x86_64-GM-DVD1.iso' --extra-args 'console=ttyS0,115200n8 serial autoyast-ses5=http://10.10.10.101/autoyast-ses5 ifcfg=eth0=10.18.10.232/24,10.18.10.1,10.10.10.11,suse-sap.net domain=suse-sap.net Textmode=1'
----

+
image::multi-s-smt.png[multi-s-smt.png,scaledwidth=95%]


* Select the SUSE Enterprise Storage 5 extension:
+
image::multi-s-addon.png[multi-s-addon.png,scaledwidth=95%]

* On the hypervisor you should also be able to route or bridge your upcoming SUSE Enterprise Storage 5.5 network segment. 
Either a traditional bridge using `brctl`, or a `virtual-bridge` will work. 
In our example, for simplicity, we’re using the same bridge and network address than our CaaSP cluster:
`--network bridge=caasp3`

* In our example below, each node is powered by 16GB of RAM, 4 VCPU, 40 GB for the root disk, 4 x 20GB OSDB disk.
+
image::multi-s-default.png[multi-s-default.png,scaledwidth=95%]

* NTP must be configured on each nodes:
+
image::multi-s-ntp.png[multi-s-ntp.png,scaledwidth=95%]

* Deselect "AppArmor" and the unnecessary "X" and "Gnome" Patterns, but select the SUSE Enterprise Storage pattern:
+
image::multi-s-patterns.png[multi-s-patterns.png,scaledwidth=95%]

* De-activate the Firewall on the nodes.

* Start the Installation on all nodes:
+
image::multi-s-install.png[multi-s-install.png,scaledwidth=95%]

* When all nodes have rebooted, log in and finish the network/host name and NTP configurations 
so that `hostname -f` returns the FQDN of the nodes and `ntpq -p` returns a stratum less than 16:
+
image::multi-s-hostname-ntp.png[multi-s-hostname-ntp.png,scaledwidth=95%]

* Using `ssh-keygen` then `ssh-copy-id`, spread your SUSE Enterprise Storage Admin Node `ssh` public key to all other nodes.

* Verify that the drive we will allocate for SUSE Enterprise Storage OSDs are clean by wiping them.
Refer to 4.3.12 of the Deployment guide: link:https://documentation.suse.com/ses/5.5/html/ses-all/ceph-install-saltstack.html#ceph-install-stack[Wipe disk]

* On all nodes, including the Admin Node, install `salt-minion`.

* On the Admin Node only, also install `salt-master` (in our example `ses55-admin.suse-sap.net`) and `deepsea`.

* Then, restart `salt-minion` on all nodes and restart `salt-master` on the Admin Node:
+
image::multi-s-salt-install-restart.png[multi-s-salt-install-restart.png,scaledwidth=95%]

* Accept the related pending Salt keys:
+
image::salt-key.png[salt-key.png,scaledwidth=30%]

* Verify that `/srv/pillar/ceph/master_minion.sls` points to your Admin Node.
In our example, it contains our `salt-master` FQDN : 
+
`master_minion: ses55-admin.suse-sap.net`

* Prepare the cluster:
+
----
# salt-run state.orch ceph.stage.0
----
+
image::ceph-stage-0.png[ceph-stage-0.png,scaledwidth=90%]

* Collect information about the nodes:
+
----
# salt-run state.orch ceph.stage.1
----
+
image::ceph-stage-1.png[ceph-stage-1.png,scaledwidth=90%]

* Adapt the file `/srv/pillar/ceph/proposals/policy.cfg` to your needs. In our example, 
where the only deployed service is OpenAttic, it contains the following :
+
----
cluster-ceph/cluster/ses55-osd2.suse-sap.net.sls
config/stack/default/ceph/cluster.yml
config/stack/default/global.yml
profile-default/cluster/ses55-admin.suse-sap.net.sls
profile-default/cluster/ses55-osd0.suse-sap.net.sls
profile-default/cluster/ses55-osd1.suse-sap.net.sls
profile-default/cluster/ses55-osd2.suse-sap.net.sls
profile-default/stack/default/ceph/minions/ses55-admin.suse-sap.net.yml
profile-default/stack/default/ceph/minions/ses55-osd0.suse-sap.net.yml
profile-default/stack/default/ceph/minions/ses55-osd1.suse-sap.net.yml
profile-default/stack/default/ceph/minions/ses55-osd2.suse-sap.net.yml
role-admin/cluster/ses55-admin.suse-sap.net.sls
role-admin/cluster/ses55-osd0.suse-sap.net.sls
role-admin/cluster/ses55-osd1.suse-sap.net.sls
role-admin/cluster/ses55-osd2.suse-sap.net.sls
role-master/cluster/ses55-admin.suse-sap.net.sls
role-mgr/cluster/ses55-osd0.suse-sap.net.sls
role-mgr/cluster/ses55-osd1.suse-sap.net.sls
role-mgr/cluster/ses55-osd2.suse-sap.net.sls
role-mon/cluster/ses55-osd0.suse-sap.net.sls
role-mon/cluster/ses55-osd1.suse-sap.net.sls
role-mon/cluster/ses55-osd2.suse-sap.net.sls
role-openattic/cluster/ses55-admin.suse-sap.net.sls
----

* Prepare the final state of configuration files set:
+
----
# salt-run state.orch ceph.stage.2
----
+
image::ceph-stage-2.png[ceph-stage-2.png,scaledwidth=90%]

* You can now safely deploy your configuration:
+
----
# salt-run state.orch ceph.stage.3
----
+
image::ceph-stage-3.png[ceph-stage-3.png,scaledwidth=90%]

* When stage 3 has completed successfully, check the cluster's health to ensure that everything is running properly:
+
----
# ceph -s
----
+
image::ceph-health.png[ceph-health.png,scaledwidth=90%]

* To get the benefits of the OpenAttic WebUI, you must now initiate the `ceph.stage.4`, which will install the OpenAttic service:
+
----
# salt-run state.orch ceph.stage.4
----
+
image::ceph-stage-4.png[ceph-stage-4.png,scaledwidth=90%]

* You can now manage your cluster through the WebUI:
+
image::openattic-dash.png[openattic-dash.png,scaledwidth=95%]


* To provide a Data Hub RBD device, you first need to create a related pool:

image::openattic-pool.png[openattic-pool.png,scaledwidth=90%]

* Then provide access to this pool through an RBD device:
+
image::openattic-rbd.png[openattic-rbd.png,scaledwidth=90%]


You can now go to <<prerequisites_caasp_cluster>> and follow the prerequisites for a SUSE CaaS Platform cluster.

// === Troubleshooting

// Standard SUSE Best Practices includes
include::common_gfdl1.2_i.adoc[]
