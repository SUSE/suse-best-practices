:docinfo:

// defining article ID
[#art-sapdi30-caasp42]

= SAP Data Intelligence 3 on CaaS Platform 4.2: Installation Guide


== Introduction

This guide describes the on-premises installation of SAP Data Intelligence 3 on
SUSE CaaS Platform 4.2.

== Prerequisites

=== Hardware

For *sizing* information, see in addition the SAP documentation:
link:https://help.sap.com/viewer/835f1e8d0dde4954ba0f451a9d4b5f10/3.0.latest/en-US[Sizing Guide for SAP Data Intelligence]

At least eight nodes are needed for a Kubernetes cluster for production use.

* Minimal requirements:
** three master nodes
** four worker nodes
** one or two loadbalancers (these can be virtual machines)

For the installation of SUSE CaaSP 4.2, additional hosts are needed:

* Management host

* Registry for storing container images

These can be virtual machines.


=== Software

* SUSE Linux Enterprise 15 SP1

* SUSE CaaSP 4.2



== Installing SUSE CaaSP 4.2

=== Documentation

SUSE CaaS Platform 4.2 is documented here:

* link:https://documentation.suse.com/suse-caasp/4.2/[SUSE CaaS Platform product documentation]


// CAVE!
// This needs to be removed when the fix is shipped as maintenance update.
// Due to a bug in cri-o version shipped with SUSE CaaSP 4.2 at the time of this writing, it is necessary to ask SUSE for a PTF for SUSE bug bsc# 117400.
// Download the PTF and install it on all your CaaSP cluster nodes.

// Link to the SUSE TID How to install a PTF.

=== Preparations

On all the nodes, install SUSE Linux Enterprise 15 SP1 or higher, as per the
documentation for CaaS Platform 4.2.

On each respective node, the following modules or products are required.

* Management host:

** SUSE Linux Enterprise 15 SP1
** SUSE Linux Enterprise 15 SP1 Containers Modules
** SUSE Linux Enterprise 15 SP1 Public Cloud
** SUSE CaaSP 4

* Kubernetes master nodes:

** SUSE Linux Enterprise 15 SP1
** SUSE Linux Enterprise 15 SP1 Public Cloud
** SUSE CaaSP 4

* Kubernetes worker nodes:

** SUSE Linux Enterprise 15 SP1
** SUSE Linux Enterprise 15 SP1 Public Cloud
** SUSE CaaSP 4

* Loadbalancer host:

** SUSE Linux Enterprise Server for SAP applications 15 SP1 
+
or
+
** SUSE Linux Enterprise 15 SP1 plus High Availability Extension

=== Installing the SUSE CaaSP 4 cluster nodes

* Install SUSE Linux Enterprise 15 SP1.
+
NOTE: Use the "Expert Partitioner" to disable and remove any automatically-configured
swap partitions on the Kubernetes nodes.

See the relevant product documentation:

* link:https://documentation.suse.com/sles/15-SP1/single-html/SLES-deployment/#book-sle-deployment[SUSE Linux Enterprise Server 15 SP1 Deployment Guide]
* link:https://documentation.suse.com/suse-caasp/4.2/[SUSE CaaS Platform Deployment Guide]


=== Installing the loadbalancer for the Kubernetes cluster

* Install SUSE Linux Enterprise 15 SP1
* Install `ha-proxy` or `nginx`
* Configure the loadbalancer

See the relevant product documentation:

* link:https://documentation.suse.com/suse-caasp/4.2/single-html/caasp-deployment/[SUSE CaaS Platform Deployment Guide]

=== Installing the management workstation

* Install SUSE Linux Enterprise 15 SP1

* Add the necessary SUSE Linux Enterprise 15 SP1 modules: value
+
----
$ sudo SUSEConnect -r CAASP_REGISTRATION_CODE
$ sudo SUSEConnect -p sle-module-containers/15.1/x86_64
$ sudo SUSEConnect -p caasp/4.0/x86_64 -r CAASP_REGISTRATION_CODE
$ sudo SUSEConnect -p sle-module-python2/15.1/x86_64
----

=== Bootstrapping the SUSE CaaSP 4 cluster

* Run the `skuba` tool for initialization of the cluster.

* Make sure that `ssh` is working between all nodes without using passwords, and
configure `ssh-agent`.
+
----
$ eval `ssh-agent`
$ ssh-add <path to key>
----
+
----
$ skuba cluster init --control-plane <LB IP/FQDN> my-cluster
----

* Bootstrap the cluster:
+
----
$ cd my-cluster
$ skuba node bootstrap --target <IP/FQDN> <NODE NAME>
----

* Add additional master nodes:
+
----
$ cd my-cluster
$ skuba node join --role master  --target <IP/FQDN> <NODE NAME>
----

* Repeat this for all the master nodes.

* Add the worker nodes:
+
----
$ cd my-cluster
$ skuba node join --role worker --target <IP/FQDN> <NODE NAME>
----

* Repeat this for all worker nodes.

* Finally, check the cluster status.
+
----
$ cd my-cluster
$ skuba cluster status
$ cp -av  ~/my-cluster/admin.conf  ~/.kube/config
$ kubectl get nodes -o wide
----

++++
<?pdfpagebreak?>
++++

== Adding secure private Docker Registry for container images

TIP: This step is optional if you already have a private secure Docker Registry.
If you skip this chapter, follow the instructions in <<SUSE Enterprise Storage>>.

To satisfy the requirements for SAP Data Intelligence 3, you also need a Docker
Registry. The easiest way to build and manage one is using the link:https://goharbor.io/[Harbor project].

To this end, you need to create a dedicated server for your Docker registry and the
Harbor stack.

WARNING: As Docker only allows characters within the range [a-z],[A-Z],[0-9] and
'-' for domain names, make sure that your FQDN does not contain any other
characters.

In our example, the server will be connected to a local bridge which provides
common services (DNS, SMT, Docker-registry) for the SAP Data Intelligence stack.
The FQDN of this server will be `harbor-registry.example.com`.

[id="Prerequisites"]
=== Prerequisites

Find the prerequisites for Harbor here: link:https://goharbor.io/docs/2.1.0/install-config/installation-prereqs/[Harbor Installation Prerequisites]

Before you can set up Harbor, you need to install Docker and Docker Compose.

* To install Docker, run:
+
----
# zypper in -y docker
----

* To install Docker Compose, you must download the executable from its
link:https://github.com/docker/compose[GitHub repository] and save it into a
directory within your $PATH.
+
For example, run:
+
----
# curl -L "https://github.com/docker/compose/releases/download/1.25.5/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose

# chmod +x /usr/local/bin/docker-compose

# ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
----

The next steps will generate the certificates used to make Harbor secure.
These can also be found in the document link:https://goharbor.io/docs/2.1.0/install-config/configure-https/[Configure HTTPS Access to Harbor].

. First, generate a CA certificate private key:
+
----
# openssl genrsa -out ca.key 4096
----

. Then, generate a certificate with the given key for your domain.
+
NOTE: For all further steps, replace <FQDN> with your fully qualified domain name. 
In our example, this would be `harbor-registry.example.com`.
+
----
# openssl req -x509 -new -nodes -sha512 -days 3650 \
  -subj "/C=DE/ST=BW/O=SUSE/CN=<FQDN>" \
  -key ca.key \
  -out ca.crt
----
+
Your CA certificate is now ready for use.

. Next, you must generate a server certificate as follows:
+
----
# openssl genrsa -out <FQDN>.key 4096
----

. Generate a certificate signing request (CSR):
+
----
# openssl req -sha512 -new \
  -subj "/C=DE/ST=BW/O=SUSE/CN=<FQDN>" \
  -key <FQDN>.key \
  -out <FQDN>.csr
----

. Create an `x509 v3` extension file with the following content:
+
----
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
extendedKeyUsage = serverAuth
subjectAltName = @alt_names

[alt_names]
DNS.1=<FQDN>
DNS.2=<hostname>
----

. Use the extension file to generate a certificate:
+
----
# openssl x509 -req -sha512 -days 3650 \
  -extfile v3.ext \
  -CA ca.crt -CAkey ca.key -CAcreateserial \
  -in $fqdn.csr \
  -out $fqdn.crt
----

. Copy the `.crt` and `.key` files to the system's certificate directory:
+
----
# cp <FQDN>.crt /etc/pki/trust/anchors/
# cp <FQDN>.key /etc/pki/trust/anchors/
----

. As Docker interprets `.crt` files as CA certificates and `.cert` files as
clients, you must convert your `.crt` file as follows:
+
----
# openssl x509 -inform PEM -in <FQDN>.crt -out $<FQDN>.cert
----

. You can now copy the newly created certificates to your Docker certificate
directory. If the directory `/etc/docker/certs.d` does not exist, create it.
+
----
# mkdir /etc/docker/certs.d/<FQDN>
# cp <FQDN>.cert /etc/docker/certs.d/<FQDN>/
# cp <FQDN>.key /etc/docker/certs.d/<FQDN>/
# cp ca.crt /etc/docker/certs.d/<FQDN>/
----
+
NOTE: If you want to expose your registry on a port other than 443, you must
create a directory `/etc/docker/certs.d/<FQDN>:<Port>` and copy the certificates to
this directory instead.

. To introduce the certificates to Docker, restart the Docker daemon:
+
----
# systemctl restart docker
----


=== Setting up Harbor

. Fetch the Harbor installer and extract its contents:
+
----
# wget https://github.com/goharbor/harbor/releases/download/v2.1.1/harbor-online-installer-v2.1.1.tgz
# tar xvf harbor-online-installer-v2.1.1.tgz
----

. Enter the extracted directory:
+
----
# cd harbor
----

. Within this directory, you should find a file called `harbor.yml.tmpl`. It contains
the configuration for the Harbor Registry and must be adjusted.

.. First, edit the `hostname` field and enter your FQDN.

.. Next, update the *HTTPS* configuration. The subentries `certificate` and
`private_key` must be adjusted so they point to the `.crt` and `.key` files you
created in <<Prerequisites>>.
+
This should look like the following:
+
image::harbor_yml.png[title="Harbor Configuration File",480,640]
+
If you want to expose registry on a port other than 443, you can change the `port`
sub-entry to match your desired port.

.. You should also change the `administrator` password for Harbor, which by default
is defined in the field `harbor_admin_password: Harbor12345`.

.. The last field to mention is `data_volume: /data`, which defines where all
Harbor data will be stored. If you want Harbor to store the data somewhere else,
enter the path to the desired directory here.

. When done, save your changes, and rename the file by removing the `.tmpl`
suffix:
+
----
# mv harbor.yml.tmpl harbor.yml
----

. As Harbor uses `nginx` as a reverse proxy for all services, you must run the
`prepare` script to configure it correctly:
+
----
# ./prepare
----

. You can now start the needed containers using Docker Compose:
+
----
# docker-compose up -d
----
+
Harbor should now be up and running.

. Distribute the CA certificate to all Kubernetes nodes, so they can access the
registry. Run the following commands on all the nodes:
+
----
# scp <FQDN>:/etc/docker/certs.d/<FQDN>/ca.crt /etc/docker/certs.d/<FQDN>/<FQDN>.crt
# systemctl restart docker
----
+
For the example <FQDN> of `harbor-registry.example.com`, this looks like the following:
+
----
# scp harbor-registry.example.com:/etc/docker/certs.d/harbor-registry.example.com/ca.crt /etc/docker/certs.d/harbor-registry.example.com/harbor-registry.example.com.crt
# systemctl restart docker
----


=== Verifying configuration and setting up Harbor projects

To verify Harbor is running, you may access its Web front-end by visiting
\https://<FQDN> from your browser.

You may get a warning about an insecure certificate, which looks similar to this:

image::harbor_security_warning.png[title="Harbor Certificate Warning",480,640]

You can toggle the lower box by clicking `Advanced...`
Then click `Accept the Risk and Continue`. 
You will be redirected to the login page of your Harbor registry.

image::harbor_login.png[title="Harbor Login",480,640]

Enter `admin` as the user name and enter the password specified in the `harbor.yml`
file. By default, this is "Harbor12345".

By default, you will be redirected to the project page which holds the
"libraries" project.

image::harbor_projects.png[title="Harbor Projects",480,640]

// TODO projects aufsetzen

You should also check if the Docker clients on your Kubernetes nodes can
access the registry. To do so, run:

----
# docker login <FQDN>
----

You will be prompted to enter a user name and password. Use `admin` for the
user name, and the password you set in `harbor.yml` (default: "Harbor12345").

If Docker can access the registry, you will see a message displayed that states
"Login Succeeded" or similar.

If your machines cannot resolve the FQDN of your registry, edit your
`/etc/hosts` file and add a line with the following information:

----
<IP>  <FQDN>  <Hostname>
----

In our example, this will look as follows:

----
192.168.180.100  harbor-registry.example.com  harbor-registry
----



//TODO remove everything below for this chapter

Create the namespaces on your registry that are needed for SAP Data Intelligence
3:

* `com.sap.hana.container`
* `com.sap.datahub.linuxx86_64`
* `com.sap.datahub.linuxx86_64.gcc6`
* `consul`
* `elasticsearch`
* `fabric8`
* `google_containers`
* `grafana`
* `kibana`
* `prom`
* `vora`
* `kaniko-project`
* `com.sap.bds.docker`

++++
<?pdfpagebreak?>
++++

== SUSE Enterprise Storage

TIP: This step is optional if you already have a storage that provides RBD
volumes and/or S3 buckets. If you skip this chapter, follow the instructions in <<installing_sap_DI_3>>.

An on-premises installation of SAP Data Intelligence 3 requires SUSE Enterprise
Storage 5.5 or higher.

If you plan to use SUSE Enterprise Storage not only for your Kubernetes dynamic
storage class but also for your Kubernetes control plane (virtualized or
not), you should reserve enough resources to address the `etcd`
requirements specified in the link:https://etcd.io/docs/current/op-guide/hardware/[`etcd` Hardware recommendations]

The following steps will deploy a minimalist, virtualized, test-oriented
instance of SUSE Enterprise Storage 6. In our example, we will build a four-nodes (1 Admin + 3 OSD) Ceph
cluster.

=== Before starting

* Obtain registration codes for SUSE Linux Enterprise Server 15 SP1 and SUSE
Enterprise Storage from https://scc.suse.com, or have SMT/RMT properly set up
and already mirroring these products.

++++
<?pdfpagebreak?>
++++

** link:https://scc.suse.com[SCC]
+
image::scc-sle.png[title="SUSE Customer Center - SUSE Linux Enterprise Server",scaledwidth=99% ]

++++
<?pdfpagebreak?>
++++

** link:https://documentation.suse.com/smt/11.3/html/SLE-smt/index.html[SMT]
+
image::scc-ses.png[title="SUSE Customer Center - SUSE Enterprise Storage,scaledwidth=99%]

++++
<?pdfpagebreak?>
++++

=== Setting up the machines

. Download the installation media for SUSE Linux Enterprise Server 15 SP1, for example from:
+
* SUSE Linux Enterprise Server:
** https://www.suse.com/download/sles/
* SUSE Enterprise Storage:
** https://www.suse.com/download/ses/

. To install the software, boot the destination machine and insert the installation media.
After boot, you should see a screen like this:
+
image::SLES_Setup_Init.png[title="SLES Setup Initial Page",480,640]

. Select your preferred language and keyboard layout. Then select
the option "SUSE Linux Enterprise Server 15 SP1" and click the "Next" button.

. You need to agree to the license agreement. Click "Next" again.

. The registration screen should be displayed.
+
image::SLES_Setup_Registration.png[title="SLES Setup Registration Page",480,640]

. Now, either enter your e-mail and the registration code you collected from the
SCC as described in the previous chapter, or select "Register System via local
SMT Server" and enter the URL of your SMT server.

. If required, you can use the *Network Configuration* button at the top right to configure your network settings.
+
image::SLES_Setup_AddOn.png[title="SLES Setup Add On Product Page", 480,640]

. When the system is registered, you will see the "Add On Product" page. You
can skip it by clicking the "Next" button again.

. The "Suggested Partitioning" page will be displayed. You may edit the proposed
partitioning if needed. For our example, we accept the proposal and continue
by clicking the "Next" button.
+
image::SLES_Setup_Partitioning.png[title="SLES Setup Partitioning Page", 480,640]

. On the "Clock and Time Zone" page, you should select your preferred Region and
Time Zone.
+
image::SLES_Setup_TimeZone.png[title="SLES Setup Clock and Time Zone Page", 480,640]
+
Click "Other Settings" to open the "Change Date and Time" page. Select the
"Synchronize with NTP-Server" option. Then select an NTP server and
click the "Accept" button.
+
image::SLES_Setup_NTP.png[title="SLES Setup Change Date and Time Page", 480,640]
+
IMPORTANT: All machines in the cluster must synchronize with the same NTP Server!

+
. Next, you can create a user with name and password of your choice. When done, confirm and click "Next".
+
image::SLES_Setup_User.png[title="SLES Setup Create User Page", 480,640]

. Now the "Installation Settings" page is displayed. Disable the firewall. 
To do so, click the "disable" button located behind "Firewall will be enabled".
+
image::SLES_Setup_Firewall.png[title="SLES Setup Installation Settings Page", 480,640]

. To finalize the installation, go to "Extension and Module Selection" and
select "SUSE Enterprise Storage 6 x86_64".
+
image::SLES_Setup_Extensions.png[title="SLES Setup Extension Page", 480,640]

Set up at least four machines the same way.


=== Setting up SUSE Enterprise Storage

Check that your machines have the correct host names:

----
# hostname -f
----

This should output the FQDN of the machine. If this is not the case, you can use
the following command to set the name:

----
# hostnamectl --set-hostname <FQDN>
----

For the document at hand, we use the following machine names:

- `admin.example.com`
- `mon1.example.com`
- `mon2.example.com`
- `mon3.example.com`

To ensure that DNS resolution for these names works properly, you should edit
the `/etc/hosts` file on the nodes as follows:

----
127.0.0.1   localhost
<Admin-IP>  admin.example.com admin
<Mon1-IP>   mon1.example.com  mon1
<Mon2-IP>   mon2.example.com  mon2
<Mon3-IP>   mon3.example.com  mon3
----

If you do not know the IP address of a machine, run:

----
$ ip a
----

This should output something similar to the following:

image::fetch_ip.png[title="Fetch IP", 480,640]


==== Preparing the nodes

. On the OSD Nodes, install `salt-minion` and `deepsea`:
+
----
# zypper in -y salt-minion deepsea
----

. On the Admin Node, install `salt-minion`, `salt-master`, and `deepsea`:
+
----
# zypper in -y salt-minion salt-master deepsea
----

. Set the Salt master on all machines by editing the associated line:
+
----
# echo "master: admin.example.com" > /etc/salt/minion
----

. On the Admin node, enable the `salt-master` service:
+
----
# systemctl enable salt-master --now
----

. Enable the `salt-minion` service on all nodes:
+
----
# systemctl enable salt-minion --now
----

. Now accept the Salt keys on the admin node:
+
----
# salt-key --accept-all -y
----
+
TIP: To check the keys, use the command `salt-key -L`. This should output something like:
+
image::salt-key.png[title="Salt Keys",scaledwidth=30%]

. Make all nodes DeepSea minions:
+
----
# echo "deepsea_minions: '*'" > /srv/pillar/ceph/deepsea_minions.sls
----

. Synchronize your Salt minions:
+
----
# salt '*' saltutil.sync_all
----

. Make sure your desired disks are all cleared. To this end, you can use:
+
----
# wipefs -a /dev/vdb
----
+
NOTE: Be aware that you might need to change the device (`vdb`) here. Ensure
you clear all disks on all nodes.

. Apply the cleared disks by using:
+
----
# salt '*' state.apply ceph.subvolume
----

==== Deploying the cluster

TIP: You can watch the progress of the Ceph stages in a separate
terminal with the command `deepsea monitor`.

Now you can run the first stage of deploying the Ceph cluster.

. First, prepare the cluster:
+
----
# salt-run state.orch ceph.stage.prep
----

. The result for the preparation stage should look similar to this:
+
image::ceph-stage-0.png[title="Cluster Deployement - Preparation",480,500]

. The next stage is collecting information about the nodes:
+
----
# salt-run state.orch ceph.stage.discovery
----
+
image::ceph-stage-1.png[title="Cluster Deployment - Stage 1",scaledwidth=70%]

. Before you can run the last three stages, you must provide a role
configuration for the nodes. This will be stored in  `/srv/pillar/ceph/proposals/policy.cfg`.
This example uses the following configuration:
+
----
#General config
config/stack/default/global.yml
config/stack/default/ceph/cluster.yml

#CEPH Cluster members
cluster-ceph/cluster/admin.example.com.sls
cluster-ceph/cluster/mon1.example.com.sls
cluster-ceph/cluster/mon2.example.com.sls
cluster-ceph/cluster/mon3.example.com.sls

#CEPH Admin nodes
role-admin/cluster/mon1.example.com.sls
role-admin/cluster/mon2.example.com.sls
role-admin/cluster/mon3.example.com.sls

#CEPH Master node
role-master/cluster/admin.example.com.sls

#CEPH Manager nodes
role-mgr/cluster/mon1.example.com.sls
role-mgr/cluster/mon2.example.com.sls
role-mgr/cluster/mon3.example.com.sls

#CEPH Monitor nodes
role-mon/cluster/mon1.example.com.sls
role-mon/cluster/mon2.example.com.sls
role-mon/cluster/mon3.example.com.sls

#CEPH RGW nodes
role-rgw/cluster/admin.example.com.sls
role-rgw/cluster/mon1.example.com.sls
role-rgw/cluster/mon2.example.com.sls
role-rgw/cluster/mon3.example.com.sls

#CEPH Storage nodes
role-storage/cluster/admin.example.com.sls
role-storage/cluster/mon1.example.com.sls
role-storage/cluster/mon2.example.com.sls
role-storage/cluster/mon3.example.com.sls
----

. You can now safely deploy your configuration:
+
----
# salt-run state.orch ceph.stage.configure
----
+
image::ceph-stage-2.png[title="Cluster Deployment - Stage 2",scaledwidth=70%]

. Deploy your configuration to the cluster:
+
----
# salt-run state.orch ceph.stage.deploy
----
+
image::ceph-stage-3.png[title="Cluster Deployment - Stage 3",scaledwidth=70%]

. When the deployment stage has been successfully passed, check the cluster
health to insure that all is running properly.
+
----
# ceph -s
----
+
image::ceph-health.png[title="Checking Cluster Health",scaledwidth=70%]

. The last stage to run is deploying the service roles, which were specified
in the `policy.cfg` file:
+
----
# salt-run state.orch ceph.stage.services
----
+
image::ceph-stage-4.png[title="Cluster Deployment - Stage 4",scaledwidth=70%]


=== Access the dashboard and create a new pool

After the Ceph cluster is up and running, you must create a pool for SAP Data
Intelligence 3.

In our example, we will use the dashboard for this purpose.

. The dashboard is published by any of the monitor nodes. To access it, use a Web
browser:
+
image::ceph-dashboard-login.png[title="Cluster Dashboard",scaledwidth=99%]
+
Get the credentials to log in by running the following command on the
Admin Node:
+
----
# salt-call grains.get dashboard_creds
----
+
image::ceph-dashboard-creds.png[title="Cluster Dashboard Login",scaledwidth=99%]
+
After logging in, the landing page should appear.

. Select the "Pools" tab at the top of the page:
+
image::ceph-dashboard-landing.png[title="Cluster Dashboard Landing Page",scaledwidth=99%]
+
The Pools page gives an overview of the currently defined pools.

. Click the "Create" button at the top of the table to create a new pool:
+
image::ceph-dashboard-pools.png[title="Cluster Dashboard Pools",scaledwidth=99%]

. Enter the name of the pool. As "Pool type", select `replicated`.

. On the left side of "Applications", click the pencil symbol and select `rbd`.

. Confirm the creation of the pool by clicking the "CreatePool" button at the
bottom.
+
image::ceph-dashboard-create-pool.png[title="Cluster Dashboard Creating Pools",scaledwidth=99%]
+
IMPORTANT: Make note of the name of the pool. It will be needed in <<creating_storage_class>>.

. After this, the pools page is displayed again, and the newly created pool is shown
in the table of pools.
+
image::ceph-dashboard-pools2.png[title="Cluster Dashboard New Pool",scaledwidth=99%]

. Now provide access to this pool through an RBD device. Go to the
RDB overview page by selecting "Block->Images"
+
image::ceph-dashboard-access-rbd.png[title="Cluster Dashboard Accessing Pool",scaledwidth=99%]
+
An overview of the configured RDBs is displayed.

. Click the *Create* button.
+
image::ceph-dashboard-rbd.png[title="Cluster Dashboard RDB Overview",scaledwidth=99%]

. Enter the name, and if it is not already selected, select the
previously created pool. Select the size of the RBD and confirm the creation by
clicking the "CreateRBD" button.
+
image::ceph-dashboard-create-rbd.png[title="Cluster Dashboard Creating New RDB",scaledwidth=99%]
+
The overview page of the RBDs is shown again. It now contains your newly created RBD.
+
image::ceph-dashboard-rbd2.png[title="Cluster Dashboard RDB Updated Overview",scaledwidth=99%]

At this point, the SUSE Enterprise Storage cluster is ready for usage with SAP Data Intelligence.


== Installing SAP Data Intelligence 3 on top of SUSE CaaSP 4.2

=== Documentation

// FIXME add links

* SAP Notes:

** link:https://launchpad.support.sap.com/#/notes/2871970[Release Notes for SAP Data Intelligence 3]

** link:https://help.sap.com/viewer/a8d90a56d61a49718ebcb5f65014bbe7/3.0.latest/en-US[Installation Guide for SAP Data Intelligence 3]

//** Installation Notes ← moved to end & commented out as no link & I can't find one — LGHP


=== Planning the installation with the SAP Maintenance Planner

For the installation of SAP Data Intelligence, you should start here:
link:https://support.sap.com/en/alm/solution-manager/processes-72/maintenance-planner.html[SAP Maintenance Planner]

NOTE: You need to have your SAP `S-User` available.

* The landing page of the SAP Maintenance Planner looks as follows:
+
image::SAP-DI-30-CaaSP42-0001.png[title="SAP Maintenance Planner Start page",480,640,scaledwidth=99%]
+
Click "Plan a New System" on the right.

* The next page displayed looks as follows:
+
image::SAP-DI-30-CaaSP42-0002.png[title="SAP Maintenance Planner: Select Plan",480,640,scaledwidth=99%]
+
You will see a circle where all options except the option "Plan" are greyed out.
Click the "Plan" option.

* The next page shows the "Define Change" step of your planning:
+
image::SAP-DI-30-CaaSP42-0003.png[title="SAP Maintenance Planner: Select Container Based Product",480,640,scaledwidth=99%]

* On the left, there is a window with three toggle buttons. Select
"CONTAINER BASED" and click "OK":
+
image::SAP-DI-30-CaaSP42-0004.png[title="SAP Maintenance Planner: Select Container Based Product #2",480,640,scaledwidth=99%]

* To the right, the option "SAP DATA INTELLIGENCE" should appear. When you select
it, a sub-selection should open with choices for "SAP DATA INTELLIGENCE 3" and
"SAP DATA HUB 2". Select "SAP DATA INTELLIGENCE 3":
+
image::SAP-DI-30-CaaSP42-0005.png[title="SAP Maintenance Planner: Select Data Intelligence 3",480,640,scaledwidth=99%]

* A pop-up window will appear to inform you about the related SAP note. Click
"Continue" to proceed:
+
image::SAP-DI-30-CaaSP42-0006.png[title="SAP Maintenance Planner: Select Continue",480,640,scaledwidth=99%]

* On the right hand side, a drop-down box is shown with "Select Support Package
Stack". Click this, and select from the available patch levels as needed:
+
image::SAP-DI-30-CaaSP42-0007.png[title="SAP Maintenance Planner: Select from available patch levels as you need. ",480,640,scaledwidth=99%]

* To the left, the sub-selection for "SAP DATA INTELLIGENCE 3" will need changes.
Select what you need and click the "Confirm Selection" at the very right:
+
image::SAP-DI-30-CaaSP42-0008.png[title="SAP Maintenance Planner: Select according your needs and confirm.",480,640,scaledwidth=80%,align=center]

* An overview of your selection is shown. If this fits your needs, click the
"Next" button at the upper right corner:
+
image::SAP-DI-30-CaaSP42-0009.png[title="SAP Maintenance Planner: Select next if satisfied",480,640,scaledwidth=99%]

* Next, select the operating system upon which SAP Data Intelligence will
be installed. Select "Linux on x86_64 64bit" and click "Confirm Selection":
+
image::SAP-DI-30-CaaSP42-0010.png[title="SAP Maintenance Planner: Select Linux and confirm",480,640,scaledwidth=99%]

* The next page shows the preselected files to use and download. Again, click
the "Next" button on the upper right:
+
image::SAP-DI-30-CaaSP42-0011.png[title="SAP Maintenance Planner: Confirm",480,640,scaledwidth=99%]

* You should now arrive at the "Download Files" page. The required *SLC bridge*
is already preselected. Click "Next" to proceed:
+
image::SAP-DI-30-CaaSP42-0012.png[title="SAP Maintenance Planner: Execute Plan",480,640,scaledwidth=99%]

* Your maintenance plan is shown as PDF. Confirm everything by clicking the
"Execute Plan" button on the upper right side:
+
image::SAP-DI-30-CaaSP42-0013.png[title="SAP Maintenance Planner: Download Stack.xml and SLC Bridge Installer",480,640,scaledwidth=99%]

* Download the *SLC Bridge Installer* and copy it to your management workstation.
You will need this file in <<Installing the SAP SLC Bridge>>.

* You will be prompted to enter the FQDN and the port of the machine your SLC
bridge will run on:
+
image::SAP-DI-30-CaaSP42-0014.png[title="SAP Maintenance Planner: Enter FQDN of host where the SLC Bridge will run",480,640,scaledwidth=99%]

* Fill in the values. An example looks as follows:
+
image::SAP-DI-30-CaaSP42-0015.png[title="SAP Maintenance Planner: Example for host and port",480,640,scaledwidth=99%]


=== Installing the SAP SLC Bridge

* Download the file containing the SLC Bridge Installer.

* If you have not already done so, copy this file to the management workstation.

// Prepare environment for kubectl, helm etc.
// FIXME check parameters for SLCB01_50

* Run the *SLC Bridge Installer* on the management workstation.
+
----
$ ./SLCB01_<YOUR DOWNLOADED VERSION>.EXE init
----
+
image::SAP-DI-30-CaaSP42-0017.png[title="SAP SLC Bridge Installer",480,640,scaledwidth=99%]

* This interactive script gathers all the necessary information to run the SAP SLC
Bridge, and at the end, deploys it into the CaaSP cluster.
+
image::SAP-DI-30-CaaSP42-0025.png[title="SAP SLC Bridge Information",480,640,scaledwidth=99%]

* Identify the service port for the SLC Bridge:
+
----
# kubectl -n sap-slcbridge get pods
NAME                              READY   STATUS    RESTARTS   AGE
di-platform-full-product-bridge   2/2     Running   0          3d23h
slcbridgebase-858f895bd6-74gps    2/2     Running   1          10d
# kubectl -n sap-slcbridge get svc
NAME                                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
di-platform-full-product-bridge-service   ClusterIP   10.101.2.224   <none>        9000/TCP         3d23h
slcbridgebase-service                     NodePort    10.97.79.26    <none>        9000:30778/TCP   10d
----
+
In our example, the port number on which the SLC Bridge listens is `30778`.

* Make a note of this information. It is needed for the installation process via
SAP Maintenance Planner.
+
image::SAP-DI-30-CaaSP42-0026.png[title="SAP SLC Bridge",480,640,scaledwidth=99%]


[id="installing_sap_DI_3"]
=== Installing SAP Data Intelligence 3

This section describes the installation of SAP Data Intelligence 3 on top of
SUSE CaaSP 4.2.

==== Preparations

Before the installation of SAP DI 3 can start, some preparation work must be
done:

* Create a namespace on the Kubernetes cluster.

* Define a (default) storage class on the Kubernetes cluster.

* Adapt `PodSecurityPolicies`.

* Create the necessary `ClusterRoleBindings`.

* If you use self-signed certificates for the private registry, a special secret
must be created.

NOTE: Unless otherwise specified, all these tasks are run from the management
workstation.

===== Creating namespace for SAP Data Intelligence 3 on Kubernetes

Define the namespace into which SAP Data Intelligence 3 will be installed:

----
$ kubectl create namespace <YOUR NAMESPACE>
----

[id="creating_storage_class"]
===== Creating Storage Class

* Create the storage class to provide volumes for SAP Data Intelligence 3 on
SUSE Enterprise Storage.

* Make sure that you have:

** the connection data for your SUSE Enterprise Storage at hand

** the IP addresses and port number (default: 6789) of the monitor nodes of your
SES cluster

** created a data pool on your SES cluster for use with SAP Data
Intelligence 3

** the name of this pool (`datahub` in this example) available

* Edit the example below to fit your environment.
+
----
$ cat > storageClass.yaml <<EOF
apiVersion: storage.kubernetes.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
  name: datahub
  namespace: default
parameters:
  adminId: admin
  adminSecretName: ceph-admin-secret
  adminSecretNamespace:  default
  imageFeatures: layering
  imageFormat: "2"
  monitors: <IP ADDRESS OF MONITOR 1>:6789, <IP ADDRESS OF MONITOR 2>:6789, <IP ADDRESS OF MONITOR 3 >:6789
  pool: datahub
  userId: admin
  userSecretName: ceph-user-secret
provisioner: kubernetes.io/rbd
reclaimPolicy: Delete
volumeBindingMode: Immediate
EOF

$ kubectl create -f storageClass.yaml
----

* Create secrets for the StorageClass.

** Create the secrets needed to access the storage.

** Obtain the keys from your SES cluster. These are located in
`ceph.admin.keyring` and `ceph.user.keyring`.
+
You must encode the keys with `base64`.
+
----
$ echo <YOUR KEY HERE> | base64
----
+
----
$ cat > ceph-admin-secret.yaml <<EOF
apiVersion: v1
kind: Secret
metadata:
    name: ceph-admin-secret
type: "kubernetes.io/rbd"
data:
   key: <YOUR BASE64 ENCODED KEY HERE>
EOF
----
+
----
$ cat > ceph-user-secret.yaml <<EOF
apiVersion: v1
kind: Secret
metadata:
    name: ceph-user-secret
type: "kubernetes.io/rbd"
data:
   key: <YOUR BASE64 ENCODED KEY HERE>
EOF

$ kubectl apply -f ceph-admin-secret.yaml
$ kubectl apply -f ceph-user-secret.yaml
----

** Create the credentials for accessing the StorageClass from the namespace where DI 3 will be installed into.
+
----
$ kubectl -n <YOUR NAMESPACE FOR DI 3> create -f ceph-admin-secret.yaml
$ kubectl -n <YOUR NAMESPACE FOR DI 3> create -f ceph-user-secret.yaml
----
//does this work ?

===== Creating `PodSecurityPolicies` and `ClusterRoleBindings`

* PodSecurityPolicies
+
----
$ kubectl edit psp suse.caasp.psp.privileged
----
+
Change the `pathPrefix` in `allowedHostPaths` to `/`

* ClusterRoleBindings
+
----
$ cat > clusterrolebinding.yaml << EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: suse:caasp:psp:privileged:default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: suse:caasp:psp:privileged
subjects:
- kind: ServiceAccount
  name: default
  namespace: XXX
- kind: ServiceAccount
  name: vora-vsystem-XXX
  namespace: XXX
- kind: ServiceAccount
  name: vora-vsystem-XXX-vrep
  namespace: XXX
- kind: ServiceAccount
  name: XXX-elasticsearch
  namespace: XXX
- kind: ServiceAccount
  name: XXX-fluentd
  namespace: XXX
- kind: ServiceAccount
  name: XXX-nodeexporter
  namespace: XXX
- kind: ServiceAccount
  name: vora-vflow-server
  namespace: XXX
- kind: ServiceAccount
  name: mlf-deployment-api
  namespace: XXX
EOF
$ sed -i s/XXX/<your-di-namespace>/g clusterrolebinding.yaml
$ kubectl apply -f clusterrolebinding.yaml
----

* Additional changes
+
----
$ kubectl edit clusterrolebinding system:node
----
+
Insert the following at the end of the file:
+
----
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:nodes
----

* If you use self-signed SSL certificates for the secure private registry,
create a secret for accessing this registry.
+
NOTE: The certificate chain should be saved in `pem` format into a single file
called `cert`.
+
----
export NAMESPACE=<your namespace>
mv cert cert_with_carriage_return
tr -d '\r' < cert_with_carriage_return > cert
kubectl create secret generic cmcertificates --from-file=cert -n $NAMESPACE
----

==== Installing SAP Data Intelligence 3

After you successfully finished the preparation stages, proceed with the
installation of SAP Data Intelligence 3. To do so, several steps must be executed.

===== Connecting to the SLC Bridge

* Point your browser to the SLC Bridge service:

** \https://<yournode>:<yourport>/docs/index.html

* Fill in your credentials you created during the deployment of the SLC Bridge.

// The installation workflow starts by connecting the service port of the slcb bridge.

//FIXME screen shot???


===== Installing the workflow

// FIXME lots of images


* Connect and authenticate to the SLC Bridge service created above. Use the credentials created during the setup of the SLC Bridge:
+
image::SAP-DI-30-CaaSP42-0030.png[title="Connect and authenticate to the SLCB Service",480,640,scaledwidth=99%]

* Select "Planned software Changes":
+
image::SAP-DI-30-CaaSP42-0031.png[title="Installing planned software changes",480,640,scaledwidth=99%]

* Select the SAP Data Intelligence deployment required by your needs. Click "Next":
+
image::SAP-DI-30-CaaSP42-0032.png[title="Installing the required SAP DI deployment",480,640,scaledwidth=99%]

* Enter the Kubernetes namespace created beforehand, for example `di310`. Click "Next":
+
image::SAP-DI-30-CaaSP42-0033.png[title="Installing Pre-requisites Check",480,640,scaledwidth=99%]
+
image::SAP-DI-30-CaaSP42-0034.png[title="Installation Enter kubernetes namespace ",480,640,scaledwidth=99%]

* Select "Advanced Installation":
+
image::SAP-DI-30-CaaSP42-0035.png[title="Installing advanced installation",480,640,scaledwidth=99%]

* Enter the URI of your Private secure registry. Click "Next":
+
image::SAP-DI-30-CaaSP42-0036.png[title="Installing Private Container registry",480,640,scaledwidth=99%]

* Enter a password for the system tenant in SAP DI 3.0:
+
image::SAP-DI-30-CaaSP42-0037.png[title="Enter System tenant password",480,640,scaledwidth=99%]

* Assign a name to the initially created tenant, for example "default":
+
image::SAP-DI-30-CaaSP42-0038.png[title="Assign a name to created default tenant",480,640,scaledwidth=99%]

* Create the administrator user for the default tenant in SAP DI 3.0:
+
image::SAP-DI-30-CaaSP42-0039.png[title="Create Admin user name of default tenant",480,640,scaledwidth=99%]

* Set the password for the administrator user of the default tenant:
+
image::SAP-DI-30-CaaSP42-0040.png[title="Set admin user password",480,640,scaledwidth=99%]

* If you need a proxy to connect to the Internet, set the proxy settings accordingly:
+
image::SAP-DI-30-CaaSP42-0041.png[title="Set proxy settings",480,640,scaledwidth=99%]

* Select if you want to use a checkpoint storage:
+
image::SAP-DI-30-CaaSP42-0042.png[title="Configure checkpoint storage",480,640,scaledwidth=99%]

* Define the storage class that should be used by SAP DI 3.0. Enter the name of
the storage class you created previously:
+
image::SAP-DI-30-CaaSP42-0043.png[title="Define storage class to be used",480,640,scaledwidth=99%]

* For the SAP DI 3 installation on SUSE CaaSP 4.2 the docker log path needs to be adapted. Check the check box and click "Next":
+
image::SAP-DI-30-CaaSP42-0044.png[title="Adapt Docker log path",480,640,scaledwidth=99%]

* Enter the docker log path: "/var/log/containers"
+
image::SAP-DI-30-CaaSP42-0045.png[title="Enter Docker log path",480,640,scaledwidth=99%]

* Enable Kaniko:
+
image::SAP-DI-30-CaaSP42-0046.png[title="Enable Kaniko",480,640,scaledwidth=99%]

* Here, a different private registry can be configured if needed.
To proceed, click "Next":
+
image::SAP-DI-30-CaaSP42-0047.png[title="Configure Docker registry for SAP DI Modeler images",480,640,scaledwidth=99%]

* Enable the loading of NFS kernel modules. This ensures that the NFS kernel
modules are loaded on all Kubernetes nodes. Check the check box and click "Next":
+
image::SAP-DI-30-CaaSP42-0048.png[title="Loading NFS kernel modules",480,640,scaledwidth=99%]

* If needed, enable Network policies. Click "Next":
+
image::SAP-DI-30-CaaSP42-0049.png[title="Enable Network policies",480,640,scaledwidth=99%]

* Configure timeout during installation, leave the default and click "Next":
+
image::SAP-DI-30-CaaSP42-0050.png[title="Configuring timeout during installation of SAP DI 3",480,640,scaledwidth=99%]

* In the field "Additional Installation Parameters", enter:
+
`-e diagnostic.fluentd.logDriverFormat=regexp -e vsystem.vRep.exportsMask=true`
+
image::SAP-DI-30-CaaSP42-0051.png[title="Additional Installation Parameters 1",480,640,scaledwidth=99%]
+
image::SAP-DI-30-CaaSP42-0053.png[title="Additional Installation Parameters 2",480,640,scaledwidth=99%]

// does not exist image::SAP-DI-30-CaaSP42-0052.png[title="Installation #23",480,640,scaledwidth=99%]

* Check the Summary page. Check if the settings are correct:
+
image::SAP-DI-30-CaaSP42-0054.png[title="Summary of Installation parameters",480,640,scaledwidth=99%]

* Click "Next" to start the deployment of SAP DI 3.0.

Your installation should now be finished.

//image::SAP-DI-30-CaaSP42-0055.png[title="Installation #26",480,640,scaledwidth=80%,align=center]
//image::SAP-DI-30-CaaSP42-0056.png[title="Installation #27",480,640,scaledwidth=80%,align=center]
//image::SAP-DI-30-CaaSP42-0057.png[title="Installation #28",480,640,scaledwidth=80%,align=center]
//image::SAP-DI-30-CaaSP42-0058.png[title="Installation #29",480,640,scaledwidth=80%,align=center]


// FIXME Parameters used in the installation workflow
// from SAP Note:
// -e Choose "Configure container log path" for "Docker Container Log Path Configuration"
// Set "Container Log Path" as "/var/log/containers"
// Set "Enable Kaniko Usage" as yes
// Enter "-e diagnostic.fluentd.logDriverFormat=regexp -e vsystem.vRep.exportsMask=true" into "Additional Installation Parameters"
// container log path
// enable kaniko
// load kernel module nfsd and nfsv4 !! CAVE !! 5.x kernels might crash when container tries to load these modules. This for future releases on SLE 15 SP2.

//    If the registry certificate is untrusted or self-signed, follow the steps below to create necessary Kubernetes secret with the certificates so that SAP Data Intelligence installation procedure can import it into SAP Data Intelligence Connection Manager.
//        Certificate chain should be saved into file named "cert" in PEM format.
//        export DI installation namespace into NAMESPACE environment variable.
//        Ensure that your file doesn't contain DOS-type line endings. To convert from DOS-type line endings, you may use the following commands:
//
//                mv cert cert_with_carriage_return
//
//                tr -d '\r' < cert_with_carriage_return > cert
//
//        run "kubectl create secret generic cmcertificates --from-file=cert -n $NAMESPACE" command.


// == Appendix

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
== Legal notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
include::common_gfdl1.2_i.adoc[]
