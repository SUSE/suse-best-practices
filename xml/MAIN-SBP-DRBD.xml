<?xml version="1.0" encoding="UTF-8"?>
<!--<?oxygen RNGSchema="http://www.oasis-open.org/docbook/xml/5.0/rng/docbook.rng" type="xml"?>-->
<!DOCTYPE article [
<!ENTITY % entity SYSTEM "entity-decl.ent">
%entity;
]>
<article xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="art.sbp.suma.life"
    xml:lang="en">

    <info>
        <title>Data Replication across Geo-clusters via DRBD included with SUSE Linux Enterprise
            High Availability Extension</title>
        <subtitle>Methods and approaches for managing updates using SUSE Manager in multi-landscape
            environments</subtitle>
        <orgname>SUSE Best Practices</orgname>
        <productname>SUSE Linux Enterprise High Availability Extension</productname>
        <productnumber>2.1 and 3.0</productnumber>

        <author>
            <personname>
                <firstname>Matt</firstname>
                <surname>Kereczman</surname>
            </personname>
            <affiliation>
                <jobtitle>Cluster Engineer</jobtitle>
                <orgname>LINBIT</orgname>
            </affiliation>
        </author>

        <author>
            <personname>
                <firstname>Philipp</firstname>
                <surname>Marek</surname>
            </personname>
            <affiliation>
                <jobtitle>Senior Software Developer</jobtitle>
                <orgname>LINBIT</orgname>
            </affiliation>
        </author>

        <author>
            <personname>
                <firstname>Kristoffer</firstname>
                <surname>Groenlund</surname>
            </personname>
            <affiliation>
                <jobtitle>Architect High Availability</jobtitle>
                <orgname>&suse;</orgname>
            </affiliation>
        </author>

        <date>October 12, 2016</date>

        <abstract>
            <para>This technical setup describes the setup of a geo cluster using Distributed
                Replicated Block Device (DRBD) as delivered with the SUSE Linux Enterprise High
                Availability Extension.</para>
        </abstract>
    </info>


    <sect1 xml:id="sec.background">
        <title>Background</title>

        <sect2 xml:id="sec.drbd">
            <title>About DRBD</title>

            <para>DRBD is a Linux-kernel block-level replication facility that is widely used as an
                shared-nothing cluster building block. It is included in vanilla kernels since
                2.6.33, and most distributions ship the necessary userspace utilities. Furthermore,
                many distributions have newer DRBD versions than the one included in the kernel
                package in extra packages.</para>

            <para>DRBD can replicate across multiple network protocols, and in (currently) three
                modes, from synchronous for local HA clusters, to asynchronous for pushing data to a
                disaster recovery site.</para>

            <para>DRBD is developed and supported world-wide by LINBIT <link
                    xlink:href="http://www.linbit.com"/>. That includes most distributions and
                architectures, with Service Level Agreements (SLA) levels up to 24/7 email and phone
                availability.</para>
        </sect2>

        <sect2 xml:id="sec.sleshae">
            <title>About SUSE Linux Enterprise High Availability Extension</title>

            <para>SUSE Linux Enterprise High Availability Extension is an integrated suite of open
                source clustering technologies that enables you to implement highly available
                physical and virtual Linux clusters, and to eliminate single points of failure. It
                ensures the high availability and manageability of critical network resources
                including data, applications, and services. Thus, it helps you maintain business
                continuity, protect data integrity, and reduce unplanned downtime for your
                mission-critical Linux workloads.</para>

            <para>SUSE Linux Enterprise High Availability Extension ships with essential monitoring,
                messaging, and cluster resource management functionality (supporting failover,
                failback, and migration (load balancing) of individually managed cluster
                resources).</para>

            <para>SUSE Linux Enterprise High Availability Extension includes DRBD.</para>
        </sect2>
    </sect1>

    <sect1 xml:id="sec.introduction">
        <title>Introduction</title>

        <para>This tech-guide describes a setup consisting of two highly available Pacemaker
            clusters in two sites, with a routed IPv4 or IPv6 connection in between. The connection
            can range from a few Mbit/sec up to 10GBit/ sec, depending on the IO load imposed on the
            cluster nodes.</para>

        <para>Various services can be distributed across the clusters. Because of latency between
            the data centers you will not be able to drive a cluster filesystem across them. But
            simply replicating the data to have a quick failover in case one site goes down is
            possible.</para>

        <para>Failover between the two sites is managed using the <emphasis role="italic">Booth
                Cluster Ticket Manager</emphasis>, which is included in the Geo Clustering for SUSE
            Linux Enterprise High Availability Extension. In addition to the two cluster sites, a
            third site is needed to run a booth Arbitrator. Arbitrators are single machines that run
            a booth instance in a special mode. As all booth instances communicate with each other,
            arbitrators help to make more reliable decisions about granting or revoking tickets.
            Arbitrators cannot hold any tickets. </para>

        <para>Lots of details will be skimmed; for example, for this technical guide it does not
            matter whether the application driving IO is SAP, an NFS server, a PostgreSQL instance,
            VMs via KVM, Apache, MySQL, or whatever else you may have in use.</para>

    </sect1>

    <sect1 xml:id="sec.installation">
        <title>Installation Requirements</title>

        <para>You will need to have the Geo Clustering for SUSE Linux Enterprise High Availability
            Extension installed on both pairs of nodes, as well as on the Arbitrator node. For a
            detailed description on how to configure the Geo cluster nodes, see the Geo Clustering
            Quick Start document included in the Geo Clustering for SUSE Linux Enterprise High
            Availability Extension documentation at <link
                xlink:href="https://www.suse.com/documentation/sle-ha-geo-12/art_ha_geo_quick/data/art_ha_geo_quick.html"
            />.</para>

        <para>It is good practice and recommended to use LVM as storage layer below DRBD. This
            facilitates quick and easy creation and online enlarging of DRBD devices.</para>

        <para>You will also need to install all the software inclusive dependencies for the services
            you want to run on all four nodes. In case you replicate Virtual Machines (VMs), these
            are typically self-contained, so you’ll just need the KVM environment like virsh on the
            cluster nodes.</para>

    </sect1>

    <sect1 xml:id="sec.architecture">
        <title>Cluster Architecture Overview</title>

        <para>The following image depicts a two site, four-node cluster with arrows showing the
            direction of replication, and blocks in orange showing that a DRBD device is Primary.
            There are two nodes in the local site, <emphasis role="italic">alice</emphasis> and
                <emphasis role="italic">bob</emphasis>, and two nodes in the remote site, <emphasis
                role="italic">charlie</emphasis> and <emphasis role="italic"
            >daisy</emphasis>.</para>

        <para>Each site will have its own DRBD resource stack and service IP to allow independent
            migration across the cluster nodes.</para>

        <figure>
            <title>Four-Node Cluster Stacked</title>
            <mediaobject>
                <imageobject role="fo">
                    <imagedata fileref="DRBD-four-node-stacked.png" width="90%" format="PNG"/>
                </imageobject>
                <imageobject role="html">
                    <imagedata fileref="DRBD-four-node-stacked.png" width="90%" format="PNG"/>
                </imageobject>
            </mediaobject>
        </figure>

        <para>The most important details of the cluster architecture are the following:</para>

        <itemizedlist>
            <listitem>
                <para>The local high availability setup: DRBD running in Protocol C, using IP
                    addresses in a LAN respective a cross-over connection.</para>
            </listitem>
            <listitem>
                <para>Just after activating the lower DRBD devices on one node, the dedicated
                    service IP address is started.</para>
                <para>This is not only used for the service as such, but also as a fixed point that
                    can be accessed by the upper DRBD device (in Secondary state) for
                    replication.</para>
            </listitem>
            <listitem>
                <para>The upper layer DRBD runs on one node per site, and is responsible to
                    replicate the data to the other data replication site. This runs in protocol A,
                    and might have a DRBD Proxy setup in between.</para>

                <note>
                    <title>Data Compression</title>
                    <para>The DRBD Proxy buffers and optionally compresses data, from one or
                        multiple DRBD resources. Compression helps if the site interconnect is
                        slower than the summed average I/O rate of the resources. The ratio is about
                        1:4 for zlib, and up to 1:50 for lzma compression. Please see the
                            <quote>DRBD Proxy 3 Compression Considerations</quote> whitepaper on
                            <link xlink:href="http://www.linbit.com/"/> for more details.</para>
                </note>
            </listitem>
            <listitem>
                <para>On the site that should actually run the service, the upper layer DRBD gets
                    set Primary, so that the filesystem therein can be mounted and used by the
                    application.</para>
            </listitem>
        </itemizedlist>
    </sect1>

    <sect1 xml:id="sec.example_config">
        <title>Example Configurations for a Single Service</title>

        <para>The example configurations below are using the following premises:</para>

        <itemizedlist>
            <listitem>
                <para>Two sites, called <emphasis role="strong">RZ1</emphasis> and <emphasis
                        role="strong">RZ2</emphasis>, with two private networks <emphasis
                        role="strong">192.168.201.x</emphasis> and <emphasis role="strong"
                        >192.168.202.x</emphasis>; routed to the other site</para>
            </listitem>
            <listitem>
                <para>Four nodes, called <emphasis role="strong">geo_rzN-M</emphasis> in four
                    combinations: <emphasis role="strong">geo-rz1-a</emphasis> to
                        <emphasis>geo-rz2-b</emphasis></para>
            </listitem>
            <listitem>
                <para>NFS is to be served; but there’s not much difference for other services</para>
            </listitem>
            <listitem>
                <para>Nodes are using LVM, VG name is <emphasis role="strong"
                    >volgroup</emphasis></para>
            </listitem>
            <listitem>
                <para>The lower DRBD layer (for the HA-clusters) uses minor 0; minor 10 is used for
                    DR replication</para>
            </listitem>
        </itemizedlist>

        <sect2 xml:id="sec.drbd_config">
            <title>DRBD Configuration</title>

            <para>The following snippets show a basic DRBD configuration. These are bare-bones;
                performance-tuning options are not included here.</para>

            <para>All three snippets can be contained in a single resource file, for example in
                    <filename role="italic">/etc/drbd.d/nfs.res</filename>. This is the recommended
                configuration, because keeping that synchronized across the four cluster nodes is
                easier. You can also consult <emphasis role="strong">csync2</emphasis> at <link
                    xlink:href="http://oss.linbit.com/csync2/"/>.</para>

            <para>If you have used the Geo Clustering Quick start guide to perform the basic
                configuration of the cluster nodes, the DRBD configuration files are already
                included in the list of files to be synchronized.</para>

            <para>To synchronize any changes to the configuration files across both cluster nodes,
                use the following command:</para>

            <para>
                <command>root # csync2 -xv /etc/drbd.d/</command>
            </para>

            <para>If you do not have csync2, or if you do not want to use it, you will need to copy
                the DRBD configuration files manually to the other node.</para>

            <sect3>
                <title>DRBD on Site 1</title>

                <para>To configure DRBD on Site 1, you should be aware of the following
                    details:</para>

                <itemizedlist>
                    <listitem>
                        <para>The resource-name has the site in it, so that the complete
                            configuration can be kept in sync across both clusters without naming
                            conflicts</para>
                    </listitem>
                    <listitem>
                        <para>The nodes' <emphasis role="strong">local</emphasis> per-node IP
                            addresses are used</para>
                    </listitem>
                    <listitem>
                        <para>A <emphasis role="italic">shared-secret</emphasis> is used to avoid
                            inadvertent wrong connections</para>

                        <note>
                            <title>uuid Program</title>
                            <para>The <package>uuid</package> program is an easy way to get unique
                                values.</para>
                        </note>
                    </listitem>
                </itemizedlist>


                <screen>resource nfs-lower-rz1 {
                        disk
                        /dev/volgroup/lv-nfs;
                        meta-disk
                        internal;
                        device
                        /dev/drbd0;
                        protocol
                        C;
                        net {
                        shared-secret
                        "2a9702a6-8747-11e3-9ebb-782bcbd0c11c";
                        }
                        on geo-rz1-a {
                        address
                        }
                        on geo-rz1-b {
                        address
                        }
                        192.168.201.111:7900;
                        192.168.201.112:7900;
                        }
                        }
</screen>

            </sect3>

            <sect3>
                <title>DRBD on Site 2</title>

                <para>Even if Site 2 is nearly identical to Site 1, you should notice the following
                    differences:</para>

                <itemizedlist>
                    <listitem>
                        <para>The resource name has changed</para>
                    </listitem>
                    <listitem>
                        <para>The node names and IP addresses are different</para>
                    </listitem>
                    <listitem>
                        <para>Another <emphasis role="italic">shared-secret</emphasis> has been
                            generated</para>
                    </listitem>
                    <listitem>
                        <para>The volume group and LV name can be kept in the <emphasis
                                role="italic">resource</emphasis> section if identical on both
                            nodes</para>
                    </listitem>
                </itemizedlist>

                <screen>resource nfs-lower-rz2 {
                    disk 		/dev/volgroup/lv-nfs;
                    meta-disk 	internal;
                    device 		/dev/drbd0;
                    protocol 	C;
                    
                    net {
                    shared-secret 	"cd9d857d-72ef-4d10-a1de-6450d1797a2c";
                    }
                    
                    on geo-rz2-a {
                    address 	192.168.202.111:7900;
                    } 
                    on geo-rz2-b {
                    address 	192.168.202.112:7900;
                    } 
}    
</screen>

            </sect3>

            <sect3>
                <title>DRBD Connection Across Sites</title>
                <para>To configure a DRBD conneciton accross sites, you should be aware of the
                    following:</para>

                <itemizedlist>
                    <listitem>
                        <para>The storage disk is the HA-cluster DRBD device, /dev/drbd0 <itemizedlist>
                                <listitem>
                                    <para>You could use /dev/drbd/by- res/nfs-lower-rzN/0, too - but
                                        that would be site-specific, and so had to be moved into the
                                        per-site (stacked-on-top-of nfs-lower-rzN)
                                        configuration</para>
                                </listitem>
                            </itemizedlist>
                        </para>
                    </listitem>
                    <listitem>
                        <para>The DRBD device drbd10 says to use minor number 10</para>
                    </listitem>
                    <listitem>
                        <para>Protocol A, and a higher ping-timeout are needed because of the higher
                            latency</para>
                    </listitem>
                    <listitem>
                        <para>A different shared-secret is used</para>
                    </listitem>
                    <listitem>
                        <para>We do not pass any hostnames, but tell DRBD to stack upon its lower
                            device; that implies that this must be Primary</para>
                    </listitem>
                    <listitem>
                        <para>To allow TCP/IP connections to the other site without knowing which
                            cluster node has the lower DRBD device Primary, we are using a (the)
                            service IP address</para>
                    </listitem>
                </itemizedlist>

                <screen>resource nfs-upper {
                    disk
                    meta-disk
                    device
                    protocol
                    /dev/drbd0;
                    internal;
                    /dev/drbd10;
                    A;
                    net {
                    shared-secret
                    ping-timeout
                    "e0fbd1fe-6b0b-47db-829a-2c4ba638bf1e";
                    20;
                    }
                    stacked-on-top-of nfs-lower-rz1 {
                    address
                    192.168.201.151:7910;
                    }
                    stacked-on-top-of nfs-lower-rz2 {
                    address
                    192.168.202.151:7910;
                    }
                    }</screen>

                <para>Using a DRBD Proxy would involve inserting proxy on ... sections into
                    stacked-on-top-of above, as well as a proxy { ... } section inside of resource.
                    See LINBIT's DRBD Proxy guide at <link
                        xlink:href="https://www.linbit.com/en/resources/technical-publications/102-disaster-recovery/544-disaster-recovery-with-drbd-proxy-
                        configuration-guide"
                    /> for more details regarding configuring DRBD Proxy.</para>

            </sect3>

        </sect2>



    </sect1>


    <?pdfpagebreak style="suse2013" formatter="fop"?>

    <xi:include href="license-gfdl.xml"/>

</article>
