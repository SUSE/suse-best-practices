[#Rancher]

=== Preparation

To have a highly available {rancher} setup, you need a load balancer for your {rancher} nodes.
This section describes how to set up a custom load balancer using `haproxy`. If you already have a load balancer, you can use that to make {rancher} highly available.

If you do not plan to set up a highly available {rancher} cluster, you can skip this section.

==== Installing an `haproxy`-based load balancer

Set up a virtual machine or a bare metal server with {sles} and SUSE Linux Enterprise High Availability or use {sles4sap}. 
Install the `haproxy` package.

[source, bash]
----
$ zypper in haproxy
----

Create the configuration for `haproxy`.
Find an example configuration file for `haproxy` below and adapt for the actual environment.

ifdef::eic[]
[source, bash]
----
# cat <<EOF > /etc/haproxy/haproxy.cfg 
global
        log /dev/log    local0
        log /dev/log    local1 notice
        chroot /var/lib/haproxy
        # stats socket /run/haproxy/admin.sock mode 660 level admin
        stats timeout 30s
        user haproxy
        group haproxy
        daemon

        # general hardlimit for the process of connections to handle, this is separate to backend/listen
        # Added in 'global' AND 'defaults'!!! - global affects only system limits (ulimit/maxsock) and defaults affects only listen/backend-limits - hez
        maxconn 400000

        # Default SSL material locations
        ca-base /etc/ssl/certs
        crt-base /etc/ssl/private

        tune.ssl.default-dh-param 2048

        # Default ciphers to use on SSL-enabled listening sockets.
        # For more information, see ciphers(1SSL). This list is from:
        #  https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/
        ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:                            !DSS
        ssl-default-bind-options ssl-min-ver TLSv1.2 no-tls-tickets

defaults
        mode tcp
        log     global
        option  tcplog
        option  redispatch
        option  tcpka
        option  dontlognull
        retries 2
        timeout connect 5s
        timeout client  5s
        timeout server  5s
        timeout tunnel  86400s
        maxconn 400000

listen stats
        bind *:9000
        mode http
        stats hide-version
        stats uri /stats

listen rancher_apiserver
        bind my_lb_address:6443
        option httpchk GET /healthz
        http-check expect status 401
        server mynode1 mynode1.domain.local:6443 check check-ssl verify none
        server mynode2 mynode2.domain.local:6443 check check-ssl verify none
        server mynode3 mynode3.domain.local:6443 check check-ssl verify none
listen rancher_register
        bind my_lb_address:9345
        option httpchk GET /ping
        http-check expect status 200
        server mynode1 mynode1.domain.local:9345 check check-ssl verify none
        server mynode2 mynode2.domain.local:9345 check check-ssl verify none
        server mynode3 mynode3.domain.local:9345 check check-ssl verify none

listen rancher_ingress80
        bind my_lb_address:80
        option httpchk GET /
        http-check expect status 404
        server mynode1 mynode1.domain.local:80 check
        server mynode2 mynode2.domain.local:80 check
        server mynode3 mynode3.domain.local:80 check

listen rancher_ingress443
        bind my_lb_address:443
        option httpchk GET /
        http-check expect status 404
        server mynode1 mynode1.domain.local:443 check check-ssl verify none
        server mynode2 mynode2.domain.local:443 check check-ssl verify none
        server mynode3 mynode3.domain.local:443 check check-ssl verify none
EOF 
----
endif::[]

ifndef::eic[]
[source, bash]
----
# cat <<EOF > /etc/haproxy/haproxy.cfg 
global
  log /dev/log daemon
  maxconn 32768
  chroot /var/lib/haproxy
  user haproxy
  group haproxy
  daemon
  tune.bufsize 32768
  tune.ssl.default-dh-param 2048
  ssl-default-bind-ciphers ALL:!aNULL:!eNULL:!EXPORT:!DES:!3DES:!MD5:!PSK:!RC4:!ADH:!LOW@STRENGTH

defaults
  log     global
  mode    tcp
  option  log-health-checks
  option  log-separate-errors
  option  dontlog-normal
  option  dontlognull
  option  tcplog
  retries 3
  option  redispatch
  maxconn 10000
  timeout connect     5s
  timeout client     50s
  timeout server    450s

listen stats
  bind 0.0.0.0:80
  bind :::80 v6only
  stats enable
  stats uri     /
  stats refresh 5s

# access the kubernetes api
frontend kubeapi
  bind *:6443
  mode tcp
  default_backend kubeapibackend

# address to register new nodes
frontend rke2server
  bind *:9345
  mode tcp
  default_backend rke2serverbackend

backend kubeapibackend
  balance roundrobin
  server mynode1 192.168.122.20:6443 check
  server mynode2 192.168.122.30:6443 check
  server mynode3 192.168.122.40:6443 check


backend rke2serverbackend
  balance roundrobin
  server mynode1 192.168.122.20:9345 check
EOF 
----
endif::[]
Check the configuration file:
[source, bash]
----
$ haproxy -f /path/to/your/haproxy.conf -c
----

Enable and start the `haproxy` load balancer:
----
$ systemctl enable haproxy
$ systemctl start haproxy
----

Do not forget to restart or reload `haproxy` if any changes are made to the haproxy configuration file.

==== Installing RKE2

To install RKE2, the script provided at https://get.rke2.io can be used as follows:
[source, bash]
----
$ curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=v1.28.13+rke2r1 sh
----

For HA setups, it is necessary to create RKE2 cluster configuration files in advance.
On the first master node:
[source, bash]
----
$ mkdir -p /etc/rancher/rke2
$ cat <<EOF > /etc/rancher/rke2/config.yaml
token: 'your cluster token'
system-default-registry: registry.rancher.com
tls-san:
  - FQDN of fixed registration address on load balancer
  - other hostname
  - IP v4 address
EOF
----

++++
<?pdfpagebreak?>
++++

Create configuration files for additional cluster nodes:
[source, bash]
----
$ cat <<EOF > /etc/rancher/rke2/config.yaml
server: https://"FQDN of registration address":9345
token: 'your cluster token'
system-default-registry: registry.rancher.com
tls-san:
  - FQDN of fixed registration address on load balancer
  - other hostname
  - IP v4 address
EOF
----

IMPORTANT: You also need take about ETCD Snapshots and to perfom backups of your Rancher instance. This is not part of this Document and you can find more information in our Documentation.

IMPORTANT: For security reasons, we generally recommend activating the CIS profile when installing RKE2. This is currently still being validated and will be included in the documentation at a later date. 

Now enable and start the RKE2 components and run the following command on each cluster node:
----
$ systemctl enable rke2-server --now
----

To verify the installation, run the following command:

[source, bash]
----
$ /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes
----

For convenience, the `kubectl` binary can be added to the *$PATH* and the given `kubeconfig` can be set via an environment variable:

[source, bash]
----
$ export PATH=$PATH:/var/lib/rancher/rke2/bin/
$ export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
----

++++
<?pdfpagebreak?>
++++


==== Installing Helm

To install {rancher} and some of its required components, you need to use Helm.

One way to install Helm is to run:
[source, bash]
----
$ curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
----

==== Installing cert-manager

To install the `cert-manager` package, do the following:
----
$ kubectl create namespace cert-manager
----

[#rancherIPS]
How to create the *imagePullSecret* is described in the xref:SAP-EIC-ImagePullSecrets.adoc#imagePullSecret[].


===== Installing the application

ifdef::eic[]
[#rancherLIR]
Before you can install the application, you need to login into the registry. You can find the instruction in xref:SAP-EIC-LoginRegistryApplicationCollection.adoc#LoginApplicationCollection[].
endif::[]

ifndef::eic[]
You will need to login to the {rac}:

[source, bash]
----
$ helm registry login dp.apps.rancher.io/charts -u <yourUser> -p <your-token>
----
endif::[]

[source, bash]
----
$ helm install cert-manager oci://dp.apps.rancher.io/charts/cert-manager \
--set crds.enabled=true \
--set-json 'global.imagePullSecrets=[{"name":"application-collection"}]' \
--namespace=cert-manager \
--version 1.15.2
----

=== Installing {rancher}

To install {rancher}, you need to add the related Helm repository.
To achieve that, use the following command:

[source, bash]
----
$ helm repo add rancher-prime https://charts.rancher.com/server-charts/prime
----

Next, create the `cattle-system` namespace in Kubernetes as follows:
----
$ kubectl create namespace cattle-system
----

The Kubernetes cluster is now ready for the installation of {rancher}:

[source, bash]
----
$ helm install rancher rancher-prime/rancher \
    --namespace cattle-system \
    --set hostname=<your.domain.com> \
    --set replicas=3
----

During the rollout of {rancher}, you can monitor the progress using the following command:

[source, bash]
----
$ kubectl -n cattle-system rollout status deploy/rancher-prime
----

When the deployment is done, you can access the {rancher} cluster at https://<your.domain.com>[]. 
Here you will also find a description about how to log in for the first time.
