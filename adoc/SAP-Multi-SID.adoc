:cluhost1: freki
:cluhost2: geri
:cluhost3: sleipnir

:sid1: YAS
:sid1-lc: yas
:inst-ascs1: 00
:inst-ers1: 10
:vip-asc1: 172.17.1.07
:vip-ers1: 172.17.1.08
:v-ascs1host: sapascs1
:v-ers1host: sapers1
:ascs1nfsexp: 172.17.0.1:/srv/install/sapascs1
:ers1nfsexp: 172.17.0.1:/srv/install/sapers1

:sid2: VAS
:sid2-lc: vas
:inst-ascs2: 20
:inst-ers2: 30
:vip-asc2: 172.17.1.17
:vip-ers2: 172.17.1.18
:v-ascs2host: sapascs2
:v-ers2host: sapers2
:ascs2nfsexp: 172.17.0.1:/srv/install/sapascs2
:ers2nfsexp: 172.17.0.1:/srv/install/sapers2

:sid3: WAS
:sid3-lc: was
:inst-ascs3: 31
:inst-ers3: 41
:vip-asc3: 172.17.1.27
:vip-ers3: 172.17.1.28
:v-ascs3host: sapascs3
:v-ers3host: sapers3
:ascs3nfsexp: 172.17.0.1:/srv/install/sapascs3
:ers3nfsexp: 172.17.0.1:/srv/install/sapers3

:sid4: XAS
:sid4-lc: xas
:inst-ascs4: 42
:inst-ers4: 52
:vip-asc4: 172.17.1.37
:vip-ers4: 172.17.1.38
:v-ascs4host: sapascs4
:v-ers4host: sapers4
:ascs4nfsexp: 172.17.0.1:/srv/install/sapascs4
:ers4nfsexp: 172.17.0.1:/srv/install/sapers4

:sid5: ZAS
:sid5-lc: zas
:inst-ascs5: 53
:inst-ers5: 63
:vip-asc5: 172.17.1.39
:vip-ers5: 172.17.1.40
:v-ascs5host: sapascs5
:v-ers5host: sapers5
:ascs5nfsexp: 172.17.0.1:/srv/install/sapascs5
:ers5nfsexp: 172.17.0.1:/srv/install/sapers5


:sap: SAP
:sapReg: SAP*
:sapBS: {sap} Business Suite
:sapBSReg: {sapReg} Business Suite
:sapNW: {sap} NetWeaver
:sapS4: {sap} S/4HANA
:sapS41809: {sap} S/4HANA 1809
:sapS4in: {sap} S/4HANA Server 1809
:sapS4pl: {sap} S/4HANA ABAP Platform
:sapCert: {sap} S/4-HA-CLU 1.0
:sapERS: {sap} Enqueue Replication Server 2
:sapHana: {sap} HANA
:s4Hana: {sap} S/4HANA
:sapStartSrv: sapstartsrv
:sapCtrl: sapcontrol
:sapHostAgent: saphostagent

:linux: Linux


:suse: SUSE
:SUSEReg: SUSE(R)
:sleAbbr: SLE
:sle: SUSE Linux Enterprise
:sleReg: {SUSEReg} Linux Enterprise
:slesAbbr: SLES
:sles: {sle} Server
:slesReg: {sleReg} Server
:sles4sapAbbr: {slesAbbr} for {SAP}
:sles4sap: {sles} for {SAP} Applications
:sles4sapReg: {slesReg} for {SAP} Applications
:sleHA: {sle} High Availability
:s4sClConnector: sap_suse_cluster_connector
:s4sClConnector3: sap-suse-cluster-connector
:sapHanaSR: {sap}HanaSR

:sap: SAP
:sapreg: SAP*
:saphana: SAP HANA
:hana: SAP HANA
:saphanasr: SAPHanaSR
:saphanara: SAPHana
:saphanatopra: SAPHanaTopology
:b1: SAP BusinessOne
:instmaster: Installation Master
:instmedia: Installation Media
:mediaset: Media Set
:supmedia: Supplementary Media
:sapin: SAP Installer
:sapina: SAPinst
:thirdmedia: Third-Party Media
:pn12: 12
:pn11: 11
:psp12: SP1
:psp11: SP4
:file1: hanasr_appendix_docupdates.xml
:sapnote12: 1984787
:sapnote11: 1310037
:usecase: performance optimized
:deploymentGuide12: https://documentation.suse.com/sles/12-SP4/html/SLES-all/book-sle-deployment.html
:autoYastGuide12: https://documentation.suse.com/sles/12-SP4/html/SLES-all/book-autoyast.html
:haAdminGuide12: https://documentation.suse.com/sle-ha/12-SP4/html/SLE-HA-all/book-sleha.html
:lauchPadNotes: https://launchpad.support.sap.com/#/notes/
:reslibrary: https://documentation.suse.com/sbp/all/
:refsidadm: <sidadm>
:refHost1: <host1>
:refInst: <instanceNumber>
:refSidLc: <sid>
:docScaleOut: SAP HANA System Replication Scale-Out - Performance Optimized Scenario
:docCostOpt: Setting up an SAP HANA SR Cost Optimized Infrastructure
:docPerfOpt: SAP HANA System Replication Scale-Up - Performance Optimized Scenario
:sapHanaSrMinVers: 0.152


:docinfo:

:localdate:


= {sapS4} and {sapNW} Multi-SID Cluster Guide

== About This Guide

=== Introduction

{sles4sapReg} is the optimal platform to
run {sapReg} applications in a high availability environment. Together with a redundant layout
of the technical infrastructure, single points of failure can be eliminated.

{sapBSReg} is a sophisticated application platform for large enterprises
and mid-size companies. Many critical business environments require the highest
possible {sapReg} application availability.

The described cluster solution can be used for {sapReg} {sapS4pl}.

{sapS4pl} is a common stack of middleware functionality used to support {sap}
business applications. The {sapERS} constitutes application
level redundancy for one of the most crucial components of the {sapS4pl} stack,
the enqueue service. An optimal effect of the enqueue replication mechanism can
be achieved when combining the application level redundancy with a high
availability cluster solution, as provided for example by {sles4sap}. During several
years of productive operations, these components mentioned above have proven their maturity
for customers of different sizes and industries.


[id="sec.resource"]

=== Additional Documentation and Resources

Several chapters in this document contain links to additional documentation resources that
are either available on the system or on the Internet.

For the latest product documentation updates, see https://documentation.suse.com/.

More guides and best practices documents referring to SUSE Linux Enterprise Server and {sap} can be
found and downloaded at the SUSE Best Practices Web page:

https://documentation.suse.com/sbp/all

Here you get access to guides for {SAPHANA} system replication
automation and High Availability (HA) scenarios for {SAPNw} and {s4hana}.

Additional resources, such as customer references, brochures or flyer, can be found at the SUSE Linux
Enterprise Server for {sap} Applications resource library:

https://www.suse.com/products/sles-for-sap/#resources .

=== Feedback
include::common_intro_feedback.adoc[]

== Scope of This Document

The document at hand explains how to integrate the central services with multi-SID setups in one cluster.
Other architectures such as the {saphana} performance optimized scenario will follow.

=== Architecture Central Services
==== Integration of {sapS4} and {sapNW} into the Cluster Using the Cluster Connector

The integration of the HA cluster through the SAP control framework using the
*{s4sClConnector}* is of special interest. The service *{SAPSTARTSRV}* controls {sap} instances since
{sap} Kernel versions 6.40. One of the classic problems running
{sap} instances in a highly available environment is the following: If an {sap}
administrator changes the status (start/stop) of an {sap} instance without using
the interfaces provided by the cluster software, the cluster framework will
detect that as an error status. In consequence, it will bring the {sap} instance into the old
status by either starting or stopping the {sap} instance. This can result in
very dangerous situations, if the cluster changes the status of an {sap} instance
during some {sap} maintenance tasks. The new updated solution enables the central component
*{SAPSTARTSRV}* to report state changes to the cluster software. This avoids dangerous situations
as previously described.
More details can be found in the blog article "Using sap_vendor_cluster_connector for interaction between cluster
framework and sapstartsrv" at https://blogs.sap.com/2014/05/08/using-sapvendorclusterconnector-for-interaction-between-cluster-framework-and-sapstartsrv/comment-page-1/.

NOTE: If you update from an {sapS4Pl} version less than 1809, read SAP Note 2641019 carefully to adapt your cluster.

.Cluster Connector to Integrate the Cluster with the {sap} Start Framework
image::sles4sap_clusterconnector.svg[SVG]

NOTE: For this scenario, an updated version of the *{s4sClConnector3}* is used.
It implements the API version 3 for the communication between the cluster
framework and the *{sapstartsrv}* service.

The new version of the *{s4sClConnector3}* allows starting, stopping and migrating
an {sap} instance. The integration between the cluster software and the
*{sapstartsrv}* also implements the option to run checks of the HA setup using either the
command line tool *sapcontrol* or the {SAP} management consoles ({SAP} MMC or
{sap} MC). Since version 3.1.0 and later the maintenance mode of cluster resources 
triggered with SAP _sapcontrol_ commands is supported.

=== Architecture DB Services

{SUSE} has developed multiple cluster architectures for {sap} databases running with
high availability. Not all of those architectures are planned for multi-SID configurations.
The following sections describe common {hana} scenarios. You will also find a note if the
architecture is available for multi-SID.

==== Scale-Up Scenarios and Resource Agents

{SUSE} has implemented the scale-up scenario with the `{SAPHanaRA}` resource
agent (RA), which performs the actual check of the {HANA} database
instances. This RA is configured as a master/slave resource. In the
scale-up scenario, the master assumes responsibility for the {hana}
databases running in primary mode. The slave is responsible for
instances that are operated in synchronous (secondary) status.

To make configuring the cluster as simple as possible, SUSE developed the 
`{SAPHanaTopRA}` resource agent. This RA runs on all nodes
of a {sles4sap} cluster and gathers information about the
statuses and configurations of {hana} system replications. It is
designed as a normal (stateless) clone.

{hana} System replication for Scale-Up is supported in the following
scenarios or use cases:

* *Performance optimized* (_A => B_). This scenario and setup *will be
described in this document in a future update*.
+
.{hana} System Replication Scale-Up in the Cluster - Performance Optimized
image::SAPHanaSR-ScaleUP-perfOpt.svg[scaledwidth=100.0%]
+
In the performance optimized scenario an {hana} RDBMS site A is synchronizing
with an {HANA} RDBMS site B on a second node. As the HANA RDBMS on the second node
is configured to pre-load the tables, the takeover time is typically very
short.
+
One big advantage of the performance optimized scenario of {hana} is the
possibility to allow read access on the secondary database site. To support
this *read enabled* scenario, a second virtual IP address is added to the cluster
and bound to the secondary role of the system replication.
+
NOTE: The performance optimized scenario could be implemented also for multi-SID setups,
if it is guaranteed that all system replication pairs are handled by the cluster separately.
It is not allowed to force an {hana} system to process a takeover only because another one is taking over.
There is also no load balance for the primary sides.

* *Cost optimized* (_A => B, Q_). This scenario and setup is described
in another document available from the documentation Web page ({reslibrary}). The document for _cost optimized_ is named
_"{docCostOpt}"_.
+
.{hana} System Replication Scale-Up in the Cluster - Cost Optimized
image::SAPHanaSR-ScaleUP-costOpt2.svg[scaledwidth=100.0%]
+
In the cost optimized scenario the second node is also used for a
non-productive {hana} RDBMS system (like QAS or TST). Whenever a takeover
is needed, the non-productive system must be stopped first. As the
productive secondary system on this node must be limited in using system
resources, the table preload must be switched off. A possible
takeover needs longer than in the performance optimized use case.
+
In the cost optimized scenario the secondary needs to be running in a reduced
memory consumption configuration. This is why _read enabled_ must not be used in this
scenario.
+
NOTE: *The cost optimized scenario is not intended to be used for multi-SID configuration.*
The reason is that, whenever one of the primary systems would be moved to the secondary node,
a second memory setup for the primary systems would be needed. Otherwise the 
removed memory limitation would tell the {hana} to consume all available memory. Currently there
is no globally defined space where to set such parameters to be used after a takeover.

* *Multi-Tier* (_A => B -> C_) and *Multi-Target* (_B <= A => C_).
+
.{hana} System Replication Scale-Up in the Cluster - Performance Optimized Chain
image::SAPHanaSR-ScaleUP-Chain.svg[scaledwidth=100.0%]
+
A _Multi-tier_ system replication has an additional target. In the past this third
side must have been connected to the secondary (chain topology). With current {saphana}
versions also _multiple target topology_ is allowed by {sap}.
+
.{hana} System Replication Scale-Up in the Cluster - Performance Optimized Multi-Target
image::SAPHanaSR-ScaleUP-MultiTarget.svg[scaledwidth=100.0%]
+
Multi-tier and multi-target systems are implemented as described in this document.
Only the first replication pair (A and B) is handled by the cluster itself.
The main difference to the plain performance optimized scenario is that the auto
registration must be switched off.
+
NOTE: From a cluster perspective, this type of scenario is very similar to the
performance optimized scenario. As long as only A and B are driven by the cluster
this scenario is also expected to work with multi-SID setups. However, this
has not been tested yet in the labs. Support for this scenario is expected to be added in the
future.

* *Multi-tenancy* or MDC.
+
Multi-tenancy is supported for all of the above scenarios and use cases. This
scenario is supported since {hana} SPS09. The setup and configuration
from a cluster point of view is the same for multi-tenancy and single
container. Thus you can use the above documents for both kinds of
scenarios.
+
NOTE: Multi-tenancy has no direct side effect to multi-SID setups. This means that multi-tenancy is
supported in combination with multi-SID, if the basic architecture is supported with multi-SID.

== Multi-SID - Central Services

This guide will cover the following scenarios:

* Example ENSA1 in a Two-Node Cluster
* Example ENSA2 in a Two-Node Cluster
* Example ENSA1 in a Multi-Node Cluster
* Example ENSA2 in a Multi-Node Cluster
* Example ENSA1 and ENSA2 in a Two-Node Cluster
* Example ENSA1 and ENSA2 in a Multi-Node Cluster

=== Two-Node Setup
- The hardware configuration of each node must be able to run all resources in case of a fail over
- Each {sap} system (SID) must be independent from each other

.Multiple {sap} Central Services Instances Running in a Two-Node Cluster
image::multi-sid-cluster-2n.svg[scaledwidth="100%"]

=== Multi-Node Setup
- The hardware configuration of each node must be able to run additional resources in case of a fail over
- Each {sap} system (SID) must be independent from each other
- ENSA2 load balancing (under observation)

.Multiple {sap} Central Services Instances Running in a Multi-Node Cluster
image::multi-sid-cluster.svg[scaledwidth="100%"]

=== General Rules and Requirements

WARNING: When adding more resources, be aware of the following risks:

* Higher complexity 
* Higher impact if one nodes goes down
* Higher administrative effort in case of maintenance activities, like OS patching

==== Installation
The installation process is described in the *SUSE Best Practices* guides for {sapS4} and {sapNW}. They can be found here:
https://documentation.suse.com/sbp/all

Repeat the steps for preparing the hosts and infrastructure:

* Provide the storage and IP addresses as needed.
* Check the name resolution and time settings on each host.
* Provide the {sap} installation sources as you will need.
* Create the directory structure similar to the first {sap} instance for all new installations.
* Manually mount the file systems and assign the IP address for the virtual host name.
* Start {sap} installation with virtual host name using _swpm_.

The installation process itself is similar to the first setup. The major difference is the ERS
installation during the profile selection. At this point you need to choose the right directory to
point to the correct ASCS installation.

==== Limitations
We recommend  to limit the numbers of {sap} systems (SID) running in one cluster. Each new SID will increase the complexity.
The tested setup was done with up to *five* SID in one cluster.

=== Example ENSA1 in a Two-Node Cluster
//[[2nensa1]]
As known from the setup based on the *SUSE Best Practices* document for {sapNW}
(https://documentation.suse.com/sbp/all -> {sapNW} High Availability Cluster 7.40 - Setup Guide),
the cluster integration can be done with a configuration file. Create one configuration file for each SID
and load them one by one into the cluster. The description below will help to do the steps in the right direction.

The cluster is already up and running and still runs one ASCS / ERS configuration based on the group concept.
The following tasks must be executed:

* Stop the new installed {sap} system, which will be added to the cluster.
* Unmount the file system with _umount_ and remove the IP address from the new {sap} system.
* Check and/or modify the ASCS and ERS profile files and add the section for the {s4sClConnector3} integration, if not done already. 
** for example for {sid2}: __/sapmnt/{sid2}/profile/{sid2}_ASCS{inst-ascs2}_{v-ascs2host}__
** for example for {sid2}: __/sapmnt/{sid2}/profile/{sid2}_ERS{inst-ers2}_{v-ers2host}__
* Modify the _haclient_ group on each cluster node and add the new <sid>adm to it, if not done already.
* Set the cluster into maintenance mode.
* Load the new configuration with _crm configure load update <filename>_.
* Release the cluster from maintenance mode.
* Check the cluster and {sap} instances with _crm_mon -1rfn_ and _sapcontrol -nr ..._.

.Configuration file and adapting cluster
====

_As user_ *root* prepare a file for the {sap} system {sid2}
//[subs="specialchars,attributes,verbatim,quotes"]
//======

[subs="attributes"]
----
# cat ensa1-2nd-{sid2-lc}.txt

primitive rsc_fs_{sid2}_ASCS{inst-ascs2} Filesystem \
	params device="{ascs2nfsexp}" directory="/usr/sap/{sid2}/ASCS{inst-ascs2}" fstype=nfs options="vers=3" \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=300s
primitive rsc_fs_{sid2}_ERS{inst-ers2} Filesystem \
	params device="{ers2nfsexp}" directory="/usr/sap/{sid2}/ERS{inst-ers2}" fstype=nfs options="vers=3" \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=300s
primitive rsc_ip_{sid2}_ASCS{inst-ascs2} IPaddr2 \
	params ip={vip-ascs2} \
	op monitor interval=10s timeout=20s
primitive rsc_ip_{sid2}_ERS{inst-ers2} IPaddr2 \
	params ip={vip-ers2} \
	op monitor interval=10s timeout=20s
primitive rsc_sap_{sid2}_ASCS{inst-ascs2} SAPInstance \
	operations $id=rsc_sap_{sid2}_ASCS{inst-ascs2}-operations \
	op monitor interval=11 timeout=60 on-fail=restart \
	params InstanceName={sid2}_ASCS{inst-ascs2}_{v-ascs2host} START_PROFILE="/sapmnt/{sid2}/profile/{sid2}_ASCS{inst-ascs2}_{v-ascs2host}" AUTOMATIC_RECOVER=false \
	meta resource-stickiness=5000 failure-timeout=60 \
        migration-threshold=1 priority=10
primitive rsc_sap_{sid2}_ERS{inst-ers2} SAPInstance \
	operations $id=rsc_sap_{sid2}_ERS{inst-ers2}-operations \
	op monitor interval=11 timeout=60 on-fail=restart \
	params InstanceName={sid2}_ERS{inst-ers2}_{v-ers2host} START_PROFILE="/sapmnt/{sid2}/profile/{sid2}_ERS{inst-ers2}_{v-ers2host}" AUTOMATIC_RECOVER=false IS_ERS=true \
	meta priority=1000
group grp_{sid2}_ASCS{inst-ascs2} rsc_ip_{sid2}_ASCS{inst-ascs2} rsc_fs_{sid2}_ASCS{inst-ascs2} rsc_sap_{sid2}_ASCS{inst-ascs2} \
	meta resource-stickiness=3000
group grp_{sid2}_ERS{inst-ers2} rsc_ip_{sid2}_ERS{inst-ers2} rsc_fs_{sid2}_ERS{inst-ers2} rsc_sap_{sid2}_ERS{inst-ers2}
colocation col_sap_{sid2}_no_both -5000: grp_{sid2}_ERS{inst-ers2} grp_{sid2}_ASCS{inst-ascs2}
location loc_sap_{sid2}_failover_to_ers rsc_sap_{sid2}_ASCS{inst-ascs2} \
         rule 2000: runs_ers_{sid2} eq 1
order ord_sap_{sid2}_first_start_ascs Optional: rsc_sap_{sid2}_ASCS{inst-ascs2}:start rsc_sap_{sid2}_ERS{inst-ers2}:stop symmetrical=false
----
//======
====

.Configuration Example for a 3rd SID, for example {sid3}
====
_As user_ *root* prepare a file for the SAP system {sid3}
//[subs="specialchars,attributes,verbatim,quotes"]
//======

[subs="attributes"]
----
# cat ensa1-3nd-{sid3-lc}.txt

primitive rsc_fs_{sid3}_ASCS{inst-ascs3} Filesystem \
	params device="{ascs3nfsexp}" directory="/usr/sap/{sid3}/ASCS{inst-ascs3}" fstype=nfs options="vers=3" \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=300s
primitive rsc_fs_{sid3}_ERS{inst-ers3} Filesystem \
	params device="{ers3nfsexp}" directory="/usr/sap/{sid3}/ERS{inst-ers3}" fstype=nfs options="vers=3" \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=300s
primitive rsc_ip_{sid3}_ASCS{inst-ascs3} IPaddr2 \
	params ip={vip-ascs3} \
	op monitor interval=10s timeout=20s
primitive rsc_ip_{sid3}_ERS{inst-ers3} IPaddr2 \
	params ip={vip-ers3} \
	op monitor interval=10s timeout=20s
primitive rsc_sap_{sid3}_ASCS{inst-ascs3} SAPInstance \
	operations $id=rsc_sap_{sid3}_ASCS{inst-ascs3}-operations \
	op monitor interval=11 timeout=60 on-fail=restart \
	params InstanceName={sid3}_ASCS{inst-ascs3}_{v-ascs3host} START_PROFILE="/sapmnt/{sid3}/profile/{sid3}_ASCS{inst-ascs3}_{v-ascs3host}" AUTOMATIC_RECOVER=false \
	meta resource-stickiness=5000 failure-timeout=60 \
        migration-threshold=1 priority=10
primitive rsc_sap_{sid3}_ERS{inst-ers3} SAPInstance \
	operations $id=rsc_sap_{sid3}_ERS{inst-ers3}-operations \
	op monitor interval=11 timeout=60 on-fail=restart \
	params InstanceName={sid3}_ERS{inst-ers3}_{v-ers3host} START_PROFILE="/sapmnt/{sid3}/profile/{sid3}_ERS{inst-ers3}_{v-ers3host}" AUTOMATIC_RECOVER=false IS_ERS=true \
	meta priority=1000
group grp_{sid3}_ASCS{inst-ascs3} rsc_ip_{sid3}_ASCS{inst-ascs3} rsc_fs_{sid3}_ASCS{inst-ascs3} rsc_sap_{sid3}_ASCS{inst-ascs3} \
	meta resource-stickiness=3000
group grp_{sid3}_ERS{inst-ers3} rsc_ip_{sid3}_ERS{inst-ers3} rsc_fs_{sid3}_ERS{inst-ers3} rsc_sap_{sid3}_ERS{inst-ers3}
colocation col_sap_{sid3}_no_both -5000: grp_{sid3}_ERS{inst-ers3} grp_{sid3}_ASCS{inst-ascs3}
location loc_sap_{sid3}_failover_to_ers rsc_sap_{sid3}_ASCS{inst-ascs3} \
         rule 2000: runs_ers_{sid3} eq 1
order ord_sap_{sid3}_first_start_ascs Optional: rsc_sap_{sid3}_ASCS{inst-ascs3}:start rsc_sap_{sid3}_ERS{inst-ers3}:stop symmetrical=false
----
//======
====

.Configuration Example for a 4th SID and following, for example {sid4}
====
This is similar to the SID examples before. You must adapt the:

* File system sources which will be mounted
* IP address for virtual host name of ASCS and ERS
* Instance number for ASCS and ERS
* Local mount point for ASCS and ERS
* Profile path for ASCS and ERS

====

After the configuration has been prepared in a file, this can be loaded into the cluster. 

.Add the 2nd and further SID into the cluster
====
The cluster is already up and running and still runs one ASCS / ERS configuration based on the group concept.
The following procedure is recommended:

* Stop the new installed {sap} system, which will be added into the cluster.
* Unmount the file system with _umount_ and remove the IP address from the new {sap} system.
* Check and/or modify the ASCS and ERS profile files and add the section for the {s4sClConnector3} integration, if not done already.
** for example for {sid2}: __/sapmnt/{sid2}/profile/{sid2}_ASCS{inst-ascs2}_{v-ascs2host}__
** for example for {sid2}: __/sapmnt/{sid2}/profile/{sid2}_ERS{inst-ers2}_{v-ers2host}__
* Modify the _haclient_ group on each cluster node and add the new <sid>adm to it, if not done already.
* Set the cluster into maintenance mode.
* Load the new configuration with _crm configure load update <filename>_.
* Pre-check the cluster with _crm status_.
* Release the cluster from maintenance mode.
* Check the cluster and {sap} instances with _crm_mon_ and _sapcontrol -nr ..._.

Log in to one of the cluster nodes and run the commands from there.

_As user_ *root*, set the cluster to maintenance and load the configuration into the cluster
with _crm configure load update <filename>_.
//[subs="specialchars,attributes,verbatim,quotes"]
//======
[subs="attributes"]
----
# crm configure property maintenance-mode=true
# crm configure load update ensa1-2nd-{sid2-lc}.txt
# crm configure load update ensa1-3nd-{sid3-lc}.txt
# crm configure load update ensa1-4nd-{sid4-lc}.txt
...
# crm status
----
//======

You should get back the prompt only. No further messages should be shown. If a message is displayed, there might be something wrong in the configuration file.

NOTE: If a wrong path is used, for example for the profile, this will not be detected during the configuration load. However, it will be shown during the cluster start action.

_As user_ *root* verify the new inactive loaded configuration
//[subs="specialchars,attributes,verbatim,quotes"]

[subs="attributes"]
//======
----
# crm status
{cluhost1}:~ # crm status
Stack: corosync
Current DC: {cluhost1} (version 1.1.18+20180430.b12c320f5-3.3.1-b12c320f5) - partition with quorum
Last updated: Tue Oct 15 16:57:07 2019
Last change: Tue Oct 15 16:56:56 2019 by hacluster via crmd on {cluhost1}

2 nodes configured
31 resources configured

*** Resource management is DISABLED ***
The cluster will not attempt to start, stop or recover services

Online: [ {cluhost1} {cluhost2} ]

Full list of resources:

Resource Group: grp_{sid1}_ASCS{inst-ascs1}
	rsc_ip_{sid1}_ASCS{inst-ascs1}	(ocf::heartbeat:IPaddr2):	Started {cluhost1}
	rsc_fs_{sid1}_ASCS{inst-ascs1}	(ocf::heartbeat:Filesystem):	Started {cluhost1}
	rsc_sap_{sid1}_ASCS{inst-ascs1}	(ocf::heartbeat:SAPInstance):	Started {cluhost1}
Resource Group: grp_{sid1}_ERS{inst-ers1}
	rsc_ip_{sid1}_ERS{inst-ers1}	(ocf::heartbeat:IPaddr2):	Started {cluhost2}
	rsc_fs_{sid1}_ERS{inst-ers1}	(ocf::heartbeat:Filesystem):	Started {cluhost2}
	rsc_sap_{sid1}_ERS{inst-ers1}	(ocf::heartbeat:SAPInstance):	Started {cluhost2}
	rsc_ip_hawk	(ocf::heartbeat:IPaddr2):	Started {cluhost1}
Resource Group: grp_{sid4}_ASCS{inst-ascs4}
	rsc_ip_{sid4}_ASCS{inst-ascs4}	(ocf::heartbeat:IPaddr2):	Stopped (unmanaged)
	rsc_fs_{sid4}_ASCS{inst-ascs4}	(ocf::heartbeat:Filesystem):	Stopped (unmanaged)
	rsc_sap_{sid4}_ASCS{inst-ascs4}	(ocf::heartbeat:SAPInstance):	Stopped (unmanaged)
Resource Group: grp_{sid4}_ERS{inst-ers4}
	rsc_ip_{sid4}_ERS{inst-ers4}	(ocf::heartbeat:IPaddr2):	Stopped (unmanaged)
	rsc_fs_{sid4}_ERS{inst-ers4}	(ocf::heartbeat:Filesystem):	Stopped (unmanaged)
	rsc_sap_{sid4}_ERS{inst-ers4}	(ocf::heartbeat:SAPInstance):	Stopped (unmanaged)
Resource Group: grp_{sid3}_ASCS{inst-ascs3}
	rsc_ip_{sid3}_ASCS{inst-ascs3}	(ocf::heartbeat:IPaddr2):	Stopped (unmanaged)
	rsc_fs_{sid3}_ASCS{inst-ascs3}	(ocf::heartbeat:Filesystem):	Stopped (unmanaged)
	rsc_sap_{sid3}_ASCS{inst-ascs3}	(ocf::heartbeat:SAPInstance):	Stopped (unmanaged)
Resource Group: grp_{sid3}_ERS{inst-ers3}
	rsc_ip_{sid3}_ERS{inst-ers3}	(ocf::heartbeat:IPaddr2):	Stopped (unmanaged)
	rsc_fs_{sid3}_ERS{inst-ers3}	(ocf::heartbeat:Filesystem):	Stopped (unmanaged)
	rsc_sap_{sid3}_ERS{inst-ers3}	(ocf::heartbeat:SAPInstance):	Stopped (unmanaged)
Resource Group: grp_{sid5}_ASCS{inst-ascs5}
	rsc_ip_{sid5}_ASCS{inst-ascs5}	(ocf::heartbeat:IPaddr2):	Stopped (unmanaged)
	rsc_fs_{sid5}_ASCS{inst-ascs5}	(ocf::heartbeat:Filesystem):	Stopped (unmanaged)
	rsc_sap_{sid5}_ASCS{inst-ascs5}	(ocf::heartbeat:SAPInstance):	Stopped (unmanaged)
Resource Group: grp_{sid5}_ERS{inst-ers5}
	rsc_ip_{sid5}_ERS{inst-ers5}	(ocf::heartbeat:IPaddr2):	Stopped (unmanaged)
	rsc_fs_{sid5}_ERS{inst-ers5}	(ocf::heartbeat:Filesystem):	Stopped (unmanaged)
	rsc_sap_{sid5}_ERS{inst-ers5}	(ocf::heartbeat:SAPInstance):	Stopped (unmanaged)
Resource Group: grp_{sid2}_ASCS{inst-ascs2}
	rsc_ip_{sid2}_ASCS{inst-ascs2}	(ocf::heartbeat:IPaddr2):	Stopped (unmanaged)
	rsc_fs_{sid2}_ASCS{inst-ascs2}	(ocf::heartbeat:Filesystem):	Stopped (unmanaged)
	rsc_sap_{sid2}_ASCS{inst-ascs2}	(ocf::heartbeat:SAPInstance):	Stopped (unmanaged)
Resource Group: grp_{sid2}_ERS{inst-ers2}
	rsc_ip_{sid2}_ERS{inst-ers2}	(ocf::heartbeat:IPaddr2):	Stopped (unmanaged)
	rsc_fs_{sid2}_ERS{inst-ers2}	(ocf::heartbeat:Filesystem):	Stopped (unmanaged)
	rsc_sap_{sid2}_ERS{inst-ers2}	(ocf::heartbeat:SAPInstance):	Stopped (unmanaged)
----
//======

_As user_ *root*, release the cluster from maintenance and check that the new cluster resources became active.
//[subs="specialchars,attributes,verbatim,quotes"]
//======

[subs="attributes"]
----
# crm configure property maintenance-mode=false
# crm_mon -rfn
----
//======

_As user {sid2-lc}adm_, for example on ASCS host, check the {sap} system.
//[subs="specialchars,attributes,verbatim,quotes"]
//======

[subs="attributes"]
----
# su - {sid2-lc}adm
# sapcontrol -nr {inst-ascs2} -function GetSystemInstanceList
# sapcontrol -nr {inst-ascs2} -function GetProcessList -host {v-ascs2host}
# sapcontrol -nr {inst-ers2} -function GetProcessList -host {v-ers2host}
----
//======

NOTE: Repeat these steps for each SID you have installed.
====

.Example output of five running SID in a two-node cluster
====
_As user_ *root*, use _crm_mon_ or _crm status_.
[subs="specialchars,attributes,quotes"]
----
# crm_mon -1rfn

Stack: corosync
Current DC: {cluhost1} (version 1.1.18+20180430.b12c320f5-3.3.1-b12c320f5) - partition with quorum
Last updated: Tue Oct 15 16:26:57 2019
Last change: Tue Oct 15 16:25:55 2019 by root via cibadmin on {cluhost1}

2 nodes configured
31 resources configured

Node {cluhost1}: online
        rsc_ip_{sid3}_ASCS{inst-ascs3}       (ocf::heartbeat:IPaddr2):       Started
        rsc_fs_{sid3}_ASCS{inst-ascs3}       (ocf::heartbeat:Filesystem):    Started
        rsc_sap_{sid3}_ASCS{inst-ascs3}      (ocf::heartbeat:SAPInstance):   Started
        rsc_ip_{sid2}_ERS{inst-ers2}        (ocf::heartbeat:IPaddr2):       Started
        rsc_fs_{sid2}_ERS{inst-ers2}        (ocf::heartbeat:Filesystem):    Started
        rsc_sap_{sid2}_ERS{inst-ers2}       (ocf::heartbeat:SAPInstance):   Started
        rsc_ip_{sid1}_ERS{inst-ers1}        (ocf::heartbeat:IPaddr2):       Started
        rsc_fs_{sid1}_ERS{inst-ers1}        (ocf::heartbeat:Filesystem):    Started
        rsc_sap_{sid1}_ERS{inst-ers1}       (ocf::heartbeat:SAPInstance):   Started
        rsc_ip_{sid5}_ERS{inst-ers5}        (ocf::heartbeat:IPaddr2):       Started
        rsc_fs_{sid5}_ERS{inst-ers5}        (ocf::heartbeat:Filesystem):    Started
        rsc_sap_{sid5}_ERS{inst-ers5}       (ocf::heartbeat:SAPInstance):   Started
        rsc_ip_{sid4}_ERS{inst-ers4}        (ocf::heartbeat:IPaddr2):       Started
        rsc_fs_{sid4}_ERS{inst-ers4}        (ocf::heartbeat:Filesystem):    Started
        rsc_sap_{sid4}_ERS{inst-ers4}       (ocf::heartbeat:SAPInstance):   Started
Node {cluhost2}: online
        rsc_ip_{sid3}_ERS{inst-ers3}        (ocf::heartbeat:IPaddr2):       Started
        rsc_fs_{sid3}_ERS{inst-ers3}        (ocf::heartbeat:Filesystem):    Started
        rsc_sap_{sid3}_ERS{inst-ers3}       (ocf::heartbeat:SAPInstance):   Started
        rsc_ip_{sid1}_ASCS{inst-ascs1}       (ocf::heartbeat:IPaddr2):       Started
        rsc_fs_{sid1}_ASCS{inst-ascs1}       (ocf::heartbeat:Filesystem):    Started
        rsc_sap_{sid1}_ASCS{inst-ascs1}      (ocf::heartbeat:SAPInstance):   Started
        rsc_ip_{sid4}_ASCS{inst-ascs4}       (ocf::heartbeat:IPaddr2):       Started
        rsc_fs_{sid4}_ASCS{inst-ascs4}       (ocf::heartbeat:Filesystem):    Started
        rsc_sap_{sid4}_ASCS{inst-ascs4}      (ocf::heartbeat:SAPInstance):   Started
        rsc_ip_{sid2}_ASCS{inst-ascs2}       (ocf::heartbeat:IPaddr2):       Started
        rsc_fs_{sid2}_ASCS{inst-ascs2}       (ocf::heartbeat:Filesystem):    Started
        rsc_sap_{sid2}_ASCS{inst-ascs2}      (ocf::heartbeat:SAPInstance):   Started
        rsc_ip_{sid5}_ASCS{inst-ascs5}       (ocf::heartbeat:IPaddr2):       Started
        rsc_fs_{sid5}_ASCS{inst-ascs5}       (ocf::heartbeat:Filesystem):    Started
        rsc_sap_{sid5}_ASCS{inst-ascs5}      (ocf::heartbeat:SAPInstance):   Started
        rsc_ip_hawk     (ocf::heartbeat:IPaddr2):       Started

No inactive resources


Migration Summary:
* Node {cluhost1}:
* Node {cluhost2}:

----

Each group consists of three resources, _rsc_ip_, _rsc_fs_ and _rsc_sap_. The resource _rsc_ip_hawk_ is the optional
virtual IP for the HAWK Web interface.

////
_As user _{sid2-lc}adm_, for example on ASCS host, check the SAP system
//[subs="specialchars,attributes,verbatim,quotes"]
//======
[subs="attributes"]
----
# su - {sid2-lc}adm
# sapcontrol -nr {inst-ascs2} -function GetSystemInstanceList
# sapcontrol -nr {inst-ascs2} -function GetProcessList -host {v-ascs2host}
# sapcontrol -nr {inst-ers2} -function GetProcessList -host {v-ers2host}
----
//======
////

====

=== Example ENSA2 in a Two-Node Cluster
//[[2nensa2]]
As known from the setup based on the *SUSE Best Practice* document for {sapS4}
(https://documentation.suse.com/sbp/all -> {sapS4} - Enqueue Replication 2 High Availability Cluster - Setup Guide),
the cluster integration can be done with a configuration file. Create one configuration file for each SID
and load them one by one into the cluster. The following description will help to do the steps in the right direction.

The cluster is already up and running and still runs one ASCS / ERS configuration based on the group concept.
The following tasks must be executed:

* Stop the new installed {sap} system, which will be added to the cluster.
* Unmount the file system with _umount_ and remove the IP address from the new {sap} system.
* Check and/or modify the ASCS and ERS profile files and add the section for the {s4sClConnector3} integration, if not done already.
** for example for {sid2}: __/sapmnt/{sid2}/profile/{sid2}_ASCS{inst-ascs2}_{v-ascs2host}__
** for example for {sid2}: __/sapmnt/{sid2}/profile/{sid2}_ERS{inst-ers2}_{v-ers2host}__
* Modify the _haclient_ group on each cluster node and add the new <sid>adm to it, if not done already.
* Set the cluster into maintenance mode.
* Load the new configuration with _crm configure load update <filename>_.
* Pre-check the cluster with _crm status_.
* Release the cluster from maintenance mode.
* Check the cluster and {sap} instances with _crm_mon_ and _sapcontrol -nr ..._.

.Prepare a configuration file and extend the cluster
====
_As user_ *root* prepare a file for the {sap} system {sid2}
//[subs="specialchars,attributes,verbatim,quotes"]
//======
[subs="attributes"]
----
#  cat ensa2-2nd-{sid2-lc}.txt

primitive rsc_fs_{sid2}_ASCS{inst-ascs2} Filesystem \
	params device="{ascs2nfsexp}" directory="/usr/sap/{sid2}/ASCS{inst-ascs2}" fstype=nfs options="vers=3" \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=300s
primitive rsc_fs_{sid2}_ERS{inst-ers2} Filesystem \
	params device="{ers2nfsexp}" directory="/usr/sap/{sid2}/ERS{inst-ers2}" fstype=nfs options="vers=3" \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=300s
primitive rsc_ip_{sid2}_ASCS{inst-ascs2} IPaddr2 \
	params ip={vip-ascs2} \
	op monitor interval=10s timeout=20s
primitive rsc_ip_{sid2}_ERS{inst-ers2} IPaddr2 \
	params ip={vip-ers2} \
	op monitor interval=10s timeout=20s
primitive rsc_sap_{sid2}_ASCS{inst-ascs2} SAPInstance \
	operations $id=rsc_sap_{sid2}_ASCS{inst-ascs2}-operations \
	op monitor interval=11 timeout=60 on-fail=restart \
	params InstanceName={sid2}_ASCS{inst-ascs2}_{v-ascs2host} START_PROFILE="/sapmnt/{sid2}/profile/{sid2}_ASCS{inst-ascs2}_{v-ascs2host}" AUTOMATIC_RECOVER=false \
	meta resource-stickiness=5000
primitive rsc_sap_{sid2}_ERS{inst-ers2} SAPInstance \
	operations $id=rsc_sap_{sid2}_ERS{inst-ers2}-operations \
	op monitor interval=11 timeout=60 on-fail=restart \
	params InstanceName={sid2}_ERS{inst-ers2}_{v-ers2host} START_PROFILE="/sapmnt/{sid2}/profile/{sid2}_ERS{inst-ers2}_{v-ers2host}" AUTOMATIC_RECOVER=false IS_ERS=true
group grp_{sid2}_ASCS{inst-ascs2} rsc_ip_{sid2}_ASCS{inst-ascs2} rsc_fs_{sid2}_ASCS{inst-ascs2} rsc_sap_{sid2}_ASCS{inst-ascs2} \
	meta resource-stickiness=3000
group grp_{sid2}_ERS{inst-ers2} rsc_ip_{sid2}_ERS{inst-ers2} rsc_fs_{sid2}_ERS{inst-ers2} rsc_sap_{sid2}_ERS{inst-ers2}
colocation col_sap_{sid2}_no_both -5000: grp_{sid2}_ERS{inst-ers2} grp_{sid2}_ASCS{inst-ascs2}
order ord_sap_{sid2}_first_start_ascs Optional: rsc_sap_{sid2}_ASCS{inst-ascs2}:start rsc_sap_{sid2}_ERS{inst-ers2}:stop symmetrical=false

----
//======

_As user_ *root* prepare a file for the {sap} system {sid3}
//[subs="specialchars,attributes,verbatim,quotes"]
//======

[subs="attributes"]
----
#  cat ensa2-3nd-{sid3-lc}.txt

primitive rsc_fs_{sid3}_ASCS{inst-ascs3} Filesystem \
	params device="{ascs3nfsexp}" directory="/usr/sap/{sid3}/ASCS{inst-ascs3}" fstype=nfs options="vers=3" \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=300s
primitive rsc_fs_{sid3}_ERS{inst-ers3} Filesystem \
	params device="{ers3nfsexp}" directory="/usr/sap/{sid3}/ERS{inst-ers3}" fstype=nfs options="vers=3" \
	op start timeout=60s interval=0 \
	op stop timeout=60s interval=0 \
	op monitor interval=20s timeout=300s
primitive rsc_ip_{sid3}_ASCS{inst-ascs3} IPaddr2 \
	params ip={vip-ascs3} \
	op monitor interval=10s timeout=20s
primitive rsc_ip_{sid3}_ERS{inst-ers3} IPaddr2 \
	params ip={vip-ers3} \
	op monitor interval=10s timeout=20s
primitive rsc_sap_{sid3}_ASCS{inst-ascs3} SAPInstance \
	operations $id=rsc_sap_{sid3}_ASCS{inst-ascs3}-operations \
	op monitor interval=11 timeout=60 on-fail=restart \
	params InstanceName={sid3}_ASCS{inst-ascs3}_{v-ascs3host} START_PROFILE="/sapmnt/{sid3}/profile/{sid3}_ASCS{inst-ascs3}_{v-ascs3host}" AUTOMATIC_RECOVER=false \
	meta resource-stickiness=5000
primitive rsc_sap_{sid3}_ERS{inst-ers3} SAPInstance \
	operations $id=rsc_sap_{sid3}_ERS{inst-ers3}-operations \
	op monitor interval=11 timeout=60 on-fail=restart \
	params InstanceName={sid3}_ERS{inst-ers3}_{v-ers3host} START_PROFILE="/sapmnt/{sid3}/profile/{sid3}_ERS{inst-ers3}_{v-ers3host}" AUTOMATIC_RECOVER=false IS_ERS=true
group grp_{sid3}_ASCS{inst-ascs3} rsc_ip_{sid3}_ASCS{inst-ascs3} rsc_fs_{sid3}_ASCS{inst-ascs3} rsc_sap_{sid3}_ASCS{inst-ascs3} \
	meta resource-stickiness=3000
group grp_{sid3}_ERS{inst-ers3} rsc_ip_{sid3}_ERS{inst-ers3} rsc_fs_{sid3}_ERS{inst-ers3} rsc_sap_{sid3}_ERS{inst-ers3}
colocation col_sap_{sid3}_no_both -5000: grp_{sid3}_ERS{inst-ers3} grp_{sid3}_ASCS{inst-ascs3}
order ord_sap_{sid3}_first_start_ascs Optional: rsc_sap_{sid3}_ASCS{inst-ascs3}:start rsc_sap_{sid3}_ERS{inst-ers3}:stop symmetrical=false
----
//======
====

.Configuration Example for SID {sid4} and following
====
This is similar to the SID examples before. You must adapt the:

* File system sources which will be mounted
* IP address for virtual host name of ASCS and ERS
* Instance number for ASCS and ERS
* Local mount point for ASCS and ERS
* Profile path for ASCS and ERS
====

.Add the 2nd and further SID into the cluster
====

The cluster is already up and running and still run one ASCS / ERS configuration based on the group concept.
The following tasks must be executed:

* Stop the new installed {sap} system, which will be added to the cluster.
* Unmount the file system with _umount_ and remove the IP address from the new {sap} system.
* Check and/or modify the ASCS and ERS profile files and add the section for the {s4sClConnector3} integration, if not done already.
** for example for {sid2}: __/sapmnt/{sid2}/profile/{sid2}_ASCS{inst-ascs2}_{v-ascs2host}__
** for example for {sid2}: __/sapmnt/{sid2}/profile/{sid2}_ERS{inst-ers2}_{v-ers2host}__
* Modify the _haclient_ group on each cluster node and add the new <sid>adm to it, if not done already.
* Set the cluster into maintenance mode.
* Load the new configuration with _crm configure load update <filename>_.
* Pre-check the cluster with _crm status_.
* Release the cluster from maintenance mode.
* Check the cluster and {sap} instances with _crm_mon -1rnf_ and _sapcontrol -nr ..._.

Login to one of the cluster nodes and run the commands from there.

_As user_ *root* set cluster from maintenance and load the configuration into the cluster
//[subs="specialchars,attributes,verbatim,quotes"]
//======

[subs="attributes"]
----
# crm configure property maintenance-mode=true
# crm configure load update ensa2-2nd-{sid2-lc}.txt
# crm configure load update ensa2-3nd-{sid3-lc}.txt
# crm configure load update ensa2-4nd-{sid4-lc}.txt
...
# crm status
----
//======

You should get back the prompt only. No further messages should be shown. If a message is displayed, there might be something wrong in the configuration file.

NOTE: If a wrong path is used, for example for the profile, this will not be detected during the configuration load. However, it will be shown during the cluster start action.

_As user_ *root* release cluster from maintenance and check the new cluster resources
//[subs="specialchars,attributes,verbatim,quotes"]
//======

[subs="attributes"]
----
# crm configure property maintenance-mode=false
# crm_mon -rfn
----
//======

_As user {sid2-lc}adm_, for example on ASCS host, check the {sap} system
//[subs="specialchars,attributes,verbatim,quotes"]
//======

[subs="attributes"]
----
# su - {sid2-lc}adm
# sapcontrol -nr {inst-ascs2} -function GetSystemInstanceList
# sapcontrol -nr {inst-ascs2} -function GetProcessList -host {v-ascs2host}
# sapcontrol -nr {inst-ers2} -function GetProcessList -host {v-ers2host}
----
//======

NOTE: Repeat the steps for each SID you have installed.

////
_As user *root*_ showing a cluster configuration for two {sap} systems
//[subs="specialchars,attributes,verbatim,quotes"]
//======
[subs="attributes"]
----
#  crm configure show


----
//======
////
====
=== Example ENSA1 in a Multi-Node Cluster
This base setup is already documented in the *SUSE Best Practices* document for {sapNW}
(https://documentation.suse.com/sbp/all -> {sapS4} - SAP NetWeaver High Availability Cluster 7.40 - Setup Guide - Best Practice Guide).
The OS and node preparation must be done for each future cluster member:

* patch level
* settings for date and time, DNS
* access to the same fencing device (methode)

The major difference compared to the two-node setup is: run the ha-cluster-join command multiple time until all nodes are member of the cluster.

The cluster integration can be done with a configuration file. Create one configuration file for each SID
and load them one by one into the cluster. The steps are equal to the steps described in
<<Example ENSA1 in a Two-Node Cluster>>.

=== Example ENSA2 in a Multi-Node Cluster
This base setup is already documented in the *SUSE Best Practices* document for {sapS4}
(https://documentation.suse.com/sbp/all -> {sapS4} - Enqueue Replication 2 High Availability Cluster - Best Practice Guide).
The OS and node preparation must be done for each future cluster member:

- patch level
- settings for date and time, DNS
- access to the same fencing device (methode)

The major difference compared to the two-node setup is: run the _ha-cluster-join_ command multiple times until all nodes are member of the cluster.

The cluster integration can be done with a configuration file. Create one configuration file for each SID
and load them one by one into the cluster. The steps are equal to the steps described in
<<Example ENSA2 in a Two-Node Cluster>>.

=== Example ENSA1 and ENSA2 in a Two-Node Cluster
//[[2nensa1ensa2]]

As known from the setup based on the *SUSE Best Practices* document for {sapNW} and {sapS4}
(https://documentation.suse.com/sbp/all -> {sapNW} High Availability Cluster 7.40 - Setup Guide)
(https://documentation.suse.com/sbp/all -> {sapS4} - Enqueue Replication 2 High Availability Cluster - Setup Guide),
the cluster integration can be done with a configuration file. Create one configuration file for each SID
and load them one by one into the cluster. 

We expect the cluster is already up and running and still runs one ASCS / ERS configuration based on the group concept.

NOTE: Follow and combine the chapters "Example ENSA1 in a Two-Node Cluster" and "Example ENSA2 in a Two-Node Cluster".
Be careful and keep in mind each {sap} system must be independent from each other.

<<Example ENSA1 in a Two-Node Cluster>>

<<Example ENSA2 in a Two-Node Cluster>>

////
[subs="specialchars,attributes,verbatim,quotes"]
======
======
////

=== Example ENSA1 and ENSA2 in a Multi-Node Cluster

The setup here is similar to the one described in the chapter <<Example ENSA1 and ENSA2 in a Two-Node Cluster>> but only with more nodes in the cluster.

After the basic cluster part is done, the procedure is the same as for a two-node cluster setup. Check the following chapter for further instructions:
<<Example ENSA1 and ENSA2 in a Two-Node Cluster>>.

////
[subs="specialchars,attributes,verbatim,quotes"]
======
======
////

=== Workload Balancing of Central Services

//TODO: load balancing

NOTE: Under evaluation.

==== Automatic Load Balancing by Pacemaker

There is no additional task necessary.

==== Manual Load Balancing by Pacemaker

NOTE: Under evaluation.

//[subs="attributes"]
//[subs="specialchars,attributes,verbatim,quotes"]
//----
//======
//# placeholder
//======
//----

=== Testing the New Configuration of Multiple SID in One Cluster

IMPORTANT: A well-defined and overall test of the new configuration is extremely recommended. 
Read the requirements for each setup carefully. The test procedure will demonstrate if the cluster works as expected.
We already have described multiple test scenarios in our base Best Practices documentation for
{sapNW} and {sapS4}.

== Multi-SID - Database Services

NOTE: This is currently under evaluation. Send an e-mail to saphana@suse.com if this might be from interest for you.

// Standard SUSE Best Practices includes
== Legal Notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
// :leveloffset: 1
include::common_gfdl1.2_i.adoc[]
